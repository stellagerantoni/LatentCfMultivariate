{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellagerantoni/LatentCfMultivariate/blob/main/FaceDetection_KDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/stellagerantoni/LatentCfMultivariate"
      ],
      "metadata": {
        "id": "1fwOXGEl_ine",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d875ff4b-71c7-4737-8987-701e3116f54e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LatentCfMultivariate'...\n",
            "remote: Enumerating objects: 212, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 212 (delta 80), reused 36 (delta 36), pack-reused 105\u001b[K\n",
            "Receiving objects: 100% (212/212), 12.40 MiB | 23.64 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wildboar\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q stumpy\n",
        "!pip install -q fastdtw\n",
        "!pip install aeon"
      ],
      "metadata": {
        "id": "89L3kts7CCan",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b873fc7-0514-40e7-8eeb-53d127c65875"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.1/169.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m926.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aeon\n",
            "  Downloading aeon-0.6.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon) (23.2.0)\n",
            "Collecting deprecated>=1.2.13 (from aeon)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: numba>=0.55 in /usr/local/lib/python3.10/dist-packages (from aeon) (0.58.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from aeon) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from aeon) (23.2)\n",
            "Requirement already satisfied: pandas<2.1.0,>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from aeon) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn<1.4.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from aeon) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon) (1.11.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->aeon) (1.14.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55->aeon) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.1.0,>=1.5.3->aeon) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.1.0,>=1.5.3->aeon) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.0,>=1.0.0->aeon) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.0,>=1.0.0->aeon) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.1.0,>=1.5.3->aeon) (1.16.0)\n",
            "Installing collected packages: deprecated, aeon\n",
            "Successfully installed aeon-0.6.0 deprecated-1.2.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from argparse import ArgumentParser\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wildboar.datasets import load_dataset\n",
        "from wildboar.ensemble import ShapeletForestClassifier\n",
        "from wildboar.explain.counterfactual import counterfactuals\n",
        "%cd '/content/LatentCfMultivariate'\n",
        "from _guided import ModifiedLatentCF\n",
        "from help_functions import *\n",
        "from keras_models import *"
      ],
      "metadata": {
        "id": "Bdkpan5lCGRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5982460-4744-449f-ceb3-eac4f822b448"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LatentCfMultivariate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "RANDOM_STATE = 39"
      ],
      "metadata": {
        "id": "GJE1AxFnE51S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACTUALL CODE**\n",
        "datasets available : 'Heartbeat', 'SelfRegulationSCP1'"
      ],
      "metadata": {
        "id": "6vVfmpyuZyC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset):\n",
        "  X, y = load_classification(dataset)\n",
        "  if dataset == 'FaceDetection':\n",
        "    pos = '1'\n",
        "    neg = '0'\n",
        "\n",
        "\n",
        "  print(\" Shape of X = \", X.shape)\n",
        "  print(\" Shape of y = \", y.shape)\n",
        "  #print(\" Meta data = \", meta_data)\n",
        "  # Convert positive and negative labels to 1 and 0\n",
        "  pos_label, neg_label = 1, 0\n",
        "  if pos != pos_label:\n",
        "      y[y==pos] = pos_label # convert/normalize positive label to 1\n",
        "  if neg != neg_label:\n",
        "      y[y==neg] = neg_label # convert negative label to 0\n",
        "\n",
        "  y = y.astype(int)\n",
        "  print(f\"\\n X[:1] = \\n{X[:1]}\")\n",
        "  return X,y\n"
      ],
      "metadata": {
        "id": "Ya5WxsJ8TSgZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 39\n",
        "X, y = load_dataset('FaceDetection')\n",
        "X = X.transpose(0,2,1)\n",
        "\n",
        "#print(f'data imformation = {data_information}')\n",
        "#keep half the dataset because it is too big\n",
        "X,X_through,y,y_through = train_test_split(X, y, test_size=0.5, random_state=RANDOM_STATE, stratify=y)\n",
        "print(f'shape of X = {X.shape}')\n",
        "print(f'shape of y = {y.shape}')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
        "print(f'shape of X train = {X_train.shape}')\n",
        "print(f'shape of y train = {y_train.shape}')"
      ],
      "metadata": {
        "id": "W4m9pwqyVY1b",
        "outputId": "ce2458fc-4594-4489-cc04-9029e49588df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X =  (9414, 144, 62)\n",
            " Shape of y =  (9414,)\n",
            "\n",
            " X[:1] = \n",
            "[[[-0.07545  -0.336703 -0.278238 ... -0.411078 -1.016122 -1.161735]\n",
            "  [ 0.05608  -0.128013 -0.323847 ... -2.114348  0.208789 -0.509533]\n",
            "  [-0.824537 -0.746068 -0.482871 ... -0.929275 -1.007972 -0.292018]\n",
            "  ...\n",
            "  [-0.56758  -1.073942 -1.136367 ... -0.541122 -0.765445 -1.73308 ]\n",
            "  [-0.23404   0.104291  0.327425 ... -1.458801  0.318952  1.854007]\n",
            "  [-0.356189 -0.511199 -0.483072 ... -1.177502 -0.728301 -0.400074]]]\n",
            "shape of X = (4707, 62, 144)\n",
            "shape of y = (4707,)\n",
            "shape of X train = (3765, 62, 144)\n",
            "shape of y train = (3765,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upsample the minority class\n",
        "\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'before: {class_counts}')\n",
        "X_train,y_train = upsample_minority_multivariate(X_train,y_train)\n",
        "X,y = upsample_minority_multivariate(X, y)\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'after: {class_counts}')"
      ],
      "metadata": {
        "id": "Q2v7QdrHieA8",
        "outputId": "e5a02670-2995-4415-b975-c8ae5434feb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: [1883 1882]\n",
            "after: [1883 1883]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing and Padding all our data\n",
        "#Padding needed for autoencoder\n",
        "\n",
        "n_training,n_timesteps, n_features= X_train.shape\n",
        "\n",
        "X, trained_scaler =  normalize_multivariate(data=X, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_train_processed, trained_scaler =  normalize_multivariate(data=X_train, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_test_processed, _ =  normalize_multivariate(data=X_test, n_timesteps=n_timesteps, scaler=trained_scaler, n_features = n_features)\n",
        "\n",
        "X, padding_size = conditional_pad_multivariate(X)\n",
        "X_train_processed_padded, padding_size = conditional_pad_multivariate(X_train_processed) # add extra padding zeros if n_timesteps cannot be divided by 4, required for 1dCNN autoencoder structure\n",
        "X_test_processed_padded, _ = conditional_pad_multivariate(X_test_processed)\n",
        "\n",
        "n_timesteps_padded = X_train_processed_padded.shape[1]\n",
        "print(f\"Data pre-processed, original #timesteps={n_timesteps}, padded #timesteps={n_timesteps_padded}.\")\n",
        "\n",
        "#check the processing (0,1) min should be min 0 and max should be max 1\n",
        "print(f\"\\nmin value = {np.min(X_train)}, max value = {np.max(X_train)}\")\n",
        "print(f\"min value normalized = {np.min(X_train_processed)}, max value normalized= {np.max(X_train_processed)}\")\n",
        "\n",
        "#check that padding paddes the right dimention\n",
        "print(f\"\\nX_train.shape = {X_train.shape}\" )\n",
        "print(f\"X_train_processed_padded.shape = {X_train_processed_padded.shape}\")\n"
      ],
      "metadata": {
        "id": "00Q9QjKy7wEZ",
        "outputId": "002bdc73-a23f-4f84-db50-b048809f21d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pre-processed, original #timesteps=62, padded #timesteps=64.\n",
            "\n",
            "min value = -24.327769, max value = 24.326942\n",
            "min value normalized = 0.0, max value normalized= 1.0000000000000002\n",
            "\n",
            "X_train.shape = (3766, 62, 144)\n",
            "X_train_processed_padded.shape = (3766, 64, 144)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_validation, y_train, y_validation = train_test_split(X_train_processed_padded, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train)"
      ],
      "metadata": {
        "id": "1mYFZmsvtB9q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the two forms of labels needed\n",
        "#-the y_classes (1,0,1,0,...)\n",
        "#-the y (one hot encoded)\n",
        "\n",
        "print(f'X_train = {X_train.shape}')\n",
        "print(f'X_validation = {X_validation.shape}')\n",
        "print(f'X_test = {X_test.shape}')\n",
        "\n",
        "y_classes = y\n",
        "y_train_classes = y_train\n",
        "y_validation_classes = y_validation\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, len(np.unique(y)))\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_validation = to_categorical(y_validation, len(np.unique(y_validation)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
        "\n",
        "print(f'\\ny_train_classes = {y_train_classes.shape}, y_validation_classes = {y_validation_classes.shape}, y_test_classes = {y_test_classes.shape}')\n",
        "print(f'y_train = {y_train.shape}, y_validation = {y_validation.shape}, y_test= {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9b2F-vytxzU",
        "outputId": "27d61081-3f69-4501-efea-0e260631f211"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train = (3012, 64, 144)\n",
            "X_validation = (754, 64, 144)\n",
            "X_test = (942, 62, 144)\n",
            "\n",
            "y_train_classes = (3012,), y_validation_classes = (754,), y_test_classes = (942,)\n",
            "y_train = (3012, 2), y_validation = (754, 2), y_test= (942, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Classifier(\n",
        "    n_timesteps, n_features, n_conv_layers=1, add_dense_layer=True, n_output=2\n",
        "):\n",
        "    # https://keras.io/examples/timeseries/timeseries_classification_from_scratch/\n",
        "\n",
        "    input_shape = ( n_timesteps,n_features)\n",
        "\n",
        "    inputs = keras.Input(shape=input_shape, dtype=\"float32\")\n",
        "\n",
        "    if add_dense_layer:\n",
        "        x = keras.layers.Dense(128, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(inputs)\n",
        "    else:\n",
        "        x = inputs\n",
        "\n",
        "    for i in range(n_conv_layers):\n",
        "        x = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "\n",
        "    x = keras.layers.MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
        "    x = keras.layers.Flatten()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output,kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return classifier"
      ],
      "metadata": {
        "id": "WoMahiopQqxb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ## LatentCF++ models\n",
        "# reset seeds for numpy, tensorflow, python random package and python environment seed\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ### 1dCNN classifier\n",
        "\n",
        "cnnClassifier = Classifier(\n",
        "    n_timesteps_padded, n_features, n_output=2, add_dense_layer = False\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "cnnClassifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=15, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM-FCN classifier:\")\n",
        "classifier_history = cnnClassifier.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=True,\n",
        "    validation_data=(X_validation, y_validation),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = cnnClassifier.predict(X_test_processed_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM-FCN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:1\", \"True:0\"],\n",
        "    columns=[\"Pred:1\", \"Pred:0\"],\n",
        ")\n",
        "print(confusion_matrix_df)\n"
      ],
      "metadata": {
        "id": "yNkKTXe6IIyF",
        "outputId": "0b343907-d742-4f8b-c8fe-622443e68ac8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM-FCN classifier:\n",
            "Epoch 1/150\n",
            "251/251 [==============================] - 7s 7ms/step - loss: 0.7165 - accuracy: 0.5697 - val_loss: 0.8381 - val_accuracy: 0.5000\n",
            "Epoch 2/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.5930 - accuracy: 0.7178 - val_loss: 0.7679 - val_accuracy: 0.5027\n",
            "Epoch 3/150\n",
            "251/251 [==============================] - 1s 6ms/step - loss: 0.4858 - accuracy: 0.7985 - val_loss: 0.6333 - val_accuracy: 0.6724\n",
            "Epoch 4/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.4049 - accuracy: 0.8612 - val_loss: 1.3723 - val_accuracy: 0.5040\n",
            "Epoch 5/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.3287 - accuracy: 0.9017 - val_loss: 1.0553 - val_accuracy: 0.5345\n",
            "Epoch 6/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.2693 - accuracy: 0.9376 - val_loss: 1.7538 - val_accuracy: 0.5080\n",
            "Epoch 7/150\n",
            "251/251 [==============================] - 2s 6ms/step - loss: 0.2304 - accuracy: 0.9552 - val_loss: 0.4558 - val_accuracy: 0.8263\n",
            "Epoch 8/150\n",
            "251/251 [==============================] - 2s 7ms/step - loss: 0.1872 - accuracy: 0.9781 - val_loss: 0.7628 - val_accuracy: 0.6724\n",
            "Epoch 9/150\n",
            "251/251 [==============================] - 1s 6ms/step - loss: 0.1608 - accuracy: 0.9884 - val_loss: 1.8307 - val_accuracy: 0.5265\n",
            "Epoch 10/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.1418 - accuracy: 0.9934 - val_loss: 0.4452 - val_accuracy: 0.8422\n",
            "Epoch 11/150\n",
            "251/251 [==============================] - 1s 6ms/step - loss: 0.1208 - accuracy: 0.9987 - val_loss: 1.5447 - val_accuracy: 0.5544\n",
            "Epoch 12/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.1120 - accuracy: 0.9993 - val_loss: 0.4135 - val_accuracy: 0.8820\n",
            "Epoch 13/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.1013 - accuracy: 1.0000 - val_loss: 0.4275 - val_accuracy: 0.8528\n",
            "Epoch 14/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.0995 - accuracy: 0.9993 - val_loss: 0.6920 - val_accuracy: 0.7493\n",
            "Epoch 15/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.0901 - accuracy: 0.9997 - val_loss: 0.6561 - val_accuracy: 0.7268\n",
            "Epoch 16/150\n",
            "251/251 [==============================] - 2s 6ms/step - loss: 0.0832 - accuracy: 1.0000 - val_loss: 0.3992 - val_accuracy: 0.8660\n",
            "Epoch 17/150\n",
            "251/251 [==============================] - 2s 8ms/step - loss: 0.0799 - accuracy: 1.0000 - val_loss: 0.5478 - val_accuracy: 0.8263\n",
            "Epoch 18/150\n",
            "251/251 [==============================] - 2s 8ms/step - loss: 0.0762 - accuracy: 1.0000 - val_loss: 0.4675 - val_accuracy: 0.8329\n",
            "Epoch 19/150\n",
            "251/251 [==============================] - 2s 7ms/step - loss: 0.0786 - accuracy: 0.9993 - val_loss: 1.8190 - val_accuracy: 0.5385\n",
            "Epoch 20/150\n",
            "251/251 [==============================] - 2s 7ms/step - loss: 0.0777 - accuracy: 1.0000 - val_loss: 0.5312 - val_accuracy: 0.7944\n",
            "Epoch 21/150\n",
            "251/251 [==============================] - 1s 6ms/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 0.3743 - val_accuracy: 0.8660\n",
            "Epoch 22/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.0609 - accuracy: 1.0000 - val_loss: 1.1056 - val_accuracy: 0.6300\n",
            "Epoch 23/150\n",
            "251/251 [==============================] - 1s 5ms/step - loss: 0.0603 - accuracy: 1.0000 - val_loss: 0.4131 - val_accuracy: 0.8488\n",
            "Epoch 24/150\n",
            "251/251 [==============================] - 1s 6ms/step - loss: 0.0560 - accuracy: 1.0000 - val_loss: 0.4003 - val_accuracy: 0.8541\n",
            "Epoch 25/150\n",
            "251/251 [==============================] - 2s 8ms/step - loss: 0.0736 - accuracy: 0.9954 - val_loss: 0.9278 - val_accuracy: 0.6578\n",
            "Epoch 26/150\n",
            "251/251 [==============================] - 2s 7ms/step - loss: 0.0662 - accuracy: 0.9997 - val_loss: 0.4508 - val_accuracy: 0.8554\n",
            "Epoch 27/150\n",
            "251/251 [==============================] - 1s 6ms/step - loss: 0.0490 - accuracy: 1.0000 - val_loss: 0.4028 - val_accuracy: 0.8714\n",
            "30/30 [==============================] - 0s 4ms/step\n",
            "LSTM-FCN classifier trained, with validation accuracy 0.743099787685775.\n",
            "        Pred:1  Pred:0\n",
            "True:1     342     129\n",
            "True:0     113     358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoder = Autoencoder( n_timesteps_padded,n_features,32)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoder.fit(\n",
        "    X_train,\n",
        "    X_train,\n",
        "    epochs=50,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_validation, X_validation),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")\n"
      ],
      "metadata": {
        "id": "E6IxH8BLEKFG",
        "outputId": "33658a83-7d1f-48ec-e182-e4e288f09c78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 64, 144)\n",
            "(None, 64, 32)\n",
            "(None, 32, 32)\n",
            "(None, 32, 16)\n",
            "(None, 16, 16)\n",
            "(None, 16, 16)\n",
            "(None, 32, 16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 32, 32)\n",
            "(None, 64, 32)\n",
            "(None, 64, 144)\n",
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "251/251 - 5s - loss: 0.0144 - val_loss: 0.0016 - 5s/epoch - 18ms/step\n",
            "Epoch 2/50\n",
            "251/251 - 2s - loss: 0.0012 - val_loss: 0.0012 - 2s/epoch - 9ms/step\n",
            "Epoch 3/50\n",
            "251/251 - 2s - loss: 0.0011 - val_loss: 0.0012 - 2s/epoch - 8ms/step\n",
            "Epoch 4/50\n",
            "251/251 - 2s - loss: 0.0010 - val_loss: 0.0011 - 2s/epoch - 7ms/step\n",
            "Epoch 5/50\n",
            "251/251 - 2s - loss: 0.0010 - val_loss: 0.0011 - 2s/epoch - 7ms/step\n",
            "Epoch 6/50\n",
            "251/251 - 2s - loss: 0.0010 - val_loss: 0.0011 - 2s/epoch - 7ms/step\n",
            "Epoch 7/50\n",
            "251/251 - 2s - loss: 0.0010 - val_loss: 0.0011 - 2s/epoch - 7ms/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.001126678311266005.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gettting the Global weights, needed for counterfactuals\n",
        "\n",
        "from _guided import get_global_weights\n",
        "from help_functions import evaluate\n",
        "pos_label = 1\n",
        "neg_label = 0\n",
        "\n",
        "step_weights = get_global_weights(\n",
        "        X,\n",
        "        y_classes,\n",
        "        cnnClassifier,\n",
        "        n_timesteps= n_timesteps,\n",
        "        n_features=n_features,\n",
        "        random_state=RANDOM_STATE,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hysd9dxSsx9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3610cf11-8794-49f1-a283-3ca8c3a8b2ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 4ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n",
            "148/148 [==============================] - 1s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_weights"
      ],
      "metadata": {
        "id": "3WcwnNgc3RdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29abf455-1a39-4b74-aa2f-ddbb4a80ee02"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "lYw0fqWV5Mqt",
        "outputId": "2f5a3970-ef0c-43a0-86f5-83bbf30035d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3012, 64, 144)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from wildboar.explain import IntervalImportance\n",
        "#from LIMESegment.Utils.explanations import LIMESegment\n",
        "\n",
        "\n",
        "class ModifiedLatentCF:\n",
        "    \"\"\"Explanations by generating a counterfacutal sample in the latent space of\n",
        "    any autoencoder.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Learning Time Series Counterfactuals via Latent Space Representations,\n",
        "    Wang, Z., Samsten, I., Mochaourab, R., Papapetrou, P., 2021.\n",
        "    in: International Conference on Discovery Science, pp. 369–384. https://doi.org/10.1007/978-3-030-88942-5_29\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        probability=0.5,\n",
        "        *,\n",
        "        tolerance=1e-6,\n",
        "        max_iter=100,\n",
        "        optimizer=None,\n",
        "        autoencoder=None,\n",
        "        margin_weight=1.0,  # weighted_steps_weight = 1 - pred_margin_weight\n",
        "        random_state=None,\n",
        "        bandwidth,\n",
        "        weighted_steps_weight,\n",
        "        data,\n",
        "        step_weights\n",
        "    ):\n",
        "        self.optimizer_ = (\n",
        "            tf.optimizers.Adam(learning_rate=1e-4) if optimizer is None else optimizer\n",
        "        )\n",
        "        #self.x_axis_eights = x_axis_eights\n",
        "        #self.y_axis_eights = y_axis_eights\n",
        "        self.data = data\n",
        "        self.mse_loss_ = keras.losses.MeanSquaredError()\n",
        "        self.probability_ = tf.constant([probability])\n",
        "        self.tolerance_ = tf.constant(tolerance)\n",
        "        self.max_iter = max_iter\n",
        "        self.autoencoder = autoencoder\n",
        "        self.random_state = random_state\n",
        "        self.weighted_steps_weight = weighted_steps_weight\n",
        "        self.step_weights = step_weights\n",
        "\n",
        "        # Weights of the different loss components\n",
        "        self.margin_weight = margin_weight\n",
        "        self.kde_weight = tf.cast(1 - self.margin_weight-self.weighted_steps_weight,tf.float32)\n",
        "        if (self.weighted_steps_weight+self.margin_weight)>1.0:\n",
        "           raise ValueError(\"(weighted_steps_weight + margin_weight) should be less that 1.0\")\n",
        "\n",
        "        self.bandwidth = bandwidth\n",
        "\n",
        "    def fit(self, model):\n",
        "        \"\"\"Fit a new counterfactual explainer to the model\n",
        "\n",
        "        Paramaters\n",
        "        ----------\n",
        "\n",
        "        model : keras.Model\n",
        "            The model\n",
        "        \"\"\"\n",
        "        if self.autoencoder:\n",
        "            (\n",
        "                encode_input,\n",
        "                encode_output,\n",
        "                decode_input,\n",
        "                decode_output,\n",
        "            ) = extract_encoder_decoder(self.autoencoder)\n",
        "            self.decoder_ = keras.Model(inputs=decode_input, outputs=decode_output)\n",
        "            self.encoder_ = keras.Model(inputs=encode_input, outputs=encode_output)\n",
        "        else:\n",
        "            self.decoder_ = None\n",
        "            self.encoder_ = None\n",
        "        self.model_ = model\n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Compute the difference between the desired and actual probability\n",
        "\n",
        "        Parameters\n",
        "        ---------\n",
        "        x : Variable\n",
        "            Variable of the sample\n",
        "        \"\"\"\n",
        "        if self.autoencoder is None:\n",
        "            z = x\n",
        "        else:\n",
        "            z = self.decoder_(x)\n",
        "\n",
        "        return self.model_(z)\n",
        "\n",
        "    # The \"pred_margin_loss\" is designed to measure the prediction probability to the desired decision boundary\n",
        "    def pred_margin_mse(self, prediction):\n",
        "        return self.mse_loss_(self.probability_, prediction)\n",
        "\n",
        "    # An auxiliary MAE loss function to measure the proximity with step_weights\n",
        "    def weighted_mae(self, original_sample, cf_sample, step_weights):\n",
        "        return tf.math.reduce_mean(\n",
        "            tf.math.multiply(tf.math.abs(original_sample - cf_sample), step_weights)\n",
        "        )\n",
        "\n",
        "    # An auxiliary normalized L2 loss function to measure the proximity with step_weights\n",
        "    def weighted_normalized_l2(self, original_sample, cf_sample, step_weights):\n",
        "        var_diff = tf.math.reduce_variance(original_sample - cf_sample)\n",
        "        var_orig = tf.math.reduce_variance(original_sample)\n",
        "        var_cf = tf.math.reduce_variance(cf_sample)\n",
        "\n",
        "        normalized_l2 = 0.5 * var_diff / (var_orig + var_cf)\n",
        "        return tf.math.reduce_mean(\n",
        "            tf.math.multiply(\n",
        "                normalized_l2,\n",
        "                step_weights,\n",
        "            )\n",
        "        )\n",
        "    #x_axis_eights*self.step_weights_x,use_scotts_rule=False,use_silvermans_rule=True,manual_bandwidth = 0.1\n",
        "    def train_gaussian_kde(self, data, use_scotts_rule,use_silvermans_rule, manual_bandwidth ):\n",
        "      \"\"\"\n",
        "      Train a Gaussian KDE on the provided data.\n",
        "\n",
        "      :param data: Multivariate data points used for KDE (2D Tensor).\n",
        "      :param bandwidth: The bandwidth of the kernel (float).\n",
        "      :return: A function that represents the trained KDE.\n",
        "      \"\"\"\n",
        "\n",
        "      n = tf.cast(tf.shape(data)[0], tf.float32)\n",
        "      d = tf.cast(tf.shape(data)[1], tf.float32)\n",
        "      def kde_fn( x_points):\n",
        "        \"\"\"\n",
        "        Compute the density estimation for given points using the trained KDE.\n",
        "\n",
        "        :param x_points: Points where the density should be estimated (2D Tensor).\n",
        "        :return: Density estimates (Tensor).\n",
        "        \"\"\"\n",
        "        d = tf.cast(tf.shape(data)[1], tf.float32)\n",
        "\n",
        "        data_exp =  tf.expand_dims(data, axis=0)\n",
        "        #print(f'x_points shape = {x_points.shape} in kde fn')\n",
        "        x_points_exp = tf.reshape(x_points, (tf.shape(x_points)[0], 64, 1))\n",
        "\n",
        "        x_points_exp = tf.expand_dims(x_points_exp, axis=1)\n",
        "        if use_scotts_rule:\n",
        "          sigma = tf.math.reduce_std(data)\n",
        "          sigma = tf.cast(sigma,tf.float32)\n",
        "          bandwidth = n ** (-1.0 / (d + 4)) * sigma\n",
        "        elif use_silvermans_rule:\n",
        "          sigma = tf.math.reduce_std(data)\n",
        "          bandwidth = (4 * sigma**5 / (3 * n)) ** (1/5)\n",
        "        else:\n",
        "          if manual_bandwidth is None:\n",
        "            raise ValueError(\"Manual bandwidth must be provided if not using Scott's Rule.\")\n",
        "          bandwidth = manual_bandwidth\n",
        "\n",
        "        bandwidth = tf.cast(bandwidth, tf.float32)\n",
        "        x_points_exp = tf.cast(x_points_exp,tf.float32)\n",
        "        data_exp= tf.cast(data_exp, tf.float32)\n",
        "        diff = x_points_exp - data_exp\n",
        "        norm = tf.reduce_sum(diff ** 2, axis=2)\n",
        "        kernel_val = tf.exp(-norm / (2.0 * bandwidth ** 2))\n",
        "\n",
        "\n",
        "        d = tf.constant(d,tf.float32)\n",
        "        density = tf.reduce_mean(kernel_val, axis=1) / (bandwidth * tf.sqrt(2.0 * np.pi * d))\n",
        "        return density\n",
        "\n",
        "      return kde_fn\n",
        "\n",
        "    def gaussian_kde_logpdf(self, kde_fn, x_points):\n",
        "      \"\"\"\n",
        "      Evaluate the logpdf of the given points using the trained KDE function.\n",
        "\n",
        "      :param kde_fn: Trained KDE function.\n",
        "      :param x_points: Points to evaluate the logpdf (2D Tensor).\n",
        "      :return: Log of the density estimates (Tensor).\n",
        "      \"\"\"\n",
        "      #print(f'x_points shape = {x_points.shape} in gaussian kde')\n",
        "      density = kde_fn(x_points)\n",
        "      return tf.math.log(density)\n",
        "\n",
        "\n",
        "    # additional input of step_weights\n",
        "    def compute_loss(self,original_sample, z_search, target_label, kde_stop):\n",
        "        loss = tf.zeros(shape=())\n",
        "        decoded = self.decoder_(z_search) if self.autoencoder is not None else z_search\n",
        "        pred = self.model_(decoded)[:, target_label]\n",
        "\n",
        "        #margin loss (y-τ)\n",
        "        margin_loss = self.pred_margin_mse(pred)\n",
        "        loss += self.margin_weight * margin_loss\n",
        "\n",
        "        kde_diffrences = []\n",
        "        for dimention in range(decoded.shape[2]):\n",
        "          data_dimention = self.data[:,:,dimention]\n",
        "          data_dimention = data_dimention[:,:,np.newaxis]\n",
        "          kde = self.train_gaussian_kde(data = data_dimention,use_scotts_rule=False,use_silvermans_rule=False,manual_bandwidth = self.bandwidth)\n",
        "          #print(f'data_dimention shape = {data_dimention.shape} in compute loss just before computting mean log likellihood for all the data ')\n",
        "          mean_log_likelihood = tf.cast(tf.reduce_mean(self.gaussian_kde_logpdf(kde, x_points = (data_dimention))),tf.float32)\n",
        "\n",
        "          decoded_dimention = decoded[:,:,dimention]\n",
        "          decoded_dimention = decoded_dimention[:,:,np.newaxis]\n",
        "          #print(f'decoded_dimention shape = {decoded_dimention.shape} in compute loss just before computting mean log likellihood for decoded data ')\n",
        "          log_likelihood_of_sample = tf.cast(self.gaussian_kde_logpdf(kde, x_points = (decoded_dimention)),tf.float32)\n",
        "\n",
        "          kde_loss = tf.cast((mean_log_likelihood - log_likelihood_of_sample),tf.float32)\n",
        "          kde_diffrences.append(kde_loss)\n",
        "        kde_total_loss = tf.math.add_n(kde_diffrences)\n",
        "        kde_total_loss = tf.math.abs(kde_total_loss)\n",
        "        #print(kde_total_loss)\n",
        "        loss +=self.kde_weight *kde_total_loss\n",
        "\n",
        "\n",
        "        weighted_steps_loss = self.weighted_mae(\n",
        "            original_sample=tf.cast(original_sample, dtype=tf.float32),\n",
        "            cf_sample=tf.cast(decoded, dtype=tf.float32),\n",
        "            step_weights=tf.cast(step_weights, tf.float32),\n",
        "        )\n",
        "        loss += self.weighted_steps_weight * weighted_steps_loss\n",
        "\n",
        "        return loss, margin_loss, kde_loss, kde_stop\n",
        "\n",
        "    # TODO: compatible with the counterfactuals of wildboar\n",
        "    #       i.e., define the desired output target per label\n",
        "\n",
        "\n",
        "    def transform(self, x, pred_labels):\n",
        "        \"\"\"Generate counterfactual explanations\n",
        "\n",
        "        x : array-like of shape [n_samples, n_timestep, n_dims]\n",
        "            The samples\n",
        "        \"\"\"\n",
        "\n",
        "        result_samples = np.empty(x.shape)\n",
        "        losses = np.empty(x.shape[0])\n",
        "        # `weights_all` needed for debugging\n",
        "        weights_all = np.empty((x.shape[0], 1, x.shape[1], x.shape[2]))\n",
        "        for i in range(x.shape[0]):\n",
        "            if i % 25 == 0:\n",
        "                print(f\"{i+1} samples been transformed.\")\n",
        "            kde_stop = False\n",
        "\n",
        "            x_sample, loss = self._transform_sample(\n",
        "                x[np.newaxis, i], pred_labels[i],kde_stop\n",
        "            )\n",
        "\n",
        "            result_samples[i] = x_sample\n",
        "            losses[i] = loss\n",
        "            weights_all[i] = step_weights\n",
        "\n",
        "        print(f\"{i+1} samples been transformed, in total.\")\n",
        "\n",
        "        return result_samples, losses, weights_all\n",
        "\n",
        "    def _transform_sample(self, x, pred_label,kde_stop):\n",
        "        \"\"\"Generate counterfactual explanations(z))\"\"\"\n",
        "        # TODO: check_is_fitted(self)\n",
        "        if self.autoencoder is not None:\n",
        "            z = tf.Variable(self.encoder_(x))\n",
        "        else:\n",
        "            z = tf.Variable(x, dtype=tf.float32)\n",
        "\n",
        "        it = 0\n",
        "        target_label = 1 - pred_label  # for binary classification\n",
        "\n",
        "        #def compute_loss(self,original_sample, z_search, target_label)\n",
        "        with tf.GradientTape(persistent = True) as tape:\n",
        "            loss, pred_margin_loss, kde_loss, kde_stop = self.compute_loss(\n",
        "                x, z,  target_label,kde_stop\n",
        "            )\n",
        "        if self.autoencoder is not None:\n",
        "            pred = self.model_(self.decoder_(z))\n",
        "        else:\n",
        "            pred = self.model_(z)\n",
        "\n",
        "\n",
        "        while (\n",
        "            (loss > self.tolerance_\n",
        "            and pred[:, target_label] < self.probability_)\n",
        "            and (it < self.max_iter if self.max_iter else True)\n",
        "            ) :\n",
        "            # Get gradients of loss wrt the sample\n",
        "            grads = tape.gradient(loss, z)\n",
        "            # Update the weights of the sample\n",
        "            self.optimizer_.apply_gradients([(grads, z)])\n",
        "            del tape\n",
        "\n",
        "            #self,original_sample, z_search, target_label\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                loss, pred_margin_loss, kde_loss, kde_stop = self.compute_loss(\n",
        "                    x, z,  target_label,kde_stop\n",
        "                )\n",
        "\n",
        "            kde_loss = tf.convert_to_tensor(kde_loss, dtype=tf.float32)\n",
        "            grads = tape.gradient(loss, z)\n",
        "            # Optionally, compute and log gradients for individual components\n",
        "            kde_grads = tape.gradient(kde_loss, z)\n",
        "            margin_grads = tape.gradient(pred_margin_loss, z)\n",
        "\n",
        "\n",
        "            it += 1\n",
        "            #if it % 50 == 0:\n",
        "              #print(f'Currently on iteration: {it}.')\n",
        "            #   # print(f'kde loss : {kde_loss}')\n",
        "            #   # print(f'margin loss : {pred_margin_loss}')\n",
        "            #   # print(f'loss : {loss}')\n",
        "\n",
        "            #   if kde_grads is  None:\n",
        "            #     print(\"KDE Gradients are None.\")\n",
        "            if self.autoencoder is not None:\n",
        "                pred = self.model_(self.decoder_(z))\n",
        "            else:\n",
        "                pred = self.model_(z)\n",
        "\n",
        "        # # uncomment for debug\n",
        "        # print(\n",
        "        #     f\"current loss: {loss}, pred_margin_loss: {pred_margin_loss}, weighted_steps_loss: {weighted_steps_loss}, pred prob:{pred}, iter: {it}. \\n\"\n",
        "        # )\n",
        "\n",
        "        res = z.numpy() if self.autoencoder is None else self.decoder_(z).numpy()\n",
        "        return res, float(loss)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N_E_cHZy4_rW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " reset_seeds()\n",
        "\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.9,\n",
        "    tolerance=1e-6,\n",
        "    max_iter=500,\n",
        "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
        "    autoencoder = autoencoder,\n",
        "    margin_weight=0.6,\n",
        "    random_state= RANDOM_STATE,\n",
        "    bandwidth = 0.9,\n",
        "    weighted_steps_weight = 0.2,\n",
        "    data = X_train[y_train_classes == 1],\n",
        "    step_weights = step_weights\n",
        "    )\n",
        "cf_model.fit(cnnClassifier)\n",
        "\n",
        "\n",
        "y_neg = y_classes[y_classes == 0][10:15]\n",
        "X_neg = X[y_classes == 0][10:15]\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    cf_embeddings, losses, weights = cf_model.transform(x = X_neg,pred_labels = y_neg)\n"
      ],
      "metadata": {
        "id": "f-Xy39aj8q9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237aa08a-3899-4378-8eac-c3229844768e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 samples been transformed.\n",
            "5 samples been transformed, in total.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total = 0\n",
        "probability = 0.5\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = cnnClassifier.predict(counterfactual)[:, 1]\n",
        "    dist = (prediction - probability)\n",
        "    total +=dist\n",
        "mean_mse = total /cf_embeddings.shape[0]\n"
      ],
      "metadata": {
        "id": "ZkPSSHScmi2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654800f0-71a0-405a-a1bf-f9373b83fa48"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 65ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "id": "6FXZX1A-5Er1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031f368c-245d-434c-b336-50fc7249665b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mean MSE of the data is: [0.40215817] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total = 0\n",
        "probability = 0.5\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = cnnClassifier.predict(counterfactual)[:, 1]\n",
        "    dist = abs(prediction - probability)\n",
        "    total +=dist\n",
        "mean_mse = total /cf_embeddings.shape[0]\n"
      ],
      "metadata": {
        "id": "AcUwDKnZmj6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e900358-d54c-47a2-f3aa-b1b681dce8b2"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Absolute Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "id": "1x5s2_8H5F3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845a7a56-5763-4e31-8bca-cbd5d8943ca3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Absolute Mean MSE of the data is: [0.40215817] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Proximity\n",
        "def euclidean_distance(X, cf_samples):\n",
        "    paired_distances = np.linalg.norm(X - cf_samples, axis=1)\n",
        "    return np.mean(paired_distances)\n",
        "euclidean_distance(X_neg, cf_embeddings)"
      ],
      "metadata": {
        "id": "50Hed3JXrm1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e714139f-a8e5-4fa3-ab74-e6a114f594d4"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6448525703353386"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_paddings(cf_samples, padding_size):\n",
        "    if padding_size != 0:\n",
        "        # use np.squeeze() to cut the last time-series dimension, for evaluation\n",
        "        cf_samples = np.squeeze(cf_samples[:, :-padding_size, :])\n",
        "    else:\n",
        "        cf_samples = np.squeeze(cf_samples)\n",
        "    return cf_samples"
      ],
      "metadata": {
        "id": "9MPQoXMjrFmn"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove paddings because KDE does not work with paddings.\n",
        "\n",
        "X_unpadded = remove_paddings(X, padding_size)\n",
        "cf_embeddings_unpadded = remove_paddings(cf_embeddings, padding_size)"
      ],
      "metadata": {
        "id": "dh_BVpXMrtXu"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "diffrences_from_abnormal = []\n",
        "diffrences_from_normal = []\n",
        "for dimention in range(cf_embeddings.shape[2]):\n",
        "\n",
        "\n",
        "  abnormal_data = X_unpadded[y_classes == 1][:,:,dimention]\n",
        "  normal_data = X_unpadded[y_classes == 0][:,:,dimention]\n",
        "  counterf_data = cf_embeddings_unpadded[:,:,dimention]\n",
        "\n",
        "  #get the kernel for every dimention of the trained\n",
        "  kernel = gaussian_kde(abnormal_data.T,bw_method=None)\n",
        "\n",
        "  #get all the log likelihoods\n",
        "  log_likelihood_abnormal = np.mean(kernel.logpdf(abnormal_data.T))\n",
        "  log_likelihood_normal = np.mean(kernel.logpdf(normal_data.T))\n",
        "  log_likelihood_counterfactual = np.mean(kernel.logpdf(counterf_data.T))\n",
        "\n",
        "  #get the diffrences from the counterfactuals\n",
        "  diff_from_abnormal = abs(log_likelihood_counterfactual-log_likelihood_abnormal)\n",
        "  diffrences_from_abnormal.append(diff_from_abnormal)\n",
        "\n",
        "  diff_from_normal = abs(log_likelihood_counterfactual-log_likelihood_normal)\n",
        "  diffrences_from_normal.append(diff_from_normal)\n",
        "\n"
      ],
      "metadata": {
        "id": "9Bb5nfXtOE4O"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(diffrences_from_normal)"
      ],
      "metadata": {
        "id": "9d4LPz8jhBve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4683cae4-80cd-4211-dce7-44b4ba30f3d8"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1076.4258810496356, 632.6407400518326, 8510.071902710826, 5111.393156827533, 2476.7147641824363, 1952.7463143285968, 141.86587722982554, 1928.0343107536787, 12087.434124526108, 184.42471885536668, 4746.913136206193, 5516.880223843746, 805.6315955330272, 747.9257510542449, 1270.7877440398945, 495.4646229533788, 505.262575753646, 7876.174417824614, 1681.591522011248, 7251.817708637753, 4540.7876026162785, 3146.961606595761, 5755.674321371489, 7398.825528535689, 1159.3880966070146, 2093.4056619553394, 16.87152900392951, 585.9892269324123, 15234.0268033617, 3276.857080389429, 561.7881412305331, 1252.8554252859124, 575.2514448771453, 189.3939856900016, 2337.2498792699657, 941.2139715105895, 291.39680750094936, 932.7408532130014, 3136.9695923374734, 718.090850687471, 5419.6152040117595, 2400.0578584295663, 6276.667277808239, 380.7736960846805, 2675.302401870868, 467.86894476334606, 706.697062578492, 16702.705988255755, 4795.8093983154395, 4241.709274902543, 4152.059401676563, 296.48734738215495, 6434.980751272845, 1040.5940213239746, 280.7600154398688, 111.3419932876231, 770.0553568149674, 547.5012448320002, 692.534606611219, 2507.823520324971, 1485.8788766385242, 231.38537135383586, 2937.8200635581265, 9155.29829061113, 86.33363679968173, 4371.582729072278, 16440.03464211604, 1924.8852208887965, 5635.076027501191, 1099.3083603692814, 2215.1435001513096, 4680.01733379366, 6112.090851813827, 279.2902980253509, 4156.26750961243, 1066.4238551946087, 1275.1996168242001, 5667.709598336988, 115.55542639003289, 209.5510972759513, 1046.4317402062434, 1263.45332670257, 2673.6800951067626, 1505.694175344127, 346.5822979359324, 58.2830801618917, 6933.1949965135245, 1557.9683126002021, 1672.0080395983655, 3032.602091652391, 1157.9021161189419, 139.08043609175218, 2518.31620742683, 7324.686138687094, 2344.381001882791, 3902.9507630487788, 1246.1230008737396, 1698.93644936084, 798.1812755959656, 8.730798876781563, 148.87450294452947, 608.6979168047008, 126.63435436324735, 291.6175990024308, 2357.769577457925, 635.5524370160251, 386.58833034504215, 1714.4981635524284, 496.57913786518816, 246.88632543398484, 916.8217774764406, 441.249744039661, 29.01239003653997, 2036.8644206238434, 46.60949122196281, 613.8747117262126, 593.3544396076662, 1367.1022092189346, 139.88842527573198, 1441.0302529205553, 661.3653316115226, 615.5916857625467, 6095.219886787582, 278.46094026809067, 483.289815676481, 575.6151540773254, 369.8285505092398, 895.1708658879197, 1476.7757888946164, 138.07702774303175, 83.81533049057705, 335.3044093128111, 48.83763607912249, 760.9761074245466, 1250.9852852968934, 1278.0554153894682, 194.57986154458956, 504.4140089363963, 89.83482016758661, 142.81290075115527, 2151.426366328511, 6.577482503264022, 95.63442859084975, 12.461782243923125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(diffrences_from_abnormal)"
      ],
      "metadata": {
        "id": "bWvnGPiMi1Yt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68262ed2-094f-46b4-d089-6e7668d42ac4"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1117.3704961924477, 675.1926595851797, 8553.469636715894, 5152.302189654867, 2516.2198751980027, 1996.5207797559553, 182.37417977836276, 1968.103466479659, 12126.430720515651, 225.54895750105322, 4790.818519994633, 5556.606816481825, 845.0687632304015, 787.8065427164802, 1312.0903637409301, 531.6483251292489, 540.9319058510091, 7913.408514981677, 1722.228763306334, 7290.153735988465, 4580.492994396218, 3188.517634665404, 5794.905346022146, 7439.796694888243, 1199.7080092141819, 2131.716706721613, 56.235574862724064, 627.6066541085481, 15271.534226407006, 3317.438966667803, 602.481902296517, 1290.6908606184859, 611.9935764036103, 229.43598726407362, 2375.6516437652263, 980.8352232558573, 328.2943264769707, 969.1868686394724, 3172.525557300211, 755.0450715741853, 5457.657301707458, 2440.332819998314, 6313.503780987749, 416.489903350249, 2712.101248444873, 508.55117725631624, 745.1308528842416, 16742.820002612374, 4837.461142039627, 4278.4028962721995, 4190.991141939416, 337.11765132563414, 6473.161884586565, 1078.825157110991, 322.2899396128756, 149.02624087890297, 808.7579474521052, 587.9517072849586, 730.5321044781426, 2547.8857285807967, 1522.9752537488475, 268.49396585889923, 2977.0347379958857, 9190.917171682106, 123.09111110555554, 4407.698674529458, 16480.996566004935, 1961.9485156100577, 5673.957605995805, 1138.193802364825, 2252.318413691535, 4719.307404847628, 6148.465283085778, 315.77106063292644, 4193.521534044281, 1103.3371881326643, 1312.431407372646, 5704.51784532922, 157.1067889083444, 249.76554764092856, 1085.43950178678, 1302.808461727123, 2710.966774298393, 1544.2838297179444, 385.8786610848278, 100.07588348211075, 6972.463944458128, 1593.9037580074723, 1709.9312417332173, 3074.1875268575345, 1197.6945941414015, 176.5659768930658, 2559.700238029264, 7361.05725062385, 2380.1612507120426, 3943.473897892876, 1283.0984678314314, 1733.9155786363024, 837.862046796199, 48.04439182937895, 190.29902239236898, 648.6631820021547, 163.1311004580616, 328.5368235403199, 2396.0876637131614, 671.763228265143, 423.92635035945193, 1751.1103726924616, 535.8630476774404, 284.71911457118676, 957.3840096859135, 482.9323832097769, 71.14892516171048, 2077.17847566476, 89.01273923452597, 655.6044103959275, 634.5794552561564, 1409.9910852150047, 182.04238104957074, 1481.8207384185562, 703.5903402960453, 657.6810875835084, 6136.028000959529, 321.2556463843425, 524.0813150196525, 617.3602921816301, 412.9480921169744, 938.5638948615087, 1519.2190646142694, 179.96370968452817, 125.05486561245095, 376.3943956588729, 90.5346490131589, 802.8239442573877, 1294.5758743126162, 1319.2434909240428, 238.29373870815226, 547.3661978494561, 132.3098980744207, 184.8960267694186, 2193.8779916101303, 49.22189648127298, 138.4418883460191, 54.71114415328293]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(diffrences_from_normal))"
      ],
      "metadata": {
        "id": "vxNyE5DOkgIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9060070e-51e3-44f8-e85e-3edc2d2c7c2c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2247.263476407336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(diffrences_from_abnormal))"
      ],
      "metadata": {
        "id": "_YUPomtilE7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6229803f-50c0-4853-a008-e794e9b54602"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2286.84010261603\n"
          ]
        }
      ]
    }
  ]
}