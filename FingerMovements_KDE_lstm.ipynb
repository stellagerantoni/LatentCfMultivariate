{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellagerantoni/LatentCfMultivariate/blob/main/FingerMovements_KDE_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/stellagerantoni/LatentCfMultivariate"
      ],
      "metadata": {
        "id": "1fwOXGEl_ine"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wildboar\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q stumpy\n",
        "!pip install -q fastdtw"
      ],
      "metadata": {
        "id": "89L3kts7CCan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aeon"
      ],
      "metadata": {
        "id": "qZh6v946Bhq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from argparse import ArgumentParser\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wildboar.datasets import load_dataset\n",
        "from wildboar.ensemble import ShapeletForestClassifier\n",
        "from wildboar.explain.counterfactual import counterfactuals\n",
        "%cd '/content/LatentCfMultivariate'\n",
        "from _guided import ModifiedLatentCF\n",
        "from help_functions import *\n",
        "from keras_models import *"
      ],
      "metadata": {
        "id": "Bdkpan5lCGRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "RANDOM_STATE = 39"
      ],
      "metadata": {
        "id": "GJE1AxFnE51S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset):\n",
        "  X, y = load_classification(dataset)\n",
        "  if dataset == 'FingerMovements':\n",
        "    pos = 'left'\n",
        "    neg = 'right'\n",
        "\n",
        "\n",
        "  print(\" Shape of X = \", X.shape)\n",
        "  print(\" Shape of y = \", y.shape)\n",
        "  #print(\" Meta data = \", meta_data)\n",
        "  # Convert positive and negative labels to 1 and 0\n",
        "  pos_label, neg_label = 1, 0\n",
        "  if pos != pos_label:\n",
        "      y[y==pos] = pos_label # convert/normalize positive label to 1\n",
        "  if neg != neg_label:\n",
        "      y[y==neg] = neg_label # convert negative label to 0\n",
        "\n",
        "  y = y.astype(int)\n",
        "  print(f\"\\n X[:1] = \\n{X[:1]}\")\n",
        "  return X,y,pos_label, neg_label"
      ],
      "metadata": {
        "id": "1gOYu412Vx7m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACTUALL CODE**\n"
      ],
      "metadata": {
        "id": "6vVfmpyuZyC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 39\n",
        "X,y,pos_label,neg_label = load_dataset('FingerMovements')\n",
        "X = X.transpose(0,2,1)\n",
        "print(f'shape of X = {X.shape}')\n",
        "print(f'shape of y = {y.shape}')\n",
        "#print(f'data imformation = {data_information}')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
        "print(f'shape of X train = {X_train.shape}')\n",
        "print(f'shape of y train = {y_train.shape}')"
      ],
      "metadata": {
        "id": "W4m9pwqyVY1b",
        "outputId": "7f5067c7-7bed-443c-989c-437b963d3d75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X =  (416, 28, 50)\n",
            " Shape of y =  (416,)\n",
            "\n",
            " X[:1] = \n",
            "[[[41.8 44.8 47.1 ... 69.8 72.6 76.1]\n",
            "  [55.2 53.8 59.9 ... 17.5 28.  12.1]\n",
            "  [-8.6 -3.6 14.4 ... 23.3 35.9 23.2]\n",
            "  ...\n",
            "  [16.9 24.5 24.5 ... 51.9 59.6 57.3]\n",
            "  [42.2 35.  41.7 ... 51.5 58.5 46.9]\n",
            "  [13.  26.6 52.5 ... -3.5 -3.2 -2.6]]]\n",
            "shape of X = (416, 50, 28)\n",
            "shape of y = (416,)\n",
            "shape of X train = (332, 50, 28)\n",
            "shape of y train = (332,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upsample the minority class\n",
        "\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'before: {class_counts}')\n",
        "X_train,y_train = upsample_minority_multivariate(X_train,y_train)\n",
        "X,y = upsample_minority_multivariate(X, y)\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'after: {class_counts}')"
      ],
      "metadata": {
        "id": "Q2v7QdrHieA8",
        "outputId": "6e0299f4-572e-475c-c249-e3b827ac7284",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: [166 166]\n",
            "after: [166 166]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing and Padding all our data\n",
        "#Padding needed for autoencoder\n",
        "\n",
        "n_training,n_timesteps, n_features= X_train.shape\n",
        "\n",
        "X, trained_scaler =  normalize_multivariate(data=X, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_train_processed, trained_scaler =  normalize_multivariate(data=X_train, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_test_processed, _ =  normalize_multivariate(data=X_test, n_timesteps=n_timesteps, scaler=trained_scaler, n_features = n_features)\n",
        "\n",
        "X, padding_size = conditional_pad_multivariate(X)\n",
        "X_train_processed_padded, padding_size = conditional_pad_multivariate(X_train_processed) # add extra padding zeros if n_timesteps cannot be divided by 4, required for 1dCNN autoencoder structure\n",
        "X_test_processed_padded, _ = conditional_pad_multivariate(X_test_processed)\n",
        "\n",
        "n_timesteps_padded = X_train_processed_padded.shape[1]\n",
        "print(f\"Data pre-processed, original #timesteps={n_timesteps}, padded #timesteps={n_timesteps_padded}.\")\n",
        "\n",
        "#check the processing (0,1) min should be min 0 and max should be max 1\n",
        "print(f\"\\nmin value = {np.min(X_train)}, max value = {np.max(X_train)}\")\n",
        "print(f\"min value normalized = {np.min(X_train_processed)}, max value normalized= {np.max(X_train_processed)}\")\n",
        "\n",
        "#check that padding paddes the right dimention\n",
        "print(f\"\\nX_train.shape = {X_train.shape}\" )\n",
        "print(f\"X_train_processed_padded.shape = {X_train_processed_padded.shape}\")\n"
      ],
      "metadata": {
        "id": "00Q9QjKy7wEZ",
        "outputId": "9679f02f-3dc8-4047-e66d-08fac4f968d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pre-processed, original #timesteps=50, padded #timesteps=52.\n",
            "\n",
            "min value = -132.1, max value = 205.1\n",
            "min value normalized = 0.0, max value normalized= 1.0\n",
            "\n",
            "X_train.shape = (332, 50, 28)\n",
            "X_train_processed_padded.shape = (332, 52, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_validation, y_train, y_validation = train_test_split(X_train_processed_padded, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train)"
      ],
      "metadata": {
        "id": "1mYFZmsvtB9q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the two forms of labels needed\n",
        "#-the y_classes (1,0,1,0,...)\n",
        "#-the y (one hot encoded)\n",
        "\n",
        "print(f'X_train = {X_train.shape}')\n",
        "print(f'X_validation = {X_validation.shape}')\n",
        "print(f'X_test = {X_test.shape}')\n",
        "\n",
        "y_classes = y\n",
        "y_train_classes = y_train\n",
        "y_validation_classes = y_validation\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, len(np.unique(y)))\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_validation = to_categorical(y_validation, len(np.unique(y_validation)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
        "\n",
        "print(f'\\ny_train_classes = {y_train_classes.shape}, y_validation_classes = {y_validation_classes.shape}, y_test_classes = {y_test_classes.shape}')\n",
        "print(f'y_train = {y_train.shape}, y_validation = {y_validation.shape}, y_test= {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9b2F-vytxzU",
        "outputId": "9b6d78ac-df94-42c8-9c8c-8ace27b69f89"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train = (265, 52, 28)\n",
            "X_validation = (67, 52, 28)\n",
            "X_test = (84, 50, 28)\n",
            "\n",
            "y_train_classes = (265,), y_validation_classes = (67,), y_test_classes = (84,)\n",
            "y_train = (265, 2), y_validation = (67, 2), y_test= (84, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ClassifierLSTM(n_timesteps, n_features, extra_lstm_layer=True, n_output=1):\n",
        "    # Define the model structure - only LSTM layers\n",
        "    # https://www.kaggle.com/szaitseff/classification-of-time-series-with-lstm-rnn\n",
        "    inputs = keras.Input(shape=(n_timesteps, n_features), dtype=\"float32\")\n",
        "    if extra_lstm_layer:\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(\n",
        "            inputs\n",
        "        )  # set return_sequences true to feed next LSTM layer\n",
        "    else:\n",
        "        x = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=False)(\n",
        "            inputs\n",
        "        )  # set return_sequences false to feed dense layer directly\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    # x = keras.layers.LSTM(32, activation='tanh', return_sequences=True)(x)\n",
        "    # x = keras.layers.BatchNormalization()(x)\n",
        "    if extra_lstm_layer:\n",
        "        x = keras.layers.LSTM(16, activation=\"tanh\", return_sequences=False)(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier2 = keras.Model(inputs, outputs)\n",
        "\n",
        "    return classifier2"
      ],
      "metadata": {
        "id": "P6DnTUiGHt_j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ## LatentCF++ models\n",
        "# reset seeds for numpy, tensorflow, python random package and python environment seed\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ### LSTM classifier\n",
        "\n",
        "classifier = ClassifierLSTM(\n",
        "    n_timesteps_padded, n_features, n_output=2\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "classifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=15, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM-FCN classifier:\")\n",
        "classifier_history = classifier.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=True,\n",
        "    validation_data=(X_validation, y_validation),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = classifier.predict(X_test_processed_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM-FCN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:1\", \"True:0\"],\n",
        "    columns=[\"Pred:1\", \"Pred:0\"],\n",
        ")\n",
        "print(confusion_matrix_df)\n"
      ],
      "metadata": {
        "id": "yNkKTXe6IIyF",
        "outputId": "002245e0-766a-4040-9e22-7c9bc1d1a7ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM-FCN classifier:\n",
            "Epoch 1/150\n",
            "23/23 [==============================] - 13s 223ms/step - loss: 0.7501 - accuracy: 0.5509 - val_loss: 0.6920 - val_accuracy: 0.4925\n",
            "Epoch 2/150\n",
            "23/23 [==============================] - 2s 75ms/step - loss: 0.6782 - accuracy: 0.6038 - val_loss: 0.6893 - val_accuracy: 0.5224\n",
            "Epoch 3/150\n",
            "23/23 [==============================] - 2s 100ms/step - loss: 0.6500 - accuracy: 0.6151 - val_loss: 0.6923 - val_accuracy: 0.4925\n",
            "Epoch 4/150\n",
            "23/23 [==============================] - 2s 83ms/step - loss: 0.6365 - accuracy: 0.6151 - val_loss: 0.6883 - val_accuracy: 0.6866\n",
            "Epoch 5/150\n",
            "23/23 [==============================] - 2s 80ms/step - loss: 0.5749 - accuracy: 0.7396 - val_loss: 0.6800 - val_accuracy: 0.5373\n",
            "Epoch 6/150\n",
            "23/23 [==============================] - 2s 81ms/step - loss: 0.5768 - accuracy: 0.7057 - val_loss: 0.6937 - val_accuracy: 0.5373\n",
            "Epoch 7/150\n",
            "23/23 [==============================] - 3s 110ms/step - loss: 0.5524 - accuracy: 0.7434 - val_loss: 0.6855 - val_accuracy: 0.5522\n",
            "Epoch 8/150\n",
            "23/23 [==============================] - 2s 88ms/step - loss: 0.5770 - accuracy: 0.7208 - val_loss: 0.6866 - val_accuracy: 0.4776\n",
            "Epoch 9/150\n",
            "23/23 [==============================] - 2s 89ms/step - loss: 0.5450 - accuracy: 0.7509 - val_loss: 0.6802 - val_accuracy: 0.5821\n",
            "Epoch 10/150\n",
            "23/23 [==============================] - 2s 73ms/step - loss: 0.5317 - accuracy: 0.7698 - val_loss: 0.8076 - val_accuracy: 0.5224\n",
            "Epoch 11/150\n",
            "23/23 [==============================] - 1s 39ms/step - loss: 0.5171 - accuracy: 0.7547 - val_loss: 0.6700 - val_accuracy: 0.6418\n",
            "Epoch 12/150\n",
            "23/23 [==============================] - 1s 37ms/step - loss: 0.5499 - accuracy: 0.7019 - val_loss: 0.6437 - val_accuracy: 0.6866\n",
            "Epoch 13/150\n",
            "23/23 [==============================] - 1s 36ms/step - loss: 0.4882 - accuracy: 0.7472 - val_loss: 0.6659 - val_accuracy: 0.6269\n",
            "Epoch 14/150\n",
            "23/23 [==============================] - 1s 37ms/step - loss: 0.4777 - accuracy: 0.7774 - val_loss: 0.7101 - val_accuracy: 0.5970\n",
            "Epoch 15/150\n",
            "23/23 [==============================] - 1s 37ms/step - loss: 0.4519 - accuracy: 0.7925 - val_loss: 0.6127 - val_accuracy: 0.6716\n",
            "Epoch 16/150\n",
            "23/23 [==============================] - 1s 45ms/step - loss: 0.5031 - accuracy: 0.7660 - val_loss: 0.9638 - val_accuracy: 0.5075\n",
            "Epoch 17/150\n",
            "23/23 [==============================] - 1s 64ms/step - loss: 0.4751 - accuracy: 0.7887 - val_loss: 0.7293 - val_accuracy: 0.5821\n",
            "Epoch 18/150\n",
            "23/23 [==============================] - 1s 43ms/step - loss: 0.4522 - accuracy: 0.8151 - val_loss: 1.1078 - val_accuracy: 0.5373\n",
            "Epoch 19/150\n",
            "23/23 [==============================] - 1s 39ms/step - loss: 0.4995 - accuracy: 0.7774 - val_loss: 1.4017 - val_accuracy: 0.5075\n",
            "3/3 [==============================] - 1s 18ms/step\n",
            "LSTM-FCN classifier trained, with validation accuracy 0.4761904761904762.\n",
            "        Pred:1  Pred:0\n",
            "True:1       9      33\n",
            "True:0      11      31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def AutoencoderLSTM(n_timesteps, n_features):\n",
        "    # Define encoder and decoder structure\n",
        "    # structure from medium post: https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
        "    def EncoderLSTM(input):\n",
        "        # x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(input)\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(input)\n",
        "        # encoded = keras.layers.LSTM(32, activation='relu', return_sequences=False)(x)\n",
        "        encoded = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=False)(x)\n",
        "        return encoded\n",
        "\n",
        "    def DecoderLSTM(encoded):\n",
        "        x = keras.layers.RepeatVector(n_timesteps)(encoded)\n",
        "        # x = keras.layers.LSTM(32, activation='relu', return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=True)(x)\n",
        "        # x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(x)\n",
        "        decoded = keras.layers.TimeDistributed(\n",
        "            keras.layers.Dense(n_features, activation=\"sigmoid\")\n",
        "        )(x)\n",
        "        return decoded\n",
        "\n",
        "    # Define the AE model\n",
        "    orig_input2 = keras.Input(shape=(n_timesteps, n_features))\n",
        "\n",
        "    autoencoder2 = keras.Model(\n",
        "        inputs=orig_input2, outputs=DecoderLSTM(EncoderLSTM(orig_input2))\n",
        "    )\n",
        "\n",
        "    return autoencoder2"
      ],
      "metadata": {
        "id": "p_VffRwwIHFu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "\n",
        "# ### LSTM autoencoder\n",
        "autoencoder = AutoencoderLSTM( n_timesteps_padded,n_features)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoder.fit(\n",
        "    X_train,\n",
        "    X_train,\n",
        "    epochs=50,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_validation, X_validation),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")\n"
      ],
      "metadata": {
        "id": "E6IxH8BLEKFG",
        "outputId": "73765028-56bf-4944-fed1-e86edd55b6f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "23/23 - 11s - loss: 0.0295 - val_loss: 0.0232 - 11s/epoch - 477ms/step\n",
            "Epoch 2/50\n",
            "23/23 - 2s - loss: 0.0227 - val_loss: 0.0196 - 2s/epoch - 78ms/step\n",
            "Epoch 3/50\n",
            "23/23 - 2s - loss: 0.0189 - val_loss: 0.0180 - 2s/epoch - 67ms/step\n",
            "Epoch 4/50\n",
            "23/23 - 2s - loss: 0.0174 - val_loss: 0.0163 - 2s/epoch - 66ms/step\n",
            "Epoch 5/50\n",
            "23/23 - 2s - loss: 0.0163 - val_loss: 0.0149 - 2s/epoch - 68ms/step\n",
            "Epoch 6/50\n",
            "23/23 - 2s - loss: 0.0148 - val_loss: 0.0139 - 2s/epoch - 66ms/step\n",
            "Epoch 7/50\n",
            "23/23 - 2s - loss: 0.0131 - val_loss: 0.0143 - 2s/epoch - 66ms/step\n",
            "Epoch 8/50\n",
            "23/23 - 2s - loss: 0.0125 - val_loss: 0.0133 - 2s/epoch - 79ms/step\n",
            "Epoch 9/50\n",
            "23/23 - 2s - loss: 0.0127 - val_loss: 0.0128 - 2s/epoch - 98ms/step\n",
            "Epoch 10/50\n",
            "23/23 - 2s - loss: 0.0114 - val_loss: 0.0096 - 2s/epoch - 70ms/step\n",
            "Epoch 11/50\n",
            "23/23 - 2s - loss: 0.0097 - val_loss: 0.0092 - 2s/epoch - 73ms/step\n",
            "Epoch 12/50\n",
            "23/23 - 2s - loss: 0.0090 - val_loss: 0.0131 - 2s/epoch - 67ms/step\n",
            "Epoch 13/50\n",
            "23/23 - 2s - loss: 0.0099 - val_loss: 0.0192 - 2s/epoch - 67ms/step\n",
            "Epoch 14/50\n",
            "23/23 - 2s - loss: 0.0131 - val_loss: 0.0111 - 2s/epoch - 67ms/step\n",
            "Epoch 15/50\n",
            "23/23 - 2s - loss: 0.0094 - val_loss: 0.0099 - 2s/epoch - 81ms/step\n",
            "Epoch 16/50\n",
            "23/23 - 2s - loss: 0.0082 - val_loss: 0.0128 - 2s/epoch - 101ms/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.009215304628014565.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gettting the Global weights, needed for counterfactuals\n",
        "\n",
        "from _guided import get_global_weights\n",
        "from help_functions import evaluate\n",
        "pos_label = 1\n",
        "neg_label = 0\n",
        "\n",
        "step_weights = get_global_weights(\n",
        "        X,\n",
        "        y_classes,\n",
        "        classifier,\n",
        "        n_timesteps= n_timesteps,\n",
        "        n_features=n_features,\n",
        "        random_state=RANDOM_STATE,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hysd9dxSsx9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_weights"
      ],
      "metadata": {
        "id": "NkOmFZIBTCsK",
        "outputId": "c03ed63a-60a4-43a9-d815-3a5e0cd465e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from wildboar.explain import IntervalImportance\n",
        "#from LIMESegment.Utils.explanations import LIMESegment\n",
        "\n",
        "\n",
        "class ModifiedLatentCF:\n",
        "    \"\"\"Explanations by generating a counterfacutal sample in the latent space of\n",
        "    any autoencoder.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Learning Time Series Counterfactuals via Latent Space Representations,\n",
        "    Wang, Z., Samsten, I., Mochaourab, R., Papapetrou, P., 2021.\n",
        "    in: International Conference on Discovery Science, pp. 369–384. https://doi.org/10.1007/978-3-030-88942-5_29\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        probability=0.5,\n",
        "        *,\n",
        "        tolerance=1e-6,\n",
        "        max_iter=100,\n",
        "        optimizer=None,\n",
        "        autoencoder=None,\n",
        "        margin_weight=1.0,  # weighted_steps_weight = 1 - pred_margin_weight\n",
        "        random_state=None,\n",
        "        bandwidth,\n",
        "        weighted_steps_weight,\n",
        "        data,\n",
        "        step_weights\n",
        "    ):\n",
        "        self.optimizer_ = (\n",
        "            tf.optimizers.Adam(learning_rate=1e-4) if optimizer is None else optimizer\n",
        "        )\n",
        "        #self.x_axis_eights = x_axis_eights\n",
        "        #self.y_axis_eights = y_axis_eights\n",
        "        self.data = data\n",
        "        self.mse_loss_ = keras.losses.MeanSquaredError()\n",
        "        self.probability_ = tf.constant([probability])\n",
        "        self.tolerance_ = tf.constant(tolerance)\n",
        "        self.max_iter = max_iter\n",
        "        self.autoencoder = autoencoder\n",
        "        self.random_state = random_state\n",
        "        self.weighted_steps_weight = weighted_steps_weight\n",
        "        self.step_weights = step_weights\n",
        "\n",
        "        # Weights of the different loss components\n",
        "        self.margin_weight = margin_weight\n",
        "        self.kde_weight = tf.cast(1 - self.margin_weight-self.weighted_steps_weight,tf.float32)\n",
        "        if (self.weighted_steps_weight+self.margin_weight)>1.0:\n",
        "           raise ValueError(\"(weighted_steps_weight + margin_weight) should be less that 1.0\")\n",
        "\n",
        "        self.bandwidth = bandwidth\n",
        "\n",
        "    def fit(self, model):\n",
        "        \"\"\"Fit a new counterfactual explainer to the model\n",
        "\n",
        "        Paramaters\n",
        "        ----------\n",
        "\n",
        "        model : keras.Model\n",
        "            The model\n",
        "        \"\"\"\n",
        "        if self.autoencoder:\n",
        "            (\n",
        "                encode_input,\n",
        "                encode_output,\n",
        "                decode_input,\n",
        "                decode_output,\n",
        "            ) = extract_encoder_decoder(self.autoencoder)\n",
        "            self.decoder_ = keras.Model(inputs=decode_input, outputs=decode_output)\n",
        "            self.encoder_ = keras.Model(inputs=encode_input, outputs=encode_output)\n",
        "        else:\n",
        "            self.decoder_ = None\n",
        "            self.encoder_ = None\n",
        "        self.model_ = model\n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Compute the difference between the desired and actual probability\n",
        "\n",
        "        Parameters\n",
        "        ---------\n",
        "        x : Variable\n",
        "            Variable of the sample\n",
        "        \"\"\"\n",
        "        if self.autoencoder is None:\n",
        "            z = x\n",
        "        else:\n",
        "            z = self.decoder_(x)\n",
        "\n",
        "        return self.model_(z)\n",
        "\n",
        "    # The \"pred_margin_loss\" is designed to measure the prediction probability to the desired decision boundary\n",
        "    def pred_margin_mse(self, prediction):\n",
        "        return self.mse_loss_(self.probability_, prediction)\n",
        "\n",
        "    # An auxiliary MAE loss function to measure the proximity with step_weights\n",
        "    def weighted_mae(self, original_sample, cf_sample, step_weights):\n",
        "        return tf.math.reduce_mean(\n",
        "            tf.math.multiply(tf.math.abs(original_sample - cf_sample), step_weights)\n",
        "        )\n",
        "\n",
        "    # An auxiliary normalized L2 loss function to measure the proximity with step_weights\n",
        "    def weighted_normalized_l2(self, original_sample, cf_sample, step_weights):\n",
        "        var_diff = tf.math.reduce_variance(original_sample - cf_sample)\n",
        "        var_orig = tf.math.reduce_variance(original_sample)\n",
        "        var_cf = tf.math.reduce_variance(cf_sample)\n",
        "\n",
        "        normalized_l2 = 0.5 * var_diff / (var_orig + var_cf)\n",
        "        return tf.math.reduce_mean(\n",
        "            tf.math.multiply(\n",
        "                normalized_l2,\n",
        "                step_weights,\n",
        "            )\n",
        "        )\n",
        "    #x_axis_eights*self.step_weights_x,use_scotts_rule=False,use_silvermans_rule=True,manual_bandwidth = 0.1\n",
        "    def train_gaussian_kde(self, data, use_scotts_rule,use_silvermans_rule, manual_bandwidth ):\n",
        "      \"\"\"\n",
        "      Train a Gaussian KDE on the provided data.\n",
        "\n",
        "      :param data: Multivariate data points used for KDE (2D Tensor).\n",
        "      :param bandwidth: The bandwidth of the kernel (float).\n",
        "      :return: A function that represents the trained KDE.\n",
        "      \"\"\"\n",
        "\n",
        "      n = tf.cast(tf.shape(data)[0], tf.float32)\n",
        "      d = tf.cast(tf.shape(data)[1], tf.float32)\n",
        "      def kde_fn( x_points):\n",
        "        \"\"\"\n",
        "        Compute the density estimation for given points using the trained KDE.\n",
        "\n",
        "        :param x_points: Points where the density should be estimated (2D Tensor).\n",
        "        :return: Density estimates (Tensor).\n",
        "        \"\"\"\n",
        "        d = tf.cast(tf.shape(data)[1], tf.float32)\n",
        "        #print(f'data.shape = {data.shape}')\n",
        "        #data.shape = (732, 8)\n",
        "\n",
        "        data_exp =  tf.expand_dims(data, axis=0)\n",
        "        #print(f'data dimentions = {data_exp.shape}')\n",
        "        #data dimentions = (1, 732, 8)\n",
        "\n",
        "        x_points_exp = tf.reshape(x_points, (tf.shape(x_points)[0], 52, 1))\n",
        "        #print(f'x_points dimentions = {x_points_exp.shape}')\n",
        "        #x_points dimentions = (732, 8, 1)\n",
        "\n",
        "        x_points_exp = tf.expand_dims(x_points_exp, axis=1)\n",
        "        #print(f'x_points dimentions = {x_points_exp.shape}')\n",
        "\n",
        "        if use_scotts_rule:\n",
        "          sigma = tf.math.reduce_std(data)\n",
        "          sigma = tf.cast(sigma,tf.float32)\n",
        "          bandwidth = n ** (-1.0 / (d + 4)) * sigma\n",
        "        elif use_silvermans_rule:\n",
        "          sigma = tf.math.reduce_std(data)\n",
        "          bandwidth = (4 * sigma**5 / (3 * n)) ** (1/5)\n",
        "        else:\n",
        "          if manual_bandwidth is None:\n",
        "            raise ValueError(\"Manual bandwidth must be provided if not using Scott's Rule.\")\n",
        "          bandwidth = manual_bandwidth\n",
        "\n",
        "        bandwidth = tf.cast(bandwidth, tf.float32)\n",
        "        #print(f'n = {n}')\n",
        "        #print(f'd = {d}')\n",
        "        #print(f'bandwidth = {bandwidth}')\n",
        "        x_points_exp = tf.cast(x_points_exp,tf.float32)\n",
        "        data_exp= tf.cast(data_exp, tf.float32)\n",
        "        #print(x_points_exp.dtype, data_exp.dtype)\n",
        "        diff = x_points_exp - data_exp\n",
        "        norm = tf.reduce_sum(diff ** 2, axis=2)\n",
        "        kernel_val = tf.exp(-norm / (2.0 * bandwidth ** 2))\n",
        "\n",
        "\n",
        "        d = tf.constant(d,tf.float32)\n",
        "        density = tf.reduce_mean(kernel_val, axis=1) / (bandwidth * tf.sqrt(2.0 * np.pi * d))\n",
        "        return density\n",
        "\n",
        "      return kde_fn\n",
        "\n",
        "    def gaussian_kde_logpdf(self, kde_fn, x_points):\n",
        "      \"\"\"\n",
        "      Evaluate the logpdf of the given points using the trained KDE function.\n",
        "\n",
        "      :param kde_fn: Trained KDE function.\n",
        "      :param x_points: Points to evaluate the logpdf (2D Tensor).\n",
        "      :return: Log of the density estimates (Tensor).\n",
        "      \"\"\"\n",
        "      #print(f'x_points.shape in gaussian kde lofpdf= {x_points.shape}')\n",
        "      density = kde_fn(x_points)\n",
        "      return tf.math.log(density)\n",
        "\n",
        "\n",
        "    # additional input of step_weights\n",
        "    def compute_loss(self,original_sample, z_search, target_label, kde_stop):\n",
        "        loss = tf.zeros(shape=())\n",
        "        decoded = self.decoder_(z_search) if self.autoencoder is not None else z_search\n",
        "        pred = self.model_(decoded)[:, target_label]\n",
        "\n",
        "        #margin loss (y-τ)\n",
        "        margin_loss = self.pred_margin_mse(pred)\n",
        "        loss += self.margin_weight * margin_loss\n",
        "\n",
        "        kde_diffrences = []\n",
        "        for dimention in range(decoded.shape[2]):\n",
        "          data_dimention = self.data[:,:,dimention]\n",
        "          data_dimention = data_dimention[:,:,np.newaxis]\n",
        "          #print(f'data_dimention in compute loss = {data_dimention.shape}')\n",
        "          kde = self.train_gaussian_kde(data = data_dimention,use_scotts_rule=False,use_silvermans_rule=False,manual_bandwidth = self.bandwidth)\n",
        "          mean_log_likelihood = tf.cast(tf.reduce_mean(self.gaussian_kde_logpdf(kde, x_points = (data_dimention))),tf.float32)\n",
        "\n",
        "          decoded_dimention = decoded[:,:,dimention]\n",
        "          decoded_dimention = decoded_dimention[:,:,np.newaxis]\n",
        "          #print(f'decoded_dimention in compute loss = {decoded_dimention.shape}')\n",
        "          log_likelihood_of_sample = tf.cast(self.gaussian_kde_logpdf(kde, x_points = (decoded_dimention)),tf.float32)\n",
        "\n",
        "          # kde_x = self.train_gaussian_kde(data = self.x_axis_eights,use_scotts_rule=False,use_silvermans_rule=False,manual_bandwidth = self.bandwidth)\n",
        "          # kde_y = self.train_gaussian_kde(data = self.y_axis_eights,use_scotts_rule=False,use_silvermans_rule=False,manual_bandwidth = self.bandwidth)\n",
        "          # mean_log_likelihood_ofData_x = tf.cast(tf.reduce_mean(self.gaussian_kde_logpdf(kde_x, x_points = ( self.x_axis_eights))),tf.float32)\n",
        "          # mean_log_likelihood_ofData_y = tf.cast(tf.reduce_mean(self.gaussian_kde_logpdf(kde_y, x_points = ( self.y_axis_eights))),tf.float32)\n",
        "\n",
        "\n",
        "          #The likelihood of the distribution beeing neer to the mean of the distributions of the known data (mean.likelihood(d) - likelihood(sample))\n",
        "          # decoded_x =  decoded [:,:,0:1]\n",
        "          # decoded_y =  decoded [:,:,1:2]\n",
        "          # log_likelihood_ofSample_x = tf.cast(self.gaussian_kde_logpdf(kde_x, x_points = (decoded_x)),tf.float32)\n",
        "          # log_likelihood_ofSample_y = tf.cast(self.gaussian_kde_logpdf(kde_y, x_points = (decoded_y)),tf.float32)\n",
        "          # print(log_likelihood_ofSample_x.dtype)\n",
        "          #print(f'mean_log_likelihood = {mean_log_likelihood}')\n",
        "          #print(f'log_likelihood_of_sample.shape = {log_likelihood_of_sample.shape}')\n",
        "          kde_loss = tf.cast((mean_log_likelihood - log_likelihood_of_sample),tf.float32)\n",
        "          #kde_loss = tf.math.abs(kde_loss)\n",
        "          #print(kde_loss)\n",
        "          kde_diffrences.append(kde_loss)\n",
        "        #print(kde_diffrences)\n",
        "        #kde_total_loss = tf.reduce_mean(kde_diffrences)\n",
        "        kde_total_loss = tf.math.add_n(kde_diffrences)\n",
        "        kde_total_loss = tf.math.abs(kde_total_loss)\n",
        "        #print(kde_total_loss)\n",
        "        loss +=self.kde_weight *kde_total_loss\n",
        "\n",
        "\n",
        "        weighted_steps_loss = self.weighted_mae(\n",
        "            original_sample=tf.cast(original_sample, dtype=tf.float32),\n",
        "            cf_sample=tf.cast(decoded, dtype=tf.float32),\n",
        "            step_weights=tf.cast(step_weights, tf.float32),\n",
        "        )\n",
        "        loss += self.weighted_steps_weight * weighted_steps_loss\n",
        "\n",
        "        return loss, margin_loss, kde_loss, kde_stop\n",
        "\n",
        "    # TODO: compatible with the counterfactuals of wildboar\n",
        "    #       i.e., define the desired output target per label\n",
        "\n",
        "\n",
        "    def transform(self, x, pred_labels):\n",
        "        \"\"\"Generate counterfactual explanations\n",
        "\n",
        "        x : array-like of shape [n_samples, n_timestep, n_dims]\n",
        "            The samples\n",
        "        \"\"\"\n",
        "\n",
        "        result_samples = np.empty(x.shape)\n",
        "        losses = np.empty(x.shape[0])\n",
        "        # `weights_all` needed for debugging\n",
        "        weights_all = np.empty((x.shape[0], 1, x.shape[1], x.shape[2]))\n",
        "        for i in range(x.shape[0]):\n",
        "            print(f\"sample {i+1} is beeing transformed.\")\n",
        "            kde_stop = False\n",
        "\n",
        "            x_sample, loss = self._transform_sample(\n",
        "                x[np.newaxis, i], pred_labels[i],kde_stop\n",
        "            )\n",
        "\n",
        "            result_samples[i] = x_sample\n",
        "            losses[i] = loss\n",
        "            weights_all[i] = step_weights\n",
        "\n",
        "        print(f\"{i+1} samples been transformed, in total.\")\n",
        "\n",
        "        return result_samples, losses, weights_all\n",
        "\n",
        "    def _transform_sample(self, x, pred_label,kde_stop):\n",
        "        \"\"\"Generate counterfactual explanations(z))\"\"\"\n",
        "        # TODO: check_is_fitted(self)\n",
        "        if self.autoencoder is not None:\n",
        "            z = tf.Variable(self.encoder_(x))\n",
        "        else:\n",
        "            z = tf.Variable(x, dtype=tf.float32)\n",
        "\n",
        "        it = 0\n",
        "        target_label = 1 - pred_label  # for binary classification\n",
        "\n",
        "        #def compute_loss(self,original_sample, z_search, target_label)\n",
        "        with tf.GradientTape(persistent = True) as tape:\n",
        "            loss, pred_margin_loss, kde_loss, kde_stop = self.compute_loss(\n",
        "                x, z,  target_label,kde_stop\n",
        "            )\n",
        "        if self.autoencoder is not None:\n",
        "            pred = self.model_(self.decoder_(z))\n",
        "        else:\n",
        "            pred = self.model_(z)\n",
        "\n",
        "        # # uncomment for debug\n",
        "        # print(\n",
        "        #     f\"current loss: {loss}, pred_margin_loss: {pred_margin_loss}, weighted_steps_loss: {weighted_steps_loss}, pred prob:{pred}, iter: {it}.\"\n",
        "        # )\n",
        "\n",
        "        # TODO: modify the loss to check both validity and proximity; how to design the condition here?\n",
        "        # while (pred_margin_loss > self.tolerance_ or pred[:, 1] < self.probability_ or weighted_steps_loss > self.step_tolerance_)?\n",
        "        # loss > tf.multiply(self.tolerance_rate_, loss_original)\n",
        "\n",
        "\n",
        "        while (\n",
        "            (loss > self.tolerance_\n",
        "            and pred[:, target_label] < self.probability_)\n",
        "            and (it < self.max_iter if self.max_iter else True)\n",
        "            ) :\n",
        "            # Get gradients of loss wrt the sample\n",
        "            grads = tape.gradient(loss, z)\n",
        "            # Update the weights of the sample\n",
        "            self.optimizer_.apply_gradients([(grads, z)])\n",
        "            del tape\n",
        "\n",
        "            #self,original_sample, z_search, target_label\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                loss, pred_margin_loss, kde_loss, kde_stop = self.compute_loss(\n",
        "                    x, z,  target_label,kde_stop\n",
        "                )\n",
        "\n",
        "            kde_loss = tf.convert_to_tensor(kde_loss, dtype=tf.float32)\n",
        "            grads = tape.gradient(loss, z)\n",
        "            # Optionally, compute and log gradients for individual components\n",
        "            kde_grads = tape.gradient(kde_loss, z)\n",
        "            margin_grads = tape.gradient(pred_margin_loss, z)\n",
        "\n",
        "\n",
        "            it += 1\n",
        "            # if it % 50 == 0:\n",
        "            #   # print(f'Currently on iteration: {it}.')\n",
        "            #   # print(f'kde loss : {kde_loss}')\n",
        "            #   # print(f'margin loss : {pred_margin_loss}')\n",
        "            #   # print(f'loss : {loss}')\n",
        "\n",
        "            #   if kde_grads is  None:\n",
        "            #     print(\"KDE Gradients are None.\")\n",
        "            if self.autoencoder is not None:\n",
        "                pred = self.model_(self.decoder_(z))\n",
        "            else:\n",
        "                pred = self.model_(z)\n",
        "\n",
        "        # # uncomment for debug\n",
        "        # print(\n",
        "        #     f\"current loss: {loss}, pred_margin_loss: {pred_margin_loss}, weighted_steps_loss: {weighted_steps_loss}, pred prob:{pred}, iter: {it}. \\n\"\n",
        "        # )\n",
        "\n",
        "        res = z.numpy() if self.autoencoder is None else self.decoder_(z).numpy()\n",
        "        return res, float(loss)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "AUB4xHm_4vOP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " reset_seeds()\n",
        "\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.5,\n",
        "    tolerance=1e-6,\n",
        "    max_iter=500,\n",
        "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
        "    autoencoder = autoencoder,\n",
        "    margin_weight=0.7,\n",
        "    random_state= RANDOM_STATE,\n",
        "    bandwidth = 0.9,\n",
        "    weighted_steps_weight = 0.2,\n",
        "    data = X_train[y_train_classes == 1],\n",
        "    step_weights = step_weights\n",
        "    )\n",
        "cf_model.fit(classifier)\n",
        "\n",
        "\n",
        "y_neg = y_classes[y_classes == 0]\n",
        "X_neg = X[y_classes == 0]\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    cf_embeddings, losses, weights = cf_model.transform(x = X_neg,pred_labels = y_neg)\n"
      ],
      "metadata": {
        "id": "f-Xy39aj8q9S",
        "outputId": "5adf4cfa-bc86-4cad-a4e5-4aa040f5d0be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample 1 is beeing transformed.\n",
            "sample 2 is beeing transformed.\n",
            "sample 3 is beeing transformed.\n",
            "sample 4 is beeing transformed.\n",
            "sample 5 is beeing transformed.\n",
            "sample 6 is beeing transformed.\n",
            "sample 7 is beeing transformed.\n",
            "sample 8 is beeing transformed.\n",
            "sample 9 is beeing transformed.\n",
            "sample 10 is beeing transformed.\n",
            "sample 11 is beeing transformed.\n",
            "sample 12 is beeing transformed.\n",
            "sample 13 is beeing transformed.\n",
            "sample 14 is beeing transformed.\n",
            "sample 15 is beeing transformed.\n",
            "sample 16 is beeing transformed.\n",
            "sample 17 is beeing transformed.\n",
            "sample 18 is beeing transformed.\n",
            "sample 19 is beeing transformed.\n",
            "sample 20 is beeing transformed.\n",
            "sample 21 is beeing transformed.\n",
            "sample 22 is beeing transformed.\n",
            "sample 23 is beeing transformed.\n",
            "sample 24 is beeing transformed.\n",
            "sample 25 is beeing transformed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Validity\n",
        "cf_pred = classifier.predict(cf_embeddings)[:, 1] # predicted probabilities of CFs\n",
        "cf_pred_labels = cf_pred\n",
        "for idx in range(cf_pred_labels.shape[0]):\n",
        "  if cf_pred_labels[idx] > 0.5:\n",
        "    cf_pred_labels[idx] = 1\n",
        "  else:\n",
        "    cf_pred_labels[idx] = 0\n",
        "\n",
        "\n",
        "print(f'Transformation_finished with validity_score = {validity_score(y_neg,cf_pred_labels)}')"
      ],
      "metadata": {
        "id": "Ov4igLqcorG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity withour absolute\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total_mse = 0\n",
        "probability = 0.5\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = classifier.predict(counterfactual)[:, 1]\n",
        "    mse = MeanSquaredError()\n",
        "    mse_dist = mse(probability, prediction).numpy()\n",
        "    total_mse +=mse_dist\n",
        "mean_mse = total_mse /cf_embeddings.shape[0]"
      ],
      "metadata": {
        "id": "Nvlack0n40dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "id": "DaGvoFKBjyX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity with absolute\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total = 0\n",
        "probability = 0.5\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = classifier.predict(counterfactual)[:, 1]\n",
        "    dist = abs(prediction - probability)\n",
        "    total +=dist\n",
        "mean_mse = total /cf_embeddings.shape[0]"
      ],
      "metadata": {
        "id": "oK3o9SQB41a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "id": "JmStyUVjj0-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Proximity\n",
        "def euclidean_distance(X, cf_samples):\n",
        "    paired_distances = np.linalg.norm(X - cf_samples, axis=1)\n",
        "    return np.mean(paired_distances)\n",
        "euclidean_distance(X_neg, cf_embeddings)"
      ],
      "metadata": {
        "id": "ZTsPvh1lotsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_paddings(cf_samples, padding_size):\n",
        "    if padding_size != 0:\n",
        "        # use np.squeeze() to cut the last time-series dimension, for evaluation\n",
        "        cf_samples = np.squeeze(cf_samples[:, :-padding_size, :])\n",
        "    else:\n",
        "        cf_samples = np.squeeze(cf_samples)\n",
        "    return cf_samples"
      ],
      "metadata": {
        "id": "9MPQoXMjrFmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove paddings because KDE does not work with paddings.\n",
        "\n",
        "X_unpadded = remove_paddings(X, padding_size)\n",
        "cf_embeddings_unpadded = remove_paddings(cf_embeddings, padding_size)"
      ],
      "metadata": {
        "id": "dh_BVpXMrtXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "diffrences_from_abnormal = []\n",
        "diffrences_from_normal = []\n",
        "for dimention in range(cf_embeddings.shape[2]):\n",
        "\n",
        "\n",
        "  abnormal_data = X_unpadded[y_classes == 1][:,:,dimention]\n",
        "  normal_data = X_unpadded[y_classes == 0][:,:,dimention]\n",
        "  counterf_data = cf_embeddings_unpadded[:,:,dimention]\n",
        "\n",
        "  #get the kernel for every dimention of the trained\n",
        "  kernel = gaussian_kde(abnormal_data.T,bw_method=None)\n",
        "\n",
        "  #get all the log likelihoods\n",
        "  log_likelihood_abnormal = np.mean(kernel.logpdf(abnormal_data.T))\n",
        "  log_likelihood_normal = np.mean(kernel.logpdf(normal_data.T))\n",
        "  log_likelihood_counterfactual = np.mean(kernel.logpdf(counterf_data.T))\n",
        "\n",
        "  #get the diffrences from the counterfactuals\n",
        "  diff_from_abnormal = abs(log_likelihood_counterfactual-log_likelihood_abnormal)\n",
        "  diffrences_from_abnormal.append(diff_from_abnormal)\n",
        "\n",
        "  diff_from_normal = abs(log_likelihood_counterfactual-log_likelihood_normal)\n",
        "  diffrences_from_normal.append(diff_from_normal)\n"
      ],
      "metadata": {
        "id": "7GSjwx8w4liQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(diffrences_from_normal))"
      ],
      "metadata": {
        "id": "BzGfR9Abt_fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(diffrences_from_abnormal))"
      ],
      "metadata": {
        "id": "C6lyK3-Pt_h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffrences_from_normal"
      ],
      "metadata": {
        "id": "G6rd-6JvFLP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffrences_from_abnormal"
      ],
      "metadata": {
        "id": "7j6tO69OFNdv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}