{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellagerantoni/LatentCfMultivariate/blob/main/FingerMovements_with_closesed_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/stellagerantoni/LatentCfMultivariate"
      ],
      "metadata": {
        "id": "1fwOXGEl_ine",
        "outputId": "b72dffa3-98c3-4316-fcbd-03f698cf3739",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LatentCfMultivariate'...\n",
            "remote: Enumerating objects: 195, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 195 (delta 69), reused 36 (delta 36), pack-reused 105\u001b[K\n",
            "Receiving objects: 100% (195/195), 12.37 MiB | 6.91 MiB/s, done.\n",
            "Resolving deltas: 100% (109/109), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wildboar\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q stumpy\n",
        "!pip install -q fastdtw\n",
        "!pip install aeon[all_extras]"
      ],
      "metadata": {
        "id": "89L3kts7CCan",
        "outputId": "d20e9570-8580-4ce4-9afe-028c559b39f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.1/169.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aeon[all_extras]\n",
            "  Downloading aeon-0.6.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (23.2.0)\n",
            "Collecting deprecated>=1.2.13 (from aeon[all_extras])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: numba>=0.55 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.58.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (23.2)\n",
            "Requirement already satisfied: pandas<2.1.0,>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn<1.4.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.11.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.2.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2023.8.1)\n",
            "Collecting filterpy>=1.4.5 (from aeon[all_extras])\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (3.9.0)\n",
            "Collecting hmmlearn>=0.2.7 (from aeon[all_extras])\n",
            "  Downloading hmmlearn-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonts>=0.12.4 (from aeon[all_extras])\n",
            "  Downloading gluonts-0.14.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-self-attention (from aeon[all_extras])\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kotsu>=0.3.1 (from aeon[all_extras])\n",
            "  Downloading kotsu-0.3.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (3.7.1)\n",
            "Collecting mne (from aeon[all_extras])\n",
            "  Downloading mne-1.6.1-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pmdarima>=1.8.0 (from aeon[all_extras])\n",
            "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prophet>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.1.5)\n",
            "Collecting pyod>=0.8.0 (from aeon[all_extras])\n",
            "  Downloading pyod-1.1.2.tar.gz (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-posthocs>=0.6.5 (from aeon[all_extras])\n",
            "  Downloading scikit_posthocs-0.8.1-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.13.1)\n",
            "Collecting statsforecast>=0.5.2 (from aeon[all_extras])\n",
            "  Downloading statsforecast-1.7.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plotly-resampler>=0.9.0 (from aeon[all_extras])\n",
            "  Downloading plotly_resampler-0.9.2-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.14.1)\n",
            "Requirement already satisfied: stumpy>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.12.0)\n",
            "Collecting tbats>=1.1.0 (from aeon[all_extras])\n",
            "  Downloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.22.0)\n",
            "Collecting tsfresh>=0.20.0 (from aeon[all_extras])\n",
            "  Downloading tsfresh-0.20.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tslearn>=0.5.2 (from aeon[all_extras])\n",
            "  Downloading tslearn-0.6.3-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2023.7.0)\n",
            "Collecting mlflow<2.4.0 (from aeon[all_extras])\n",
            "  Downloading mlflow-2.3.2-py3-none-any.whl (17.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.10.1)\n",
            "Collecting esig<0.9.8.3,>=0.9.7 (from aeon[all_extras])\n",
            "  Downloading esig-0.9.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->aeon[all_extras]) (1.14.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (1.10.13)\n",
            "Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (4.66.1)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (4.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (2.8.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (8.1.7)\n",
            "Collecting databricks-cli<1,>=0.8.7 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.4)\n",
            "Collecting gitpython<4,>=2.1.0 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (6.0.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2023.3.post1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.31.0)\n",
            "Collecting importlib-metadata!=4.7.0,<7,>=3.7.0 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.4.4)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker<7,>=4.0.0 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.2.5)\n",
            "Collecting querystring-parser<2 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.0.24)\n",
            "Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (10.0.1)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.5.2)\n",
            "Collecting gunicorn<21 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.1.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55->aeon[all_extras]) (0.41.1)\n",
            "Collecting dash>=2.9.0 (from plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading dash-2.14.2-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.8.0 (from plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly<6.0.0,>=5.5.0 in /usr/local/lib/python3.10/dist-packages (from plotly-resampler>=0.9.0->aeon[all_extras]) (5.15.0)\n",
            "Collecting tsdownsample==0.1.2 (from plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading tsdownsample-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->aeon[all_extras]) (1.3.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->aeon[all_extras]) (3.0.8)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->aeon[all_extras]) (2.0.7)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->aeon[all_extras]) (67.7.2)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (1.2.0)\n",
            "Requirement already satisfied: holidays>=0.25 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (0.40)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (6.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyod>=0.8.0->aeon[all_extras]) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.0,>=1.0.0->aeon[all_extras]) (3.2.0)\n",
            "Collecting fugue>=0.8.1 (from statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading fugue-0.8.7-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting utilsforecast>=0.0.24 (from statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading utilsforecast-0.0.24-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.1->aeon[all_extras]) (0.5.6)\n",
            "Requirement already satisfied: distributed>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.20.0->aeon[all_extras]) (2023.8.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->aeon[all_extras]) (2023.6.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->aeon[all_extras]) (1.4.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne->aeon[all_extras]) (1.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne->aeon[all_extras]) (4.4.2)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from mne->aeon[all_extras]) (0.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (2.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->aeon[all_extras]) (0.1.8)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->aeon[all_extras]) (0.42.0)\n",
            "Requirement already satisfied: stanio~=0.3.0 in /usr/local/lib/python3.10/dist-packages (from cmdstanpy>=1.0.4->prophet>=1.1.0->aeon[all_extras]) (0.3.0)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras]) (3.0.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Collecting retrying (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Collecting ansi2html (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading ansi2html-1.9.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras]) (1.5.8)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (0.9.0)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (1.0.7)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (6.3.2)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (3.0.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow<2.4.0->aeon[all_extras]) (1.7.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow<2.4.0->aeon[all_extras]) (2.1.2)\n",
            "Collecting triad>=0.9.3 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading triad-0.9.3-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading adagio-0.2.4-py3-none-any.whl (26 kB)\n",
            "Collecting qpd>=0.4.4 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading qpd-0.4.4-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.2/169.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fugue-sql-antlr>=0.1.6 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading fugue-sql-antlr-0.2.0.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (19.9.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=2.1.0->mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow<2.4.0->aeon[all_extras]) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow<2.4.0->aeon[all_extras]) (2.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly<6.0.0,>=5.5.0->plotly-resampler>=0.9.0->aeon[all_extras]) (8.2.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne->aeon[all_extras]) (4.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow<2.4.0->aeon[all_extras]) (3.0.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (0.7.2)\n",
            "Collecting antlr4-python3-runtime<4.12 (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading antlr4_python3_runtime-4.11.1-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (1.3.1)\n",
            "Collecting fs (from triad>=0.9.3->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (0.5.1)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (1.4.4)\n",
            "Building wheels for collected packages: filterpy, pyod, keras-self-attention, fugue-sql-antlr\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110458 sha256=cdea2287244abedc1db327a4fa41992aeed6ca8671bedb7d2a2d34a098f05900\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-1.1.2-py3-none-any.whl size=190292 sha256=3cc0ee649559a11810205bb99fe5d73e099247ac78849a19cdf0329838d0d0ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/1b/61/aa85b78c3c0c8871f4231e3f4a03bb23cecb7db829498380ee\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=1500b4c30574bc455c42498b99a60c2cf8191d559700c1af049de9ca593cf639\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "  Building wheel for fugue-sql-antlr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fugue-sql-antlr: filename=fugue_sql_antlr-0.2.0-py3-none-any.whl size=158196 sha256=5dae40b7a19d5279589a15b85603a03d48745a0efc084b12362efe52d99b25c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/b5/4e/216953a1c711da55de29ed7ecf158b4a5bf32ef93d69ad66dd\n",
            "Successfully built filterpy pyod keras-self-attention fugue-sql-antlr\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, antlr4-python3-runtime, tsdownsample, smmap, retrying, querystring-parser, orjson, Mako, keras-self-attention, importlib-metadata, gunicorn, fs, esig, deprecated, ansi2html, gitdb, docker, databricks-cli, alembic, utilsforecast, tslearn, triad, pyod, mne, kotsu, hmmlearn, gluonts, gitpython, filterpy, dash, aeon, scikit-posthocs, pmdarima, plotly-resampler, mlflow, fugue-sql-antlr, adagio, tsfresh, tbats, qpd, fugue, statsforecast\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "Successfully installed Mako-1.3.0 adagio-0.2.4 aeon-0.6.0 alembic-1.13.1 ansi2html-1.9.1 antlr4-python3-runtime-4.11.1 dash-2.14.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 databricks-cli-0.18.0 deprecated-1.2.14 docker-6.1.3 esig-0.9.8.2 filterpy-1.4.5 fs-2.4.16 fugue-0.8.7 fugue-sql-antlr-0.2.0 gitdb-4.0.11 gitpython-3.1.41 gluonts-0.14.3 gunicorn-20.1.0 hmmlearn-0.3.0 importlib-metadata-6.11.0 keras-self-attention-0.51.0 kotsu-0.3.3 mlflow-2.3.2 mne-1.6.1 orjson-3.9.10 plotly-resampler-0.9.2 pmdarima-2.0.4 pyod-1.1.2 qpd-0.4.4 querystring-parser-1.2.4 retrying-1.3.4 scikit-posthocs-0.8.1 smmap-5.0.1 statsforecast-1.7.1 tbats-1.1.3 triad-0.9.3 tsdownsample-0.1.2 tsfresh-0.20.1 tslearn-0.6.3 utilsforecast-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from argparse import ArgumentParser\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wildboar.datasets import load_dataset\n",
        "from wildboar.ensemble import ShapeletForestClassifier\n",
        "from wildboar.explain.counterfactual import counterfactuals\n",
        "%cd '/content/LatentCfMultivariate'\n",
        "from _guided import ModifiedLatentCF\n",
        "from help_functions import *\n",
        "from keras_models import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdkpan5lCGRH",
        "outputId": "417e6235-67fb-47ec-edd3-a36cb293a984"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LatentCfMultivariate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "RANDOM_STATE = 39"
      ],
      "metadata": {
        "id": "GJE1AxFnE51S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset):\n",
        "  X, y = load_classification(dataset)\n",
        "  if dataset == 'FingerMovements':\n",
        "    pos = 'left'\n",
        "    neg = 'right'\n",
        "\n",
        "\n",
        "  print(\" Shape of X = \", X.shape)\n",
        "  print(\" Shape of y = \", y.shape)\n",
        "  #print(\" Meta data = \", meta_data)\n",
        "  # Convert positive and negative labels to 1 and 0\n",
        "  pos_label, neg_label = 1, 0\n",
        "  if pos != pos_label:\n",
        "      y[y==pos] = pos_label # convert/normalize positive label to 1\n",
        "  if neg != neg_label:\n",
        "      y[y==neg] = neg_label # convert negative label to 0\n",
        "\n",
        "  y = y.astype(int)\n",
        "  print(f\"\\n X[:1] = \\n{X[:1]}\")\n",
        "  return X,y,pos_label, neg_label"
      ],
      "metadata": {
        "id": "gwp5jdiT_Lxr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACTUALL CODE**"
      ],
      "metadata": {
        "id": "6vVfmpyuZyC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 39\n",
        "X,y,pos_label,neg_label = load_dataset('FingerMovements')\n",
        "X = X.transpose(0,2,1)\n",
        "print(f'shape of X = {X.shape}')\n",
        "print(f'shape of y = {y.shape}')\n",
        "#print(f'data imformation = {data_information}')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
        "print(f'shape of X train = {X_train.shape}')\n",
        "print(f'shape of y train = {y_train.shape}')"
      ],
      "metadata": {
        "id": "W4m9pwqyVY1b",
        "outputId": "550d5122-73e8-491b-a1e4-61da5c2753b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X =  (416, 28, 50)\n",
            " Shape of y =  (416,)\n",
            "\n",
            " X[:1] = \n",
            "[[[41.8 44.8 47.1 ... 69.8 72.6 76.1]\n",
            "  [55.2 53.8 59.9 ... 17.5 28.  12.1]\n",
            "  [-8.6 -3.6 14.4 ... 23.3 35.9 23.2]\n",
            "  ...\n",
            "  [16.9 24.5 24.5 ... 51.9 59.6 57.3]\n",
            "  [42.2 35.  41.7 ... 51.5 58.5 46.9]\n",
            "  [13.  26.6 52.5 ... -3.5 -3.2 -2.6]]]\n",
            "shape of X = (416, 50, 28)\n",
            "shape of y = (416,)\n",
            "shape of X train = (332, 50, 28)\n",
            "shape of y train = (332,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upsample the minority class\n",
        "\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'before: {class_counts}')\n",
        "X_train,y_train = upsample_minority_multivariate(X_train,y_train)\n",
        "X,y = upsample_minority_multivariate(X, y)\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'after: {class_counts}')"
      ],
      "metadata": {
        "id": "Q2v7QdrHieA8",
        "outputId": "200c361d-e96e-4cf5-ce60-e945c1723d87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: [166 166]\n",
            "after: [166 166]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing and Padding all our data\n",
        "#Padding needed for autoencoder\n",
        "\n",
        "n_training,n_timesteps, n_features= X_train.shape\n",
        "\n",
        "X, trained_scaler =  normalize_multivariate(data=X, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_train_processed, trained_scaler =  normalize_multivariate(data=X_train, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_test_processed, _ =  normalize_multivariate(data=X_test, n_timesteps=n_timesteps, scaler=trained_scaler, n_features = n_features)\n",
        "\n",
        "X, padding_size = conditional_pad_multivariate(X)\n",
        "X_train_processed_padded, padding_size = conditional_pad_multivariate(X_train_processed) # add extra padding zeros if n_timesteps cannot be divided by 4, required for 1dCNN autoencoder structure\n",
        "X_test_processed_padded, _ = conditional_pad_multivariate(X_test_processed)\n",
        "\n",
        "n_timesteps_padded = X_train_processed_padded.shape[1]\n",
        "print(f\"Data pre-processed, original #timesteps={n_timesteps}, padded #timesteps={n_timesteps_padded}.\")\n",
        "\n",
        "#check the processing (0,1) min should be min 0 and max should be max 1\n",
        "print(f\"\\nmin value = {np.min(X_train)}, max value = {np.max(X_train)}\")\n",
        "print(f\"min value normalized = {np.min(X_train_processed)}, max value normalized= {np.max(X_train_processed)}\")\n",
        "\n",
        "#check that padding paddes the right dimention\n",
        "print(f\"\\nX_train.shape = {X_train.shape}\" )\n",
        "print(f\"X_train_processed_padded.shape = {X_train_processed_padded.shape}\")\n"
      ],
      "metadata": {
        "id": "00Q9QjKy7wEZ",
        "outputId": "cd63ff9a-e6b5-4d81-ad4a-c06b97e71cf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pre-processed, original #timesteps=50, padded #timesteps=52.\n",
            "\n",
            "min value = -132.1, max value = 205.1\n",
            "min value normalized = 0.0, max value normalized= 1.0\n",
            "\n",
            "X_train.shape = (332, 50, 28)\n",
            "X_train_processed_padded.shape = (332, 52, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_validation, y_train, y_validation = train_test_split(X_train_processed_padded, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train)"
      ],
      "metadata": {
        "id": "1mYFZmsvtB9q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the two forms of labels needed\n",
        "#-the y_classes (1,0,1,0,...)\n",
        "#-the y (one hot encoded)\n",
        "\n",
        "print(f'X_train = {X_train.shape}')\n",
        "print(f'X_validation = {X_validation.shape}')\n",
        "print(f'X_test = {X_test.shape}')\n",
        "\n",
        "y_classes = y\n",
        "y_train_classes = y_train\n",
        "y_validation_classes = y_validation\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, len(np.unique(y)))\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_validation = to_categorical(y_validation, len(np.unique(y_validation)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
        "\n",
        "print(f'\\ny_train_classes = {y_train_classes.shape}, y_validation_classes = {y_validation_classes.shape}, y_test_classes = {y_test_classes.shape}')\n",
        "print(f'y_train = {y_train.shape}, y_validation = {y_validation.shape}, y_test= {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9b2F-vytxzU",
        "outputId": "fe6af505-8740-41e7-bc41-93e4a67226a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train = (265, 52, 28)\n",
            "X_validation = (67, 52, 28)\n",
            "X_test = (84, 50, 28)\n",
            "\n",
            "y_train_classes = (265,), y_validation_classes = (67,), y_test_classes = (84,)\n",
            "y_train = (265, 2), y_validation = (67, 2), y_test= (84, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1D CNN Classifier"
      ],
      "metadata": {
        "id": "wpSEaxaxGYd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ## LatentCF++ models\n",
        "# reset seeds for numpy, tensorflow, python random package and python environment seed\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ### 1dCNN classifier\n",
        "\n",
        "cnnClassifier = Classifier(\n",
        "    n_timesteps_padded, n_features, n_output=2, add_dense_layer = False\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "cnnClassifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=15, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM-FCN classifier:\")\n",
        "classifier_history = cnnClassifier.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=True,\n",
        "    validation_data=(X_validation, y_validation),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = cnnClassifier.predict(X_test_processed_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM-FCN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:1\", \"True:0\"],\n",
        "    columns=[\"Pred:1\", \"Pred:0\"],\n",
        ")\n",
        "print(confusion_matrix_df)\n"
      ],
      "metadata": {
        "id": "yNkKTXe6IIyF",
        "outputId": "13bff7b2-95ec-4403-d779-274b429b8e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM-FCN classifier:\n",
            "Epoch 1/150\n",
            "23/23 [==============================] - 3s 46ms/step - loss: 0.8307 - accuracy: 0.5132 - val_loss: 0.7577 - val_accuracy: 0.4925\n",
            "Epoch 2/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.6913 - accuracy: 0.6226 - val_loss: 0.7282 - val_accuracy: 0.5224\n",
            "Epoch 3/150\n",
            "23/23 [==============================] - 0s 13ms/step - loss: 0.6332 - accuracy: 0.7132 - val_loss: 0.7123 - val_accuracy: 0.6567\n",
            "Epoch 4/150\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.7008 - accuracy: 0.6189 - val_loss: 0.7063 - val_accuracy: 0.6418\n",
            "Epoch 5/150\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.5797 - accuracy: 0.7585 - val_loss: 0.7009 - val_accuracy: 0.6567\n",
            "Epoch 6/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.5776 - accuracy: 0.7736 - val_loss: 0.7315 - val_accuracy: 0.4925\n",
            "Epoch 7/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.5878 - accuracy: 0.7321 - val_loss: 0.7397 - val_accuracy: 0.5075\n",
            "Epoch 8/150\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.5362 - accuracy: 0.7811 - val_loss: 0.7104 - val_accuracy: 0.5224\n",
            "Epoch 9/150\n",
            "23/23 [==============================] - 1s 27ms/step - loss: 0.4929 - accuracy: 0.8264 - val_loss: 0.7111 - val_accuracy: 0.5821\n",
            "Epoch 10/150\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.4731 - accuracy: 0.8717 - val_loss: 0.6920 - val_accuracy: 0.5821\n",
            "Epoch 11/150\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.4994 - accuracy: 0.8038 - val_loss: 0.6319 - val_accuracy: 0.8209\n",
            "Epoch 12/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.4903 - accuracy: 0.8189 - val_loss: 0.6336 - val_accuracy: 0.7164\n",
            "Epoch 13/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.4320 - accuracy: 0.8717 - val_loss: 0.6221 - val_accuracy: 0.6567\n",
            "Epoch 14/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.4101 - accuracy: 0.8906 - val_loss: 0.7378 - val_accuracy: 0.5672\n",
            "Epoch 15/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.4828 - accuracy: 0.8000 - val_loss: 0.6105 - val_accuracy: 0.7015\n",
            "Epoch 16/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.3912 - accuracy: 0.8943 - val_loss: 0.5868 - val_accuracy: 0.6567\n",
            "Epoch 17/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.3797 - accuracy: 0.8981 - val_loss: 0.5746 - val_accuracy: 0.7761\n",
            "Epoch 18/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.3747 - accuracy: 0.8830 - val_loss: 0.6514 - val_accuracy: 0.6119\n",
            "Epoch 19/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.3989 - accuracy: 0.8717 - val_loss: 0.5857 - val_accuracy: 0.7463\n",
            "Epoch 20/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.3639 - accuracy: 0.8868 - val_loss: 0.5641 - val_accuracy: 0.7761\n",
            "Epoch 21/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.3212 - accuracy: 0.9019 - val_loss: 0.5201 - val_accuracy: 0.7612\n",
            "Epoch 22/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.2989 - accuracy: 0.9321 - val_loss: 0.7337 - val_accuracy: 0.6119\n",
            "Epoch 23/150\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.3010 - accuracy: 0.9321 - val_loss: 0.4953 - val_accuracy: 0.8060\n",
            "Epoch 24/150\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.2984 - accuracy: 0.9245 - val_loss: 0.4660 - val_accuracy: 0.8657\n",
            "Epoch 25/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.2896 - accuracy: 0.9245 - val_loss: 1.5668 - val_accuracy: 0.5075\n",
            "Epoch 26/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.3239 - accuracy: 0.8943 - val_loss: 0.4682 - val_accuracy: 0.8507\n",
            "Epoch 27/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.2924 - accuracy: 0.9396 - val_loss: 1.2695 - val_accuracy: 0.5373\n",
            "Epoch 28/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.3948 - accuracy: 0.8491 - val_loss: 0.5414 - val_accuracy: 0.8209\n",
            "Epoch 29/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.3381 - accuracy: 0.8830 - val_loss: 0.4692 - val_accuracy: 0.8507\n",
            "Epoch 30/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2810 - accuracy: 0.9208 - val_loss: 0.5567 - val_accuracy: 0.7761\n",
            "Epoch 31/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.2764 - accuracy: 0.9472 - val_loss: 0.4888 - val_accuracy: 0.7910\n",
            "Epoch 32/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.2381 - accuracy: 0.9736 - val_loss: 0.4446 - val_accuracy: 0.8209\n",
            "Epoch 33/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2908 - accuracy: 0.9170 - val_loss: 0.5471 - val_accuracy: 0.7015\n",
            "Epoch 34/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2417 - accuracy: 0.9660 - val_loss: 1.6652 - val_accuracy: 0.5224\n",
            "Epoch 35/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.2545 - accuracy: 0.9623 - val_loss: 0.9761 - val_accuracy: 0.5970\n",
            "Epoch 36/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.2357 - accuracy: 0.9547 - val_loss: 0.5188 - val_accuracy: 0.7910\n",
            "Epoch 37/150\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.2169 - accuracy: 0.9811 - val_loss: 0.4607 - val_accuracy: 0.8358\n",
            "Epoch 38/150\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.2486 - accuracy: 0.9472 - val_loss: 0.5106 - val_accuracy: 0.7761\n",
            "Epoch 39/150\n",
            "23/23 [==============================] - 0s 14ms/step - loss: 0.1965 - accuracy: 0.9849 - val_loss: 1.3675 - val_accuracy: 0.6119\n",
            "3/3 [==============================] - 0s 9ms/step\n",
            "LSTM-FCN classifier trained, with validation accuracy 0.4523809523809524.\n",
            "        Pred:1  Pred:0\n",
            "True:1      19      23\n",
            "True:0      23      19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1dCNN autoencoder"
      ],
      "metadata": {
        "id": "JNDwtLfMGfnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoder = Autoencoder( n_timesteps_padded,n_features,32)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoder.fit(\n",
        "    X_train,\n",
        "    X_train,\n",
        "    epochs=50,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_validation, X_validation),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")\n"
      ],
      "metadata": {
        "id": "E6IxH8BLEKFG",
        "outputId": "f7796538-278e-4ab5-b1d1-0126320e8911",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 52, 28)\n",
            "(None, 52, 32)\n",
            "(None, 26, 32)\n",
            "(None, 26, 16)\n",
            "(None, 13, 16)\n",
            "(None, 13, 16)\n",
            "(None, 26, 16)\n",
            "(None, 26, 32)\n",
            "(None, 52, 32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 52, 28)\n",
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "23/23 - 2s - loss: 0.1518 - val_loss: 0.0500 - 2s/epoch - 94ms/step\n",
            "Epoch 2/50\n",
            "23/23 - 0s - loss: 0.0250 - val_loss: 0.0140 - 123ms/epoch - 5ms/step\n",
            "Epoch 3/50\n",
            "23/23 - 0s - loss: 0.0131 - val_loss: 0.0112 - 127ms/epoch - 6ms/step\n",
            "Epoch 4/50\n",
            "23/23 - 0s - loss: 0.0112 - val_loss: 0.0099 - 130ms/epoch - 6ms/step\n",
            "Epoch 5/50\n",
            "23/23 - 0s - loss: 0.0101 - val_loss: 0.0092 - 116ms/epoch - 5ms/step\n",
            "Epoch 6/50\n",
            "23/23 - 0s - loss: 0.0094 - val_loss: 0.0083 - 145ms/epoch - 6ms/step\n",
            "Epoch 7/50\n",
            "23/23 - 0s - loss: 0.0087 - val_loss: 0.0077 - 127ms/epoch - 6ms/step\n",
            "Epoch 8/50\n",
            "23/23 - 0s - loss: 0.0080 - val_loss: 0.0071 - 130ms/epoch - 6ms/step\n",
            "Epoch 9/50\n",
            "23/23 - 0s - loss: 0.0074 - val_loss: 0.0066 - 128ms/epoch - 6ms/step\n",
            "Epoch 10/50\n",
            "23/23 - 0s - loss: 0.0068 - val_loss: 0.0060 - 130ms/epoch - 6ms/step\n",
            "Epoch 11/50\n",
            "23/23 - 0s - loss: 0.0061 - val_loss: 0.0055 - 133ms/epoch - 6ms/step\n",
            "Epoch 12/50\n",
            "23/23 - 0s - loss: 0.0057 - val_loss: 0.0052 - 208ms/epoch - 9ms/step\n",
            "Epoch 13/50\n",
            "23/23 - 0s - loss: 0.0054 - val_loss: 0.0052 - 239ms/epoch - 10ms/step\n",
            "Epoch 14/50\n",
            "23/23 - 0s - loss: 0.0052 - val_loss: 0.0049 - 197ms/epoch - 9ms/step\n",
            "Epoch 15/50\n",
            "23/23 - 0s - loss: 0.0050 - val_loss: 0.0046 - 187ms/epoch - 8ms/step\n",
            "Epoch 16/50\n",
            "23/23 - 0s - loss: 0.0048 - val_loss: 0.0047 - 195ms/epoch - 8ms/step\n",
            "Epoch 17/50\n",
            "23/23 - 0s - loss: 0.0048 - val_loss: 0.0045 - 183ms/epoch - 8ms/step\n",
            "Epoch 18/50\n",
            "23/23 - 0s - loss: 0.0046 - val_loss: 0.0044 - 219ms/epoch - 10ms/step\n",
            "Epoch 19/50\n",
            "23/23 - 0s - loss: 0.0045 - val_loss: 0.0043 - 262ms/epoch - 11ms/step\n",
            "Epoch 20/50\n",
            "23/23 - 0s - loss: 0.0045 - val_loss: 0.0042 - 315ms/epoch - 14ms/step\n",
            "Epoch 21/50\n",
            "23/23 - 0s - loss: 0.0044 - val_loss: 0.0042 - 257ms/epoch - 11ms/step\n",
            "Epoch 22/50\n",
            "23/23 - 0s - loss: 0.0043 - val_loss: 0.0040 - 229ms/epoch - 10ms/step\n",
            "Epoch 23/50\n",
            "23/23 - 0s - loss: 0.0042 - val_loss: 0.0040 - 246ms/epoch - 11ms/step\n",
            "Epoch 24/50\n",
            "23/23 - 0s - loss: 0.0042 - val_loss: 0.0041 - 287ms/epoch - 12ms/step\n",
            "Epoch 25/50\n",
            "23/23 - 0s - loss: 0.0042 - val_loss: 0.0039 - 260ms/epoch - 11ms/step\n",
            "Epoch 26/50\n",
            "23/23 - 0s - loss: 0.0041 - val_loss: 0.0039 - 324ms/epoch - 14ms/step\n",
            "Epoch 27/50\n",
            "23/23 - 0s - loss: 0.0041 - val_loss: 0.0044 - 294ms/epoch - 13ms/step\n",
            "Epoch 28/50\n",
            "23/23 - 0s - loss: 0.0042 - val_loss: 0.0039 - 306ms/epoch - 13ms/step\n",
            "Epoch 29/50\n",
            "23/23 - 0s - loss: 0.0040 - val_loss: 0.0040 - 287ms/epoch - 12ms/step\n",
            "Epoch 30/50\n",
            "23/23 - 0s - loss: 0.0040 - val_loss: 0.0038 - 184ms/epoch - 8ms/step\n",
            "Epoch 31/50\n",
            "23/23 - 0s - loss: 0.0040 - val_loss: 0.0037 - 184ms/epoch - 8ms/step\n",
            "Epoch 32/50\n",
            "23/23 - 0s - loss: 0.0039 - val_loss: 0.0037 - 190ms/epoch - 8ms/step\n",
            "Epoch 33/50\n",
            "23/23 - 0s - loss: 0.0038 - val_loss: 0.0036 - 139ms/epoch - 6ms/step\n",
            "Epoch 34/50\n",
            "23/23 - 0s - loss: 0.0038 - val_loss: 0.0037 - 130ms/epoch - 6ms/step\n",
            "Epoch 35/50\n",
            "23/23 - 0s - loss: 0.0038 - val_loss: 0.0036 - 134ms/epoch - 6ms/step\n",
            "Epoch 36/50\n",
            "23/23 - 0s - loss: 0.0038 - val_loss: 0.0036 - 143ms/epoch - 6ms/step\n",
            "Epoch 37/50\n",
            "23/23 - 0s - loss: 0.0037 - val_loss: 0.0035 - 116ms/epoch - 5ms/step\n",
            "Epoch 38/50\n",
            "23/23 - 0s - loss: 0.0037 - val_loss: 0.0036 - 122ms/epoch - 5ms/step\n",
            "Epoch 39/50\n",
            "23/23 - 0s - loss: 0.0037 - val_loss: 0.0035 - 121ms/epoch - 5ms/step\n",
            "Epoch 40/50\n",
            "23/23 - 0s - loss: 0.0036 - val_loss: 0.0035 - 117ms/epoch - 5ms/step\n",
            "Epoch 41/50\n",
            "23/23 - 0s - loss: 0.0036 - val_loss: 0.0036 - 112ms/epoch - 5ms/step\n",
            "Epoch 42/50\n",
            "23/23 - 0s - loss: 0.0036 - val_loss: 0.0034 - 125ms/epoch - 5ms/step\n",
            "Epoch 43/50\n",
            "23/23 - 0s - loss: 0.0035 - val_loss: 0.0034 - 130ms/epoch - 6ms/step\n",
            "Epoch 44/50\n",
            "23/23 - 0s - loss: 0.0035 - val_loss: 0.0034 - 123ms/epoch - 5ms/step\n",
            "Epoch 45/50\n",
            "23/23 - 0s - loss: 0.0035 - val_loss: 0.0034 - 116ms/epoch - 5ms/step\n",
            "Epoch 46/50\n",
            "23/23 - 0s - loss: 0.0035 - val_loss: 0.0033 - 116ms/epoch - 5ms/step\n",
            "Epoch 47/50\n",
            "23/23 - 0s - loss: 0.0034 - val_loss: 0.0034 - 119ms/epoch - 5ms/step\n",
            "Epoch 48/50\n",
            "23/23 - 0s - loss: 0.0034 - val_loss: 0.0033 - 122ms/epoch - 5ms/step\n",
            "Epoch 49/50\n",
            "23/23 - 0s - loss: 0.0034 - val_loss: 0.0033 - 128ms/epoch - 6ms/step\n",
            "Epoch 50/50\n",
            "23/23 - 0s - loss: 0.0034 - val_loss: 0.0033 - 133ms/epoch - 6ms/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.003296064445748925.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting weights for LatentCF"
      ],
      "metadata": {
        "id": "Bw1PkQzEGzRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gettting the Global weights, needed for counterfactuals\n",
        "\n",
        "from _guided import get_global_weights\n",
        "from help_functions import evaluate\n",
        "pos_label = 1\n",
        "neg_label = 0\n",
        "\n",
        "step_weights = get_global_weights(\n",
        "        X,\n",
        "        y_classes,\n",
        "        cnnClassifier,\n",
        "        n_timesteps= n_timesteps,\n",
        "        n_features=n_features,\n",
        "        random_state=RANDOM_STATE,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hysd9dxSsx9h",
        "outputId": "124bc012-7f7b-44fe-8f43-f8478db455e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 6ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 6ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from wildboar.explain import IntervalImportance\n",
        "#from LIMESegment.Utils.explanations import LIMESegment\n",
        "\n",
        "\n",
        "class ModifiedLatentCF:\n",
        "    \"\"\"Explanations by generating a counterfacutal sample in the latent space of\n",
        "    any autoencoder.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Learning Time Series Counterfactuals via Latent Space Representations,\n",
        "    Wang, Z., Samsten, I., Mochaourab, R., Papapetrou, P., 2021.\n",
        "    in: International Conference on Discovery Science, pp. 369–384. https://doi.org/10.1007/978-3-030-88942-5_29\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        probability=0.5,\n",
        "        *,\n",
        "        tolerance=1e-6,\n",
        "        max_iter=100,\n",
        "        optimizer=None,\n",
        "        autoencoder=None,\n",
        "        pred_margin_weight=1.0,  # weighted_steps_weight = 1 - pred_margin_weight\n",
        "        step_weights=\"local\",\n",
        "        random_state=None,\n",
        "        max_weighted_steps_loss= 0.002\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        probability : float, optional\n",
        "            The desired probability assigned by the model\n",
        "\n",
        "        tolerance : float, optional\n",
        "            The maximum difference between the desired and assigned probability\n",
        "\n",
        "        optimizer :\n",
        "            Optimizer with a defined learning rate\n",
        "\n",
        "        max_iter : int, optional\n",
        "            The maximum number of iterations\n",
        "\n",
        "        autoencoder : int, optional\n",
        "            The autoencoder for the latent representation\n",
        "\n",
        "            - if None the sample is generated in the original space\n",
        "            - if given, the autoencoder is expected to have `k` decoder layer and `k`\n",
        "              encoding layers.\n",
        "        \"\"\"\n",
        "        self.optimizer_ = (\n",
        "            tf.optimizers.Adam(learning_rate=1e-4) if optimizer is None else optimizer\n",
        "        )\n",
        "        self.mse_loss_ = keras.losses.MeanSquaredError()\n",
        "        self.probability_ = tf.constant([probability])\n",
        "        self.tolerance_ = tf.constant(tolerance)\n",
        "        self.max_iter = max_iter\n",
        "        self.autoencoder = autoencoder\n",
        "        self.max_weighted_steps_loss = max_weighted_steps_loss\n",
        "\n",
        "        # Weights of the different loss components\n",
        "        self.pred_margin_weight = pred_margin_weight\n",
        "        self.weighted_steps_weight = 1 - self.pred_margin_weight\n",
        "\n",
        "        self.step_weights = step_weights\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def fit(self, model):\n",
        "        \"\"\"Fit a new counterfactual explainer to the model\n",
        "\n",
        "        Paramaters\n",
        "        ----------\n",
        "\n",
        "        model : keras.Model\n",
        "            The model\n",
        "        \"\"\"\n",
        "        if self.autoencoder:\n",
        "            (\n",
        "                encode_input,\n",
        "                encode_output,\n",
        "                decode_input,\n",
        "                decode_output,\n",
        "            ) = extract_encoder_decoder(self.autoencoder)\n",
        "            self.decoder_ = keras.Model(inputs=decode_input, outputs=decode_output)\n",
        "            self.encoder_ = keras.Model(inputs=encode_input, outputs=encode_output)\n",
        "        else:\n",
        "            self.decoder_ = None\n",
        "            self.encoder_ = None\n",
        "        self.model_ = model\n",
        "        return self\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Compute the difference between the desired and actual probability\n",
        "\n",
        "        Parameters\n",
        "        ---------\n",
        "        x : Variable\n",
        "            Variable of the sample\n",
        "        \"\"\"\n",
        "        if self.autoencoder is None:\n",
        "            z = x\n",
        "        else:\n",
        "            z = self.decoder_(x)\n",
        "\n",
        "        return self.model_(z)\n",
        "\n",
        "    # The \"pred_margin_loss\" is designed to measure the prediction probability to the desired decision boundary\n",
        "    def pred_margin_mse(self, prediction):\n",
        "        return self.mse_loss_(self.probability_, prediction)\n",
        "\n",
        "    # An auxiliary MAE loss function to measure the proximity with step_weights\n",
        "    def weighted_mae(self, original_sample, cf_sample, step_weights):\n",
        "        return tf.math.reduce_mean(\n",
        "            tf.math.multiply(tf.math.abs(original_sample - cf_sample), step_weights)\n",
        "        )\n",
        "\n",
        "    # An auxiliary normalized L2 loss function to measure the proximity with step_weights\n",
        "    def weighted_normalized_l2(self, example, cf_sample, step_weights):\n",
        "        var_diff = tf.math.reduce_variance(example - cf_sample)\n",
        "        var_orig = tf.math.reduce_variance(example)\n",
        "        var_cf = tf.math.reduce_variance(cf_sample)\n",
        "\n",
        "        normalized_l2 = 0.5 * var_diff / (var_orig + var_cf)\n",
        "        return tf.math.reduce_mean(\n",
        "            tf.math.multiply(\n",
        "                normalized_l2,\n",
        "                step_weights,\n",
        "            )\n",
        "        )\n",
        "    def euclidian_distance(self,x,y):\n",
        "      euclidean_distance = np.linalg.norm(x - y)\n",
        "      return(euclidean_distance)\n",
        "\n",
        "\n",
        "    # additional input of step_weights\n",
        "    def compute_loss(self, example, z_search, step_weights, target_label):\n",
        "        loss = tf.zeros(shape=())\n",
        "        decoded = self.decoder_(z_search) if self.autoencoder is not None else z_search\n",
        "        pred = self.model_(decoded)[:, target_label]\n",
        "\n",
        "        pred_margin_loss = self.pred_margin_mse(pred)\n",
        "        loss += self.pred_margin_weight * pred_margin_loss\n",
        "\n",
        "        # euclidian_distance=self.euclidian_distance(decoded,self.example)\n",
        "\n",
        "        # weighted_steps_loss = self.weighted_mae(\n",
        "        #     original_sample=tf.cast(self.example, dtype=tf.float32),\n",
        "        #     cf_sample=tf.cast(decoded, dtype=tf.float32),\n",
        "        #     step_weights=tf.cast(step_weights, tf.float32),\n",
        "        # )\n",
        "        weighted_steps_loss = self.weighted_normalized_l2(\n",
        "            example=tf.cast(example, dtype=tf.float32),\n",
        "            cf_sample=tf.cast(decoded, dtype=tf.float32),\n",
        "            step_weights=tf.cast(step_weights, tf.float32)\n",
        "        )\n",
        "        loss += self.weighted_steps_weight * weighted_steps_loss\n",
        "\n",
        "        return loss, pred_margin_loss, weighted_steps_loss\n",
        "\n",
        "    def get_closesed_counterfactual(self, X, X_ofOpp,step_weights):\n",
        "      best_example = X_ofOpp[0][np.newaxis]\n",
        "      min_distance = self.weighted_normalized_l2(X_ofOpp[0][np.newaxis],X,step_weights)\n",
        "      for idx in range(X_ofOpp.shape[0]):\n",
        "        sample =X_ofOpp[idx][np.newaxis]\n",
        "        distance = self.weighted_normalized_l2(sample,X,step_weights)\n",
        "        if distance < min_distance:\n",
        "          min_distance = distance\n",
        "          best_example = sample\n",
        "      return best_example\n",
        "\n",
        "    # TODO: compatible with the counterfactuals of wildboar\n",
        "    #       i.e., define the desired output target per label\n",
        "    def transform(self, x, pred_labels,X_ofOpp):\n",
        "        \"\"\"Generate counterfactual explanations\n",
        "\n",
        "        x : array-like of shape [n_samples, n_timestep, n_dims]\n",
        "            The samples\n",
        "        \"\"\"\n",
        "\n",
        "        result_samples = np.empty(x.shape)\n",
        "        losses = np.empty(x.shape[0])\n",
        "        # `weights_all` needed for debugging\n",
        "        weights_all = np.empty((x.shape[0], 1, x.shape[1], x.shape[2]))\n",
        "\n",
        "        for i in range(x.shape[0]):\n",
        "            if i % 25 == 0:\n",
        "                print(f\"{i+1} samples been transformed.\")\n",
        "\n",
        "\n",
        "\n",
        "            # if self.step_weights == \"global\" OR \"uniform\"\n",
        "            if isinstance(self.step_weights, np.ndarray):  #  \"global\" OR \"uniform\"\n",
        "                step_weights = self.step_weights\n",
        "            elif self.step_weights == \"local\":\n",
        "                # ignore warning of matrix multiplication, from LIMESegment: `https://stackoverflow.com/questions/29688168/mean-nanmean-and-warning-mean-of-empty-slice`\n",
        "                # ignore warning of scipy package warning, from LIMESegment: `https://github.com/paulvangentcom/heartrate_analysis_python/issues/31`\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "                    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
        "                    step_weights = get_local_weights(\n",
        "                        x[i],\n",
        "                        self.model_,\n",
        "                        random_state=self.random_state,\n",
        "                        pred_label=pred_labels[i],\n",
        "                    )\n",
        "            else:\n",
        "                raise NotImplementedError(\n",
        "                    \"step_weights not implemented, please choose 'local', 'global' or 'uniform'.\"\n",
        "                )\n",
        "\n",
        "            example = self.get_closesed_counterfactual(x[np.newaxis, i], X_ofOpp, step_weights)\n",
        "            #visualise_decoded_digit(x[np.newaxis, i],0)\n",
        "            #visualise_decoded_digit(example,1)\n",
        "            x_sample, loss = self._transform_sample(\n",
        "                x[np.newaxis, i], step_weights, pred_labels[i],example\n",
        "            )\n",
        "\n",
        "            result_samples[i] = x_sample\n",
        "            losses[i] = loss\n",
        "            weights_all[i] = step_weights\n",
        "\n",
        "        print(f\"{i+1} samples been transformed, in total.\")\n",
        "\n",
        "        return result_samples, losses, weights_all\n",
        "\n",
        "    def _transform_sample(self, x, step_weights, pred_label,example):\n",
        "        \"\"\"Generate counterfactual explanations\n",
        "\n",
        "        x : array-like of shape [n_samples, n_timestep, n_dims]\n",
        "            The samples\n",
        "        \"\"\"\n",
        "        # TODO: check_is_fitted(self)\n",
        "        if self.autoencoder is not None:\n",
        "            z = tf.Variable(self.encoder_(x))\n",
        "        else:\n",
        "            z = tf.Variable(x, dtype=tf.float32)\n",
        "\n",
        "        it = 0\n",
        "        target_label = 1 - pred_label  # for binary classification\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss, pred_margin_loss, weighted_steps_loss = self.compute_loss(\n",
        "                example, z, step_weights, target_label\n",
        "            )\n",
        "\n",
        "        if self.autoencoder is not None:\n",
        "            pred = self.model_(self.decoder_(z))\n",
        "        else:\n",
        "            pred = self.model_(z)\n",
        "\n",
        "        # # uncomment for debug\n",
        "        # print(\n",
        "        #     f\"current loss: {loss}, pred_margin_loss: {pred_margin_loss}, weighted_steps_loss: {weighted_steps_loss}, pred prob:{pred}, iter: {it}.\"\n",
        "        # )\n",
        "\n",
        "        # TODO: modify the loss to check both validity and proximity; how to design the condition here?\n",
        "        # while (pred_margin_loss > self.tolerance_ or pred[:, 1] < self.probability_ or weighted_steps_loss > self.step_tolerance_)?\n",
        "        # loss > tf.multiply(self.tolerance_rate_, loss_original)\n",
        "        #\n",
        "        while (\n",
        "            pred_margin_loss > self.tolerance_\n",
        "            and weighted_steps_loss > self.max_weighted_steps_loss\n",
        "        ) and (it < self.max_iter if self.max_iter else True):\n",
        "            # Get gradients of loss wrt the sample\n",
        "            grads = tape.gradient(loss, z)\n",
        "            # Update the weights of the sample\n",
        "            self.optimizer_.apply_gradients([(grads, z)])\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss, pred_margin_loss, weighted_steps_loss = self.compute_loss(\n",
        "                    example, z, step_weights, target_label\n",
        "                )\n",
        "            it += 1\n",
        "\n",
        "            if self.autoencoder is not None:\n",
        "                pred = self.model_(self.decoder_(z))\n",
        "            else:\n",
        "                pred = self.model_(z)\n",
        "\n",
        "        # # uncomment for debug\n",
        "        # print(\n",
        "        #     f\"current loss: {loss}, pred_margin_loss: {pred_margin_loss}, weighted_steps_loss: {weighted_steps_loss}, pred prob:{pred}, iter: {it}. \\n\"\n",
        "        # )\n",
        "\n",
        "        res = z.numpy() if self.autoencoder is None else self.decoder_(z).numpy()\n",
        "        return res, float(loss)\n",
        "\n",
        "\n",
        "def extract_encoder_decoder(autoencoder):\n",
        "    \"\"\"Extract the encoder and decoder from an autoencoder\n",
        "\n",
        "    autoencoder : keras.Model\n",
        "        The autoencoder of `k` encoders and `k` decoders\n",
        "    \"\"\"\n",
        "    depth = len(autoencoder.layers) // 2\n",
        "    encoder = autoencoder.layers[1](autoencoder.input)\n",
        "    for i in range(2, depth):\n",
        "        encoder = autoencoder.layers[i](encoder)\n",
        "\n",
        "    encode_input = keras.Input(shape=encoder.shape[1:])\n",
        "    decoder = autoencoder.layers[depth](encode_input)\n",
        "    for i in range(depth + 1, len(autoencoder.layers)):\n",
        "        decoder = autoencoder.layers[i](decoder)\n",
        "\n",
        "    return autoencoder.input, encoder, encode_input, decoder"
      ],
      "metadata": {
        "id": "BdTpIDFD-YL3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.9,tolerance=1e-6, max_iter=500, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),autoencoder = autoencoder,\n",
        "    pred_margin_weight=0.5, step_weights = step_weights, random_state= RANDOM_STATE, max_weighted_steps_loss= 0.01)\n",
        "cf_model.fit(cnnClassifier)\n",
        "\n",
        "y_neg = y_train_classes[y_train_classes == 0]\n",
        "X_neg = X_train[y_train_classes == 0]\n",
        "\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning) # ignore warning of matrix multiplication: https://stackoverflow.com/questions/29688168/mean-nanmean-and-warning-mean-of-empty-slice\n",
        "    cf_embeddings, losses, weights = cf_model.transform(X_neg, y_neg, X_train[y_train_classes == 1],) # x, pred_label, X_OppClass\n",
        "z_pred = cnnClassifier.predict(cf_embeddings)\n",
        "cf_pred_labels = np.argmax(z_pred, axis=1)\n",
        "\n",
        "#evaluate_res = evaluate(X_pred_neg, best_cf_samples, z_pred, n_timesteps, tree, max_distance)\n",
        "#evaluate_res #  proximity, validity, cost_mean, cost_std\n",
        "\n",
        "print(f'Transformation_finished with validity_score = {validity_score(y_neg,cf_pred_labels)}')"
      ],
      "metadata": {
        "id": "f-Xy39aj8q9S",
        "outputId": "e63f9f57-dc55-48fb-b0a8-9ab31c86389d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 samples been transformed.\n",
            "26 samples been transformed.\n",
            "51 samples been transformed.\n",
            "76 samples been transformed.\n",
            "101 samples been transformed.\n",
            "126 samples been transformed.\n",
            "133 samples been transformed, in total.\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "Transformation_finished with validity_score = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Validity\n",
        "cf_pred = cnnClassifier.predict(cf_embeddings)[:, 1] # predicted probabilities of CFs\n",
        "cf_pred_labels = cf_pred\n",
        "for idx in range(cf_pred_labels.shape[0]):\n",
        "  if cf_pred_labels[idx] > 0.5:\n",
        "    cf_pred_labels[idx] = 1\n",
        "  else:\n",
        "    cf_pred_labels[idx] = 0\n",
        "\n",
        "\n",
        "print(f'Transformation_finished with validity_score = {validity_score(y_neg,cf_pred_labels)}')"
      ],
      "metadata": {
        "id": "Ov4igLqcorG9",
        "outputId": "b9df890c-40b8-4ace-f2d1-0a4d1c0bb668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step\n",
            "Transformation_finished with validity_score = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity withour absolute\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total_mse = 0\n",
        "probability = 0.5\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = cnnClassifier.predict(counterfactual)[:, 1]\n",
        "    mse = MeanSquaredError()\n",
        "    mse_dist = mse(probability, prediction).numpy()\n",
        "    total_mse +=mse_dist\n",
        "mean_mse = total_mse /cf_embeddings.shape[0]\n"
      ],
      "metadata": {
        "id": "Nvlack0n40dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGFIQmVG2hFe",
        "outputId": "32f60b87-e4f6-4e1d-bfcb-3f5c53066da0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mean MSE of the data is: 1.600409483216188e-05 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity with absolute\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total = 0\n",
        "probability = 0.5\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = cnnClassifier.predict(counterfactual)[:, 1]\n",
        "    dist = abs(prediction - probability)\n",
        "    total +=dist\n",
        "mean_mse = total /cf_embeddings.shape[0]"
      ],
      "metadata": {
        "id": "oK3o9SQB41a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uYKtc9Z2iMx",
        "outputId": "5eb4a305-eee4-4220-8045-8451713a8f4c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mean MSE of the data is: [0.00213552] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Proximity\n",
        "def euclidean_distance(X, cf_samples):\n",
        "    paired_distances = np.linalg.norm(X - cf_samples, axis=1)\n",
        "    return np.mean(paired_distances)\n",
        "euclidean_distance(X_neg, cf_embeddings)"
      ],
      "metadata": {
        "id": "ZTsPvh1lotsk",
        "outputId": "3929570f-92c7-4637-a42a-985ab879a6c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.478405804616136"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_paddings(cf_samples, padding_size):\n",
        "    if padding_size != 0:\n",
        "        # use np.squeeze() to cut the last time-series dimension, for evaluation\n",
        "        cf_samples = np.squeeze(cf_samples[:, :-padding_size, :])\n",
        "    else:\n",
        "        cf_samples = np.squeeze(cf_samples)\n",
        "    return cf_samples"
      ],
      "metadata": {
        "id": "9MPQoXMjrFmn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove paddings because KDE does not work with paddings.\n",
        "\n",
        "X_unpadded = remove_paddings(X, padding_size)\n",
        "cf_embeddings_unpadded = remove_paddings(cf_embeddings, padding_size)"
      ],
      "metadata": {
        "id": "dh_BVpXMrtXu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "diffrences_from_abnormal = []\n",
        "diffrences_from_normal = []\n",
        "for dimention in range(cf_embeddings.shape[2]):\n",
        "\n",
        "\n",
        "  abnormal_data = X_unpadded[y_classes == 1][:,:,dimention]\n",
        "  normal_data = X_unpadded[y_classes == 0][:,:,dimention]\n",
        "  counterf_data = cf_embeddings_unpadded[:,:,dimention]\n",
        "\n",
        "  #get the kernel for every dimention of the trained\n",
        "  kernel = gaussian_kde(abnormal_data.T,bw_method=None)\n",
        "\n",
        "  #get all the log likelihoods\n",
        "  log_likelihood_abnormal = np.mean(kernel.logpdf(abnormal_data.T))\n",
        "  log_likelihood_normal = np.mean(kernel.logpdf(normal_data.T))\n",
        "  log_likelihood_counterfactual = np.mean(kernel.logpdf(counterf_data.T))\n",
        "\n",
        "  #get the diffrences from the counterfactuals\n",
        "  diff_from_abnormal = abs(log_likelihood_counterfactual-log_likelihood_abnormal)\n",
        "  diffrences_from_abnormal.append(diff_from_abnormal)\n",
        "\n",
        "  diff_from_normal = abs(log_likelihood_counterfactual-log_likelihood_normal)\n",
        "  diffrences_from_normal.append(diff_from_normal)\n"
      ],
      "metadata": {
        "id": "7GSjwx8w4liQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(diffrences_from_normal))"
      ],
      "metadata": {
        "id": "BzGfR9Abt_fD",
        "outputId": "dd3f0ea8-15fb-4dd8-a764-481c13d40640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.773607492566192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(diffrences_from_abnormal))"
      ],
      "metadata": {
        "id": "C6lyK3-Pt_h3",
        "outputId": "0cb14f5b-7908-4d61-8a4a-85ee21f6b1fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71.26389444849778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diffrences_from_normal"
      ],
      "metadata": {
        "id": "G6rd-6JvFLP1",
        "outputId": "47d78e2e-ee47-47d5-ab99-9fffa05c6f12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.031435419792032,\n",
              " 84.4758623379435,\n",
              " 20.21850541049649,\n",
              " 39.21413356541084,\n",
              " 9.630788797744728,\n",
              " 16.044529768003983,\n",
              " 31.999577440379973,\n",
              " 43.237341345398534,\n",
              " 12.174214012109772,\n",
              " 21.586088889892906,\n",
              " 0.3897328832103,\n",
              " 6.417653009168859,\n",
              " 17.855394751218356,\n",
              " 41.330707099142124,\n",
              " 94.72442140492659,\n",
              " 7.613217194232519,\n",
              " 5.6789591344059716,\n",
              " 19.191871001876805,\n",
              " 40.80163727151994,\n",
              " 62.22577281239691,\n",
              " 5.706631824862953,\n",
              " 10.407499179633277,\n",
              " 9.296702795632555,\n",
              " 6.618872434094001,\n",
              " 6.195419901480719,\n",
              " 12.27843163751092,\n",
              " 2.2437856858243066,\n",
              " 6.071822783543595]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diffrences_from_abnormal"
      ],
      "metadata": {
        "id": "7j6tO69OFNdv",
        "outputId": "72712c80-8946-4fe0-fe75-55849614462d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[58.3862528431782,\n",
              " 138.04295615435518,\n",
              " 77.19191440741942,\n",
              " 96.45137523804789,\n",
              " 47.718126158814286,\n",
              " 72.37237074417327,\n",
              " 89.44054934052318,\n",
              " 100.83864528135604,\n",
              " 43.986830209145765,\n",
              " 32.0173476687642,\n",
              " 54.60602878225862,\n",
              " 60.54746343277651,\n",
              " 36.20401065087901,\n",
              " 95.18795629302284,\n",
              " 148.46662262683904,\n",
              " 47.074045573026325,\n",
              " 49.44067844243615,\n",
              " 77.45215786268842,\n",
              " 101.82182792676302,\n",
              " 124.56642525514904,\n",
              " 64.58490499589394,\n",
              " 62.199327749932735,\n",
              " 63.124661073387,\n",
              " 58.048876686463515,\n",
              " 58.26671804437679,\n",
              " 40.328180890114055,\n",
              " 49.70878896629803,\n",
              " 47.314001259855445]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}