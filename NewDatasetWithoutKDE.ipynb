{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+ucObKHfAkbvTfqsvVMtj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellagerantoni/LatentCfMultivariate/blob/main/NewDatasetWithoutKDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48qQzSqPSihy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/stellagerantoni/LatentCfMultivariate"
      ],
      "metadata": {
        "id": "1fwOXGEl_ine",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685c609b-188a-422f-93fe-7df53cffc213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LatentCfMultivariate'...\n",
            "remote: Enumerating objects: 134, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (132/132), done.\u001b[K\n",
            "remote: Total 134 (delta 77), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (134/134), 7.20 MiB | 8.70 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wildboar\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q stumpy\n",
        "!pip install -q fastdtw\n",
        "!pip install aeon"
      ],
      "metadata": {
        "id": "89L3kts7CCan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from argparse import ArgumentParser\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wildboar.datasets import load_dataset\n",
        "from wildboar.ensemble import ShapeletForestClassifier\n",
        "from wildboar.explain.counterfactual import counterfactuals\n",
        "%cd '/content/LatentCfMultivariate'\n",
        "from _guided import ModifiedLatentCF\n",
        "from help_functions import *\n",
        "from keras_models import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdkpan5lCGRH",
        "outputId": "d6cb61cc-687e-4b12-d64b-b798ba0723ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LatentCfMultivariate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "RANDOM_STATE = 39"
      ],
      "metadata": {
        "id": "GJE1AxFnE51S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FUNCTIONS**"
      ],
      "metadata": {
        "id": "IHBw9E_4Zm5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset):\n",
        "  X, y = load_classification(dataset)\n",
        "  if dataset == 'Heartbeat':\n",
        "    pos = 'normal'\n",
        "    neg = 'abnormal'\n",
        "  if dataset == 'SelfRegulationSCP1':\n",
        "    pos = 'positivity'\n",
        "    neg = 'negativity'\n",
        "  if dataset == 'Blink':\n",
        "    pos = 'shortblink'\n",
        "    neg = 'longblink'\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "  if dataset == 'FingerMovements':\n",
        "    pos = 'left'\n",
        "    neg = 'right'\n",
        "  if dataset == 'Cricket':\n",
        "    return X,y,meta_data\n",
        "  if dataset == 'SpokenArabicDigits':\n",
        "    return X,y,meta_data\n",
        "  if dataset == 'PenDigits':\n",
        "    return X,y,meta_data\n",
        "\n",
        "\n",
        "  print(\" Shape of X = \", X.shape)\n",
        "  print(\" Shape of y = \", y.shape)\n",
        "  #print(\" Meta data = \", meta_data)\n",
        "  # Convert positive and negative labels to 1 and 0\n",
        "  pos_label, neg_label = 1, 0\n",
        "  if pos != pos_label:\n",
        "      y[y==pos] = pos_label # convert/normalize positive label to 1\n",
        "  if neg != neg_label:\n",
        "      y[y==neg] = neg_label # convert negative label to 0\n",
        "\n",
        "  y = y.astype(int)\n",
        "  print(f\"\\n X[:1] = \\n{X[:1]}\")\n",
        "  return X,y,pos_label, neg_label"
      ],
      "metadata": {
        "id": "QsJJktx22dXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACTUALL CODE**\n",
        "datasets available : 'Heartbeat', 'SelfRegulationSCP1'"
      ],
      "metadata": {
        "id": "6vVfmpyuZyC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 39\n",
        "X,y,pos_label,neg_label = load_dataset('FingerMovements')\n",
        "X = X.transpose(0,2,1)\n",
        "print(f'shape of X = {X.shape}')\n",
        "print(f'shape of y = {y.shape}')\n",
        "#print(f'data imformation = {data_information}')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
        "print(f'shape of X train = {X_train.shape}')\n",
        "print(f'shape of y train = {y_train.shape}')"
      ],
      "metadata": {
        "id": "W4m9pwqyVY1b",
        "outputId": "00502c30-5187-4989-f567-c9a1e36b022d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Shape of X =  (416, 28, 50)\n",
            " Shape of y =  (416,)\n",
            "\n",
            " X[:1] = \n",
            "[[[41.8 44.8 47.1 ... 69.8 72.6 76.1]\n",
            "  [55.2 53.8 59.9 ... 17.5 28.  12.1]\n",
            "  [-8.6 -3.6 14.4 ... 23.3 35.9 23.2]\n",
            "  ...\n",
            "  [16.9 24.5 24.5 ... 51.9 59.6 57.3]\n",
            "  [42.2 35.  41.7 ... 51.5 58.5 46.9]\n",
            "  [13.  26.6 52.5 ... -3.5 -3.2 -2.6]]]\n",
            "shape of X = (416, 50, 28)\n",
            "shape of y = (416,)\n",
            "shape of X train = (332, 50, 28)\n",
            "shape of y train = (332,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsample the minority class\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'before: {class_counts}')\n",
        "X_train,y_train = upsample_minority_multivariate(X_train,y_train)\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'after: {class_counts}')"
      ],
      "metadata": {
        "id": "Q2v7QdrHieA8",
        "outputId": "f3b3e60c-7a73-4dfc-cc37-bc01e4238d2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: [166 166]\n",
            "after: [166 166]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_training,n_timesteps, n_features= X_train.shape\n",
        "\n",
        "X, trained_scaler =  normalize_multivariate(data=X, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_train_processed, trained_scaler =  normalize_multivariate(data=X_train, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_test_processed, _ =  normalize_multivariate(data=X_test, n_timesteps=n_timesteps, scaler=trained_scaler, n_features = n_features)\n",
        "\n",
        "X, padding_size = conditional_pad_multivariate(X)\n",
        "X_train_processed_padded, padding_size = conditional_pad_multivariate(X_train_processed) # add extra padding zeros if n_timesteps cannot be divided by 4, required for 1dCNN autoencoder structure\n",
        "X_test_processed_padded, _ = conditional_pad_multivariate(X_test_processed)\n",
        "\n",
        "n_timesteps_padded = X_train_processed_padded.shape[1]\n",
        "print(f\"Data pre-processed, original #timesteps={n_timesteps}, padded #timesteps={n_timesteps_padded}.\")\n",
        "\n",
        "#check the processing (0,1) min should be min 0 and max should be max 1\n",
        "print(f\"\\nmin value = {np.min(X_train)}, max value = {np.max(X_train)}\")\n",
        "print(f\"min value normalized = {np.min(X_train_processed)}, max value normalized= {np.max(X_train_processed)}\")\n",
        "\n",
        "#check that padding paddes the right dimention\n",
        "print(f\"\\nX_train.shape = {X_train.shape}\" )\n",
        "print(f\"X_train_processed_padded.shape = {X_train_processed_padded.shape}\")\n"
      ],
      "metadata": {
        "id": "00Q9QjKy7wEZ",
        "outputId": "e65e7a30-82d0-47fb-a4d2-b7c686a9c07b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pre-processed, original #timesteps=50, padded #timesteps=52.\n",
            "\n",
            "min value = -132.1, max value = 205.1\n",
            "min value normalized = 0.0, max value normalized= 1.0\n",
            "\n",
            "X_train.shape = (332, 50, 28)\n",
            "X_train_processed_padded.shape = (332, 52, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_processed_padded,X_validation, y_train, y_validation = train_test_split(X_train_processed_padded, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train)\n"
      ],
      "metadata": {
        "id": "1mYFZmsvtB9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'X_train = {X_train_processed_padded.shape}')\n",
        "print(f'X_validation = {X_validation.shape}')\n",
        "print(f'X_test = {X_test_processed_padded.shape}')\n",
        "\n",
        "y_classes = y\n",
        "y_train_classes = y_train\n",
        "y_validation_classes = y_validation\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y,len(np.unique(y)))\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_validation = to_categorical(y_validation, len(np.unique(y_validation)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
        "\n",
        "print(f'\\ny_train_classes = {y_train_classes.shape}, y_validation_classes = {y_validation_classes.shape}, y_test_classes = {y_test_classes.shape}')\n",
        "print(f'y_train = {y_train.shape}, y_validation = {y_validation.shape}, y_test= {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9b2F-vytxzU",
        "outputId": "7228b3df-6686-42a7-b958-8aa64dff6be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train = (265, 52, 28)\n",
            "X_validation = (67, 52, 28)\n",
            "X_test = (84, 52, 28)\n",
            "\n",
            "y_train_classes = (265,), y_validation_classes = (67,), y_test_classes = (84,)\n",
            "y_train = (265, 2), y_validation = (67, 2), y_test= (84, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_classes.shape"
      ],
      "metadata": {
        "id": "bvT2RQKeezAq",
        "outputId": "ac48d40e-7e3e-4b5e-89d2-45b295587194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(640,)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "uqF_ZMToezDh",
        "outputId": "135f514c-51a3-4012-8a4d-7d2387069c61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(640, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ## 2. LatentCF models\n",
        "# reset seeds for numpy, tensorflow, python random package and python environment seed\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ### 1dCNN classifier\n",
        "\n",
        "cnnClassifier = Classifier(\n",
        "    n_timesteps_padded, n_features, n_output=2, add_dense_layer = False\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "cnnClassifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=15, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM-FCN classifier:\")\n",
        "classifier_history = cnnClassifier.fit(\n",
        "    X_train_processed_padded,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=True,\n",
        "    validation_data=(X_validation, y_validation),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = cnnClassifier.predict(X_test_processed_padded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM-FCN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:8\", \"True:0\"],\n",
        "    columns=[\"Pred:8\", \"Pred:0\"],\n",
        ")\n",
        "print(confusion_matrix_df)\n"
      ],
      "metadata": {
        "id": "yNkKTXe6IIyF",
        "outputId": "964078a2-3f02-4920-8dff-01fd9633f602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM-FCN classifier:\n",
            "Epoch 1/150\n",
            "23/23 [==============================] - 1s 17ms/step - loss: 0.8307 - accuracy: 0.5132 - val_loss: 0.7577 - val_accuracy: 0.4925\n",
            "Epoch 2/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6913 - accuracy: 0.6226 - val_loss: 0.7282 - val_accuracy: 0.5224\n",
            "Epoch 3/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.6332 - accuracy: 0.7132 - val_loss: 0.7123 - val_accuracy: 0.6567\n",
            "Epoch 4/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7008 - accuracy: 0.6189 - val_loss: 0.7063 - val_accuracy: 0.6418\n",
            "Epoch 5/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5797 - accuracy: 0.7585 - val_loss: 0.7009 - val_accuracy: 0.6567\n",
            "Epoch 6/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5776 - accuracy: 0.7736 - val_loss: 0.7315 - val_accuracy: 0.4925\n",
            "Epoch 7/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.5878 - accuracy: 0.7321 - val_loss: 0.7397 - val_accuracy: 0.5075\n",
            "Epoch 8/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.5362 - accuracy: 0.7811 - val_loss: 0.7104 - val_accuracy: 0.5224\n",
            "Epoch 9/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4929 - accuracy: 0.8264 - val_loss: 0.7111 - val_accuracy: 0.5821\n",
            "Epoch 10/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4731 - accuracy: 0.8717 - val_loss: 0.6920 - val_accuracy: 0.5821\n",
            "Epoch 11/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4994 - accuracy: 0.8038 - val_loss: 0.6319 - val_accuracy: 0.8209\n",
            "Epoch 12/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.4903 - accuracy: 0.8189 - val_loss: 0.6336 - val_accuracy: 0.7164\n",
            "Epoch 13/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4320 - accuracy: 0.8717 - val_loss: 0.6221 - val_accuracy: 0.6567\n",
            "Epoch 14/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.4101 - accuracy: 0.8906 - val_loss: 0.7378 - val_accuracy: 0.5672\n",
            "Epoch 15/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.4828 - accuracy: 0.8000 - val_loss: 0.6105 - val_accuracy: 0.7015\n",
            "Epoch 16/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3912 - accuracy: 0.8943 - val_loss: 0.5868 - val_accuracy: 0.6567\n",
            "Epoch 17/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3797 - accuracy: 0.8981 - val_loss: 0.5746 - val_accuracy: 0.7761\n",
            "Epoch 18/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3747 - accuracy: 0.8830 - val_loss: 0.6514 - val_accuracy: 0.6119\n",
            "Epoch 19/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.3989 - accuracy: 0.8717 - val_loss: 0.5857 - val_accuracy: 0.7463\n",
            "Epoch 20/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.3639 - accuracy: 0.8868 - val_loss: 0.5641 - val_accuracy: 0.7761\n",
            "Epoch 21/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.3212 - accuracy: 0.9019 - val_loss: 0.5201 - val_accuracy: 0.7612\n",
            "Epoch 22/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.2989 - accuracy: 0.9321 - val_loss: 0.7337 - val_accuracy: 0.6119\n",
            "Epoch 23/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.3010 - accuracy: 0.9321 - val_loss: 0.4953 - val_accuracy: 0.8060\n",
            "Epoch 24/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.2984 - accuracy: 0.9245 - val_loss: 0.4660 - val_accuracy: 0.8657\n",
            "Epoch 25/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.2896 - accuracy: 0.9245 - val_loss: 1.5668 - val_accuracy: 0.5075\n",
            "Epoch 26/150\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.3239 - accuracy: 0.8943 - val_loss: 0.4682 - val_accuracy: 0.8507\n",
            "Epoch 27/150\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 0.2924 - accuracy: 0.9396 - val_loss: 1.2695 - val_accuracy: 0.5373\n",
            "Epoch 28/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.3948 - accuracy: 0.8491 - val_loss: 0.5414 - val_accuracy: 0.8209\n",
            "Epoch 29/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.3381 - accuracy: 0.8830 - val_loss: 0.4692 - val_accuracy: 0.8507\n",
            "Epoch 30/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.2810 - accuracy: 0.9208 - val_loss: 0.5567 - val_accuracy: 0.7761\n",
            "Epoch 31/150\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.2764 - accuracy: 0.9472 - val_loss: 0.4888 - val_accuracy: 0.7910\n",
            "Epoch 32/150\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.2381 - accuracy: 0.9736 - val_loss: 0.4446 - val_accuracy: 0.8209\n",
            "Epoch 33/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2908 - accuracy: 0.9170 - val_loss: 0.5471 - val_accuracy: 0.7015\n",
            "Epoch 34/150\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.2417 - accuracy: 0.9660 - val_loss: 1.6652 - val_accuracy: 0.5224\n",
            "Epoch 35/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.2545 - accuracy: 0.9623 - val_loss: 0.9761 - val_accuracy: 0.5970\n",
            "Epoch 36/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.2357 - accuracy: 0.9547 - val_loss: 0.5188 - val_accuracy: 0.7910\n",
            "Epoch 37/150\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.2169 - accuracy: 0.9811 - val_loss: 0.4607 - val_accuracy: 0.8358\n",
            "Epoch 38/150\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.2486 - accuracy: 0.9472 - val_loss: 0.5106 - val_accuracy: 0.7761\n",
            "Epoch 39/150\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.1965 - accuracy: 0.9849 - val_loss: 1.3675 - val_accuracy: 0.6119\n",
            "3/3 [==============================] - 0s 7ms/step\n",
            "LSTM-FCN classifier trained, with validation accuracy 0.4523809523809524.\n",
            "        Pred:8  Pred:0\n",
            "True:8      19      23\n",
            "True:0      23      19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "\n",
        "\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoder = Autoencoder(n_timesteps_padded,n_features,32)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoder.fit(\n",
        "    X_train_processed_padded,\n",
        "    X_train_processed_padded,\n",
        "    epochs=50,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_validation, X_validation),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")\n"
      ],
      "metadata": {
        "id": "E6IxH8BLEKFG",
        "outputId": "122b352c-c4c8-4fd6-d7e0-6e03a590fbac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 52, 28)\n",
            "(None, 52, 32)\n",
            "(None, 26, 32)\n",
            "(None, 26, 16)\n",
            "(None, 13, 16)\n",
            "(None, 13, 16)\n",
            "(None, 26, 16)\n",
            "(None, 26, 32)\n",
            "(None, 52, 32)\n",
            "(None, 52, 28)\n",
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "23/23 - 2s - loss: 0.1518 - val_loss: 0.0500 - 2s/epoch - 75ms/step\n",
            "Epoch 2/50\n",
            "23/23 - 0s - loss: 0.0250 - val_loss: 0.0140 - 185ms/epoch - 8ms/step\n",
            "Epoch 3/50\n",
            "23/23 - 0s - loss: 0.0131 - val_loss: 0.0112 - 174ms/epoch - 8ms/step\n",
            "Epoch 4/50\n",
            "23/23 - 0s - loss: 0.0112 - val_loss: 0.0099 - 172ms/epoch - 7ms/step\n",
            "Epoch 5/50\n",
            "23/23 - 0s - loss: 0.0101 - val_loss: 0.0092 - 177ms/epoch - 8ms/step\n",
            "Epoch 6/50\n",
            "23/23 - 0s - loss: 0.0094 - val_loss: 0.0083 - 158ms/epoch - 7ms/step\n",
            "Epoch 7/50\n",
            "23/23 - 0s - loss: 0.0087 - val_loss: 0.0077 - 149ms/epoch - 6ms/step\n",
            "Epoch 8/50\n",
            "23/23 - 0s - loss: 0.0080 - val_loss: 0.0071 - 203ms/epoch - 9ms/step\n",
            "Epoch 9/50\n",
            "23/23 - 0s - loss: 0.0074 - val_loss: 0.0066 - 161ms/epoch - 7ms/step\n",
            "Epoch 10/50\n",
            "23/23 - 0s - loss: 0.0068 - val_loss: 0.0060 - 162ms/epoch - 7ms/step\n",
            "Epoch 11/50\n",
            "23/23 - 0s - loss: 0.0061 - val_loss: 0.0055 - 164ms/epoch - 7ms/step\n",
            "Epoch 12/50\n",
            "23/23 - 0s - loss: 0.0057 - val_loss: 0.0052 - 168ms/epoch - 7ms/step\n",
            "Epoch 13/50\n",
            "23/23 - 0s - loss: 0.0054 - val_loss: 0.0052 - 165ms/epoch - 7ms/step\n",
            "Epoch 14/50\n",
            "23/23 - 0s - loss: 0.0052 - val_loss: 0.0049 - 161ms/epoch - 7ms/step\n",
            "Epoch 15/50\n",
            "23/23 - 0s - loss: 0.0050 - val_loss: 0.0046 - 167ms/epoch - 7ms/step\n",
            "Epoch 16/50\n",
            "23/23 - 0s - loss: 0.0048 - val_loss: 0.0047 - 156ms/epoch - 7ms/step\n",
            "Epoch 17/50\n",
            "23/23 - 0s - loss: 0.0048 - val_loss: 0.0045 - 157ms/epoch - 7ms/step\n",
            "Epoch 18/50\n",
            "23/23 - 0s - loss: 0.0046 - val_loss: 0.0044 - 156ms/epoch - 7ms/step\n",
            "Epoch 19/50\n",
            "23/23 - 0s - loss: 0.0045 - val_loss: 0.0043 - 155ms/epoch - 7ms/step\n",
            "Epoch 20/50\n",
            "23/23 - 0s - loss: 0.0045 - val_loss: 0.0042 - 152ms/epoch - 7ms/step\n",
            "Epoch 21/50\n",
            "23/23 - 0s - loss: 0.0044 - val_loss: 0.0042 - 181ms/epoch - 8ms/step\n",
            "Epoch 22/50\n",
            "23/23 - 0s - loss: 0.0043 - val_loss: 0.0040 - 162ms/epoch - 7ms/step\n",
            "Epoch 23/50\n",
            "23/23 - 0s - loss: 0.0042 - val_loss: 0.0040 - 150ms/epoch - 7ms/step\n",
            "Epoch 24/50\n",
            "23/23 - 0s - loss: 0.0042 - val_loss: 0.0041 - 141ms/epoch - 6ms/step\n",
            "Epoch 25/50\n",
            "23/23 - 0s - loss: 0.0042 - val_loss: 0.0039 - 143ms/epoch - 6ms/step\n",
            "Epoch 26/50\n",
            "23/23 - 0s - loss: 0.0041 - val_loss: 0.0039 - 140ms/epoch - 6ms/step\n",
            "Epoch 27/50\n",
            "23/23 - 0s - loss: 0.0041 - val_loss: 0.0044 - 168ms/epoch - 7ms/step\n",
            "Epoch 28/50\n",
            "23/23 - 0s - loss: 0.0042 - val_loss: 0.0039 - 147ms/epoch - 6ms/step\n",
            "Epoch 29/50\n",
            "23/23 - 0s - loss: 0.0040 - val_loss: 0.0040 - 157ms/epoch - 7ms/step\n",
            "Epoch 30/50\n",
            "23/23 - 0s - loss: 0.0040 - val_loss: 0.0038 - 157ms/epoch - 7ms/step\n",
            "Epoch 31/50\n",
            "23/23 - 0s - loss: 0.0040 - val_loss: 0.0037 - 164ms/epoch - 7ms/step\n",
            "Epoch 32/50\n",
            "23/23 - 0s - loss: 0.0039 - val_loss: 0.0037 - 160ms/epoch - 7ms/step\n",
            "Epoch 33/50\n",
            "23/23 - 0s - loss: 0.0038 - val_loss: 0.0036 - 204ms/epoch - 9ms/step\n",
            "Epoch 34/50\n",
            "23/23 - 0s - loss: 0.0038 - val_loss: 0.0037 - 158ms/epoch - 7ms/step\n",
            "Epoch 35/50\n",
            "23/23 - 0s - loss: 0.0038 - val_loss: 0.0036 - 158ms/epoch - 7ms/step\n",
            "Epoch 36/50\n",
            "23/23 - 0s - loss: 0.0038 - val_loss: 0.0036 - 165ms/epoch - 7ms/step\n",
            "Epoch 37/50\n",
            "23/23 - 0s - loss: 0.0037 - val_loss: 0.0035 - 153ms/epoch - 7ms/step\n",
            "Epoch 38/50\n",
            "23/23 - 0s - loss: 0.0037 - val_loss: 0.0036 - 157ms/epoch - 7ms/step\n",
            "Epoch 39/50\n",
            "23/23 - 0s - loss: 0.0037 - val_loss: 0.0035 - 165ms/epoch - 7ms/step\n",
            "Epoch 40/50\n",
            "23/23 - 0s - loss: 0.0036 - val_loss: 0.0035 - 172ms/epoch - 7ms/step\n",
            "Epoch 41/50\n",
            "23/23 - 0s - loss: 0.0036 - val_loss: 0.0036 - 162ms/epoch - 7ms/step\n",
            "Epoch 42/50\n",
            "23/23 - 0s - loss: 0.0036 - val_loss: 0.0034 - 162ms/epoch - 7ms/step\n",
            "Epoch 43/50\n",
            "23/23 - 0s - loss: 0.0035 - val_loss: 0.0034 - 160ms/epoch - 7ms/step\n",
            "Epoch 44/50\n",
            "23/23 - 0s - loss: 0.0035 - val_loss: 0.0034 - 159ms/epoch - 7ms/step\n",
            "Epoch 45/50\n",
            "23/23 - 0s - loss: 0.0035 - val_loss: 0.0034 - 162ms/epoch - 7ms/step\n",
            "Epoch 46/50\n",
            "23/23 - 0s - loss: 0.0035 - val_loss: 0.0033 - 180ms/epoch - 8ms/step\n",
            "Epoch 47/50\n",
            "23/23 - 0s - loss: 0.0034 - val_loss: 0.0034 - 171ms/epoch - 7ms/step\n",
            "Epoch 48/50\n",
            "23/23 - 0s - loss: 0.0034 - val_loss: 0.0033 - 223ms/epoch - 10ms/step\n",
            "Epoch 49/50\n",
            "23/23 - 0s - loss: 0.0034 - val_loss: 0.0033 - 216ms/epoch - 9ms/step\n",
            "Epoch 50/50\n",
            "23/23 - 0s - loss: 0.0034 - val_loss: 0.0033 - 277ms/epoch - 12ms/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.003296064445748925.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from _guided import get_global_weights\n",
        "PRED_MARGIN_W_LIST = [1]\n",
        "from help_functions import evaluate\n",
        "w_type = \"global\"\n",
        "pos_label = 1\n",
        "neg_label = 0\n",
        "\n",
        "if w_type == \"global\":\n",
        "    step_weights = get_global_weights(\n",
        "        X,\n",
        "        y_classes,\n",
        "        cnnClassifier,\n",
        "        n_timesteps= n_timesteps,\n",
        "        n_features=n_features,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "elif w_type == \"uniform\":\n",
        "    step_weights = np.ones((1, n_timesteps_padded, n_features))\n",
        "elif w_type.lower() == \"local\":\n",
        "    step_weights = \"local\"\n",
        "else:\n",
        "    raise NotImplementedError(\n",
        "        \"A.w_type not implemented, please choose 'local', 'global' or 'uniform'.\"\n",
        "    )\n",
        "### Evaluation metrics\n",
        "for pred_margin_weight in PRED_MARGIN_W_LIST:\n",
        "    print(f\"The current prediction margin weight is {pred_margin_weight}.\")\n",
        "\n",
        "    lr_list = [0.001]\n"
      ],
      "metadata": {
        "id": "hysd9dxSsx9h",
        "outputId": "e65bbcee-7511-418a-b86e-d3b24ef0ec1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 6ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 6ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 6ms/step\n",
            "13/13 [==============================] - 0s 6ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 6ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 4ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 6ms/step\n",
            "13/13 [==============================] - 0s 5ms/step\n",
            "The current prediction margin weight is 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step_weights"
      ],
      "metadata": {
        "id": "BdTpIDFD-YL3",
        "outputId": "001eb514-38b1-457d-c75c-bb38b2cc7471",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.5,tolerance=1e-6, max_iter=500, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),autoencoder = autoencoder,\n",
        "    pred_margin_weight=0.7, step_weights = step_weights, random_state= RANDOM_STATE)\n",
        "cf_model.fit(cnnClassifier)\n",
        "\n",
        "y_neg = y_classes[y_classes == 0][10:15]\n",
        "X_neg = X[y_classes == 0][10:15]\n",
        "\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning) # ignore warning of matrix multiplication: https://stackoverflow.com/questions/29688168/mean-nanmean-and-warning-mean-of-empty-slice\n",
        "    cf_embeddings, losses, weights = cf_model.transform(X_neg, y_neg) #self, x, pred_label\n",
        "cf_pred_labels = cnnClassifier.predict(cf_embeddings)[:,1]# predicted probabilities of CFs\n",
        "for idx in range(cf_pred_labels.shape[0]):\n",
        "  if cf_pred_labels[idx] > 0.5:\n",
        "    cf_pred_labels[idx] = 1\n",
        "  else:\n",
        "    cf_pred_labels[idx] = 0\n",
        "\n",
        "print(f'Transformation_finished with validity_score = {validity_score(y_neg,cf_pred_labels)}')"
      ],
      "metadata": {
        "id": "f-Xy39aj8q9S",
        "outputId": "cef6a5f4-fc77-4209-f288-4b20cae81bb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 samples been transformed.\n",
            "5 samples been transformed, in total.\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Transformation_finished with validity_score = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Proximity\n",
        "def euclidean_distance(X, cf_samples):\n",
        "    paired_distances = np.linalg.norm(X - cf_samples, axis=1)\n",
        "    return np.mean(paired_distances)\n",
        "euclidean_distance(X_neg, cf_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50Hed3JXrm1Z",
        "outputId": "10227234-c69c-4583-800c-f7c9ddb1d666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40918086563892114"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "# Reshape and standardize the data\n",
        "X_reshaped = X.reshape(X.shape[0], n_timesteps_padded * n_features)\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(X_reshaped)\n",
        "\n",
        "# Reduce dimensions using PCA\n",
        "pca = PCA(n_components=1500)  # for example, reducing to 100 dimensions\n",
        "X_pca = pca.fit_transform(X_standardized)\n",
        "\n",
        "# Now apply KDE on the reduced data\n",
        "# For example, applying KDE on the first component\n",
        "#kernel = gaussian_kde(X_pca[:, 0])\n"
      ],
      "metadata": {
        "id": "fjLhbn1Z_9JZ",
        "outputId": "ac3915e2-bbe4-4923-bf4b-10397c2ce976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-98-ffe5fce1acce>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Reduce dimensions using PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# for example, reducing to 100 dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mX_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_standardized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Now apply KDE on the reduced data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;31m# Call different fits for either full or truncated SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"arpack\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"randomized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_full\u001b[0;34m(self, X, n_components)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 )\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    527\u001b[0m                 \u001b[0;34m\"n_components=%r must be between 0 and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;34m\"min(n_samples, n_features)=%r with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: n_components=1500 must be between 0 and min(n_samples, n_features)=950 with svd_solver='full'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normal_data.shape"
      ],
      "metadata": {
        "id": "w85dgz-9LzNQ",
        "outputId": "7fb9e309-3b48-4bfb-d943-66153dfbcfd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(208, 52)"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abnormal_data[:, np.newaxis].shape"
      ],
      "metadata": {
        "id": "94l8BwFs_9VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(cf_embeddings.shape[2]):\n",
        "  dimention = i+1\n",
        "\n",
        "  #X_stacked_data.shape = (554, 408, 61)\n",
        "\n",
        "  abnormal_data = X[y_classes == 1][:,:,dimention]\n",
        "  normal_data = X[y_classes == 0][:,:,dimention]\n",
        "  #abnormal_data.shape =normal_data.shape =  (295, 408)\n",
        "\n",
        "  #cf_embeddings.shape = (5, 408, 61)\n",
        "  counterf_data = cf_embeddings[:,:,dimention]\n",
        "  #counterf_data.shape = (5, 408)\n",
        "\n",
        "  #get the kernel for every dimention of the trained\n",
        "  kernel = gaussian_kde(abnormal_data.T,bw_method=0.5)"
      ],
      "metadata": {
        "id": "9Bb5nfXtOE4O",
        "outputId": "c674a2c0-e6f6-448a-82f0-41dad9b0fce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LinAlgError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_kde.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, bw_method, weights)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_bandwidth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbw_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbw_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_kde.py\u001b[0m in \u001b[0;36mset_bandwidth\u001b[0;34m(self, bw_method)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_covariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_kde.py\u001b[0m in \u001b[0;36m_compute_covariance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    582\u001b[0m                                                aweights=self.weights))\n\u001b[0;32m--> 583\u001b[0;31m             self._data_cho_cov = linalg.cholesky(self._data_covariance,\n\u001b[0m\u001b[1;32m    584\u001b[0m                                                  lower=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\u001b[0m in \u001b[0;36mcholesky\u001b[0;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \"\"\"\n\u001b[0;32m---> 89\u001b[0;31m     c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n\u001b[0m\u001b[1;32m     90\u001b[0m                          check_finite=check_finite)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_cholesky.py\u001b[0m in \u001b[0;36m_cholesky\u001b[0;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n\u001b[0m\u001b[1;32m     38\u001b[0m                           \"definite\" % info)\n",
            "\u001b[0;31mLinAlgError\u001b[0m: 51-th leading minor of the array is not positive definite",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-dc653c397a26>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m#get the kernel for every dimention of the trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgaussian_kde\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabnormal_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbw_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_kde.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, bw_method, weights)\u001b[0m\n\u001b[1;32m    230\u001b[0m                    \u001b[0;34m\"analysis / dimensionality reduction and using \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                    \"`gaussian_kde` with the transformed data.\")\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLinAlgError\u001b[0m: The data appears to lie in a lower-dimensional subspace of the space in which it is expressed. This has resulted in a singular data covariance matrix, which cannot be treated using the algorithms implemented in `gaussian_kde`. Consider performing principle component analysis / dimensionality reduction and using `gaussian_kde` with the transformed data."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "kernel_x = gaussian_kde(x_eights_train.T,bw_method=None)\n",
        "\n",
        "\n",
        "log_likelihood_of_xtest_eights = np.mean(kernel_x.logpdf(x_eights_test.T))\n",
        "print(f\"log_likelihood_of_xtest_eights = {log_likelihood_of_xtest_eights}\")\n",
        "\n",
        "log_likelihood_of_xtest_zeroes = np.mean(kernel_x.logpdf(x_zeros_test.T))\n",
        "print(f\"log_likelihood_of_xtest_zeroes = {log_likelihood_of_xtest_zeroes}\")\n",
        "\n",
        "\n",
        "log_likelihood_of_xcf = np.mean(kernel_x.logpdf(x_eights_cf.T))\n",
        "print(f\"log_likelihood_of_xcf = {log_likelihood_of_xcf}\\n\")\n",
        "\n",
        "kernel_y = gaussian_kde(y_eights_train.T,bw_method=None)\n",
        "\n",
        "\n",
        "log_likelihood_of_ytest_eights = np.mean(kernel_y.logpdf(y_eights_test.T))\n",
        "print(f\"log_likelihood_of_ytest_eights = {log_likelihood_of_ytest_eights}\")\n",
        "\n",
        "log_likelihood_of_ytest_zeroes = np.mean(kernel_y.logpdf(y_zeros_test.T))\n",
        "print(f\"log_likelihood_of_ytest_zeroes = {log_likelihood_of_ytest_zeroes}\")\n",
        "\n",
        "\n",
        "log_likelihood_of_ycf = np.mean(kernel_y.logpdf(y_eights_cf.T))\n",
        "print(f\"log_likelihood_of_ycf = {log_likelihood_of_ycf}\")"
      ],
      "metadata": {
        "id": "JOvWhvoQjGtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_eights_train = X_train_processed_padded[y_train_classes == 1][:,:,0]\n",
        "y_eights_train = X_train_processed_padded[y_train_classes == 1][:,:,1]\n",
        "print(f\"shape of eights_train = {x_eights_train.shape}\")\n",
        "\n",
        "x_eights_test = X_test_processed_padded[y_test_classes == 1][:,:,0]\n",
        "y_eights_test = X_test_processed_padded[y_test_classes == 1][:,:,1]\n",
        "print(f\"shape of eights_test = {x_eights_test.shape}\")\n",
        "\n",
        "x_zeros_test = X_test_processed_padded[y_test_classes == 0][:,:,0]\n",
        "y_zeros_test = X_test_processed_padded[y_test_classes == 0][:,:,1]\n",
        "print(f\"shape of zeros_test = {x_zeros_test.shape}\")\n",
        "\n",
        "x_eights_cf = cf_embeddings[:,:,0]\n",
        "y_eights_cf = cf_embeddings[:,:,1]\n",
        "print(f\"shape of eights_cf = {x_eights_cf.shape}\")"
      ],
      "metadata": {
        "id": "6-QslGacrm8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "kernel_x = gaussian_kde(x_eights_train.T,bw_method=None)\n",
        "\n",
        "\n",
        "log_likelihood_of_xtest_eights = np.mean(kernel_x.logpdf(x_eights_test.T))\n",
        "print(f\"log_likelihood_of_xtest_eights = {log_likelihood_of_xtest_eights}\")\n",
        "\n",
        "log_likelihood_of_xtest_zeroes = np.mean(kernel_x.logpdf(x_zeros_test.T))\n",
        "print(f\"log_likelihood_of_xtest_zeroes = {log_likelihood_of_xtest_zeroes}\")\n",
        "\n",
        "\n",
        "log_likelihood_of_xcf = np.mean(kernel_x.logpdf(x_eights_cf.T))\n",
        "print(f\"log_likelihood_of_xcf = {log_likelihood_of_xcf}\\n\")\n",
        "\n",
        "kernel_y = gaussian_kde(y_eights_train.T,bw_method=None)\n",
        "\n",
        "\n",
        "log_likelihood_of_ytest_eights = np.mean(kernel_y.logpdf(y_eights_test.T))\n",
        "print(f\"log_likelihood_of_ytest_eights = {log_likelihood_of_ytest_eights}\")\n",
        "\n",
        "log_likelihood_of_ytest_zeroes = np.mean(kernel_y.logpdf(y_zeros_test.T))\n",
        "print(f\"log_likelihood_of_ytest_zeroes = {log_likelihood_of_ytest_zeroes}\")\n",
        "\n",
        "\n",
        "log_likelihood_of_ycf = np.mean(kernel_y.logpdf(y_eights_cf.T))\n",
        "print(f\"log_likelihood_of_ycf = {log_likelihood_of_ycf}\")"
      ],
      "metadata": {
        "id": "Skr46dtSrnG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total = 0\n",
        "probability = 0.5\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = cnnClassifier.predict(counterfactual)[:, 1]\n",
        "    dist = (prediction - probability)\n",
        "    total +=dist\n",
        "mean_mse = total /cf_embeddings.shape[0]\n",
        "print(f\"The Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "id": "Yk5ICCrqruZY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}