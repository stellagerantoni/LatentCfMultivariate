{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellagerantoni/LatentCfMultivariate/blob/main/WalkingSittingStanding_simple_KDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/stellagerantoni/LatentCfMultivariate"
      ],
      "metadata": {
        "id": "1fwOXGEl_ine"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wildboar\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q stumpy\n",
        "!pip install -q fastdtw\n",
        "!pip install aeon"
      ],
      "metadata": {
        "id": "89L3kts7CCan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from argparse import ArgumentParser\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wildboar.datasets import load_dataset\n",
        "from wildboar.ensemble import ShapeletForestClassifier\n",
        "from wildboar.explain.counterfactual import counterfactuals\n",
        "%cd '/content/LatentCfMultivariate'\n",
        "from _guided import ModifiedLatentCF\n",
        "from help_functions import *\n",
        "from keras_models import *"
      ],
      "metadata": {
        "id": "Bdkpan5lCGRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "RANDOM_STATE = 39"
      ],
      "metadata": {
        "id": "GJE1AxFnE51S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 39\n",
        "X, y = load_classification('WalkingSittingStanding')\n",
        "X = X.transpose(0,2,1)\n",
        "\n",
        "#print(f'data imformation = {data_information}')\n",
        "#keep half the dataset because it is too big\n",
        "X,X_through,y,y_through = train_test_split(X, y, test_size=0.5, random_state=RANDOM_STATE, stratify=y)\n",
        "print(f'shape of X = {X.shape}')\n",
        "print(f'shape of y = {y.shape}')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
        "print(f'shape of X train = {X_train.shape}')\n",
        "print(f'shape of y train = {y_train.shape}')"
      ],
      "metadata": {
        "id": "W4m9pwqyVY1b",
        "outputId": "f7816935-8778-4fcc-b053-ea0889ca9168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of X = (5149, 206, 3)\n",
            "shape of y = (5149,)\n",
            "shape of X train = (4119, 206, 3)\n",
            "shape of y train = (4119,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upsample the minority class\n",
        "\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'before: {class_counts}')\n",
        "X_train,y_train = upsample_minority_multivariate(X_train,y_train)\n",
        "X,y = upsample_minority_multivariate(X, y)\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'after: {class_counts}')"
      ],
      "metadata": {
        "id": "Q2v7QdrHieA8",
        "outputId": "20ea4493-bc4b-4d5f-ef34-45785c81863e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: [689 618 562 710 762 778]\n",
            "after: [778 778 778 778 778 778]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing and Padding all our data\n",
        "#Padding needed for autoencoder\n",
        "\n",
        "n_training,n_timesteps, n_features= X_train.shape\n",
        "\n",
        "X, trained_scaler =  normalize_multivariate(data=X, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_train_processed, trained_scaler =  normalize_multivariate(data=X_train, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_test_processed, _ =  normalize_multivariate(data=X_test, n_timesteps=n_timesteps, scaler=trained_scaler, n_features = n_features)\n",
        "\n",
        "X, padding_size = conditional_pad_multivariate(X)\n",
        "X_train_processed_padded, padding_size = conditional_pad_multivariate(X_train_processed) # add extra padding zeros if n_timesteps cannot be divided by 4, required for 1dCNN autoencoder structure\n",
        "X_test_processed_padded, _ = conditional_pad_multivariate(X_test_processed)\n",
        "\n",
        "n_timesteps_padded = X_train_processed_padded.shape[1]\n",
        "print(f\"Data pre-processed, original #timesteps={n_timesteps}, padded #timesteps={n_timesteps_padded}.\")\n",
        "\n",
        "#check the processing (0,1) min should be min 0 and max should be max 1\n",
        "print(f\"\\nmin value = {np.min(X_train)}, max value = {np.max(X_train)}\")\n",
        "print(f\"min value normalized = {np.min(X_train_processed)}, max value normalized= {np.max(X_train_processed)}\")\n",
        "\n",
        "#check that padding paddes the right dimention\n",
        "print(f\"\\nX_train.shape = {X_train.shape}\" )\n",
        "print(f\"X_train_processed_padded.shape = {X_train_processed_padded.shape}\")\n"
      ],
      "metadata": {
        "id": "00Q9QjKy7wEZ",
        "outputId": "e9964376-5369-4928-e4a7-34acd12ffac5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pre-processed, original #timesteps=206, padded #timesteps=208.\n",
            "\n",
            "min value = -1.639609, max value = 2.157473\n",
            "min value normalized = 0.0, max value normalized= 1.0\n",
            "\n",
            "X_train.shape = (4668, 206, 3)\n",
            "X_train_processed_padded.shape = (4668, 208, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_two_classes(X,y,n_1,n_2, RANDOM_STATE):\n",
        "  #get the normal and abnormal label\n",
        "  #abnormal is the target label for the latentCF++ model\n",
        "  normal_label = n_1\n",
        "  abnormal_label = n_2\n",
        "\n",
        "  #Get the indices\n",
        "  normal_indices = np.where(y == normal_label)[0]\n",
        "  abnormal_indices = np.where(y == abnormal_label)[0]\n",
        "\n",
        "  #Use the indices to get the wanted data points\n",
        "  X_abnormal = X[abnormal_indices]\n",
        "  y_abnormal = y[abnormal_indices]\n",
        "\n",
        "  X_normal = X[normal_indices]\n",
        "  y_normal = y[normal_indices]\n",
        "\n",
        "  #stack all the data together again\n",
        "  X = np.concatenate([X_abnormal,X_normal])\n",
        "  y = np.concatenate([y_abnormal,y_normal])\n",
        "\n",
        "  #shuffle them\n",
        "  X, y = shuffle(X, y, random_state=RANDOM_STATE)\n",
        "\n",
        "  #iterate over the dataset to make the labels 0 for abnormal and 1 for normal\n",
        "\n",
        "  for i in range(y.shape[0]):\n",
        "    if y[i] == n_2:\n",
        "      y[i]=1\n",
        "    else:\n",
        "      y [i]=0\n",
        "\n",
        "  y = y.astype(int)\n",
        "  print(f'Class 0 represents number {n_1}. [1.,0.]')\n",
        "  print(f'Class 1 represents number {n_2}. [0.,1.]\\n')\n",
        "\n",
        "  return X,y"
      ],
      "metadata": {
        "id": "0m4A2SbnqdFP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "ZTDy6lmUqlz2",
        "outputId": "fd3298eb-82a0-4367-945c-f83bf182c6ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0.0', '0.0', '0.0', ..., '5.0', '5.0', '5.0'], dtype='<U3')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = extract_two_classes(X,y,'0.0','5.0', RANDOM_STATE)\n",
        "X_train, y_train = extract_two_classes(X_train_processed_padded,y_train,'0.0','5.0', RANDOM_STATE)\n",
        "X_test, y_test = extract_two_classes(X_test_processed_padded,y_test,'0.0','5.0', RANDOM_STATE)"
      ],
      "metadata": {
        "id": "eNEim6c_qjSm",
        "outputId": "6228171e-16ff-4828-ed40-0c0d0e227eca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0 represents number 0.0. [1.,0.]\n",
            "Class 1 represents number 5.0. [0.,1.]\n",
            "\n",
            "Class 0 represents number 0.0. [1.,0.]\n",
            "Class 1 represents number 5.0. [0.,1.]\n",
            "\n",
            "Class 0 represents number 0.0. [1.,0.]\n",
            "Class 1 represents number 5.0. [0.,1.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train)"
      ],
      "metadata": {
        "id": "1mYFZmsvtB9q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the two forms of labels needed\n",
        "#-the y_classes (1,0,1,0,...)\n",
        "#-the y (one hot encoded)\n",
        "\n",
        "print(f'X_train = {X_train.shape}')\n",
        "print(f'X_validation = {X_validation.shape}')\n",
        "print(f'X_test = {X_test.shape}')\n",
        "\n",
        "y_classes = y\n",
        "y_train_classes = y_train\n",
        "y_validation_classes = y_validation\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, len(np.unique(y)))\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_validation = to_categorical(y_validation, len(np.unique(y_validation)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
        "\n",
        "print(f'\\ny_train_classes = {y_train_classes.shape}, y_validation_classes = {y_validation_classes.shape}, y_test_classes = {y_test_classes.shape}')\n",
        "print(f'y_train = {y_train.shape}, y_validation = {y_validation.shape}, y_test= {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9b2F-vytxzU",
        "outputId": "759de47a-15f3-4195-fb5d-8851bd170222"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train = (1244, 208, 3)\n",
            "X_validation = (312, 208, 3)\n",
            "X_test = (366, 208, 3)\n",
            "\n",
            "y_train_classes = (1244,), y_validation_classes = (312,), y_test_classes = (366,)\n",
            "y_train = (1244, 2), y_validation = (312, 2), y_test= (366, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ClassifierLSTM(n_timesteps, n_features, extra_lstm_layer=True, n_output=1):\n",
        "    # Define the model structure - only LSTM layers\n",
        "    # https://www.kaggle.com/szaitseff/classification-of-time-series-with-lstm-rnn\n",
        "    inputs = keras.Input(shape=(n_timesteps, n_features), dtype=\"float32\")\n",
        "    if extra_lstm_layer:\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(\n",
        "            inputs\n",
        "        )  # set return_sequences true to feed next LSTM layer\n",
        "    else:\n",
        "        x = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=False)(\n",
        "            inputs\n",
        "        )  # set return_sequences false to feed dense layer directly\n",
        "    x = keras.layers.BatchNormalization()(x)\n",
        "    # x = keras.layers.LSTM(32, activation='tanh', return_sequences=True)(x)\n",
        "    # x = keras.layers.BatchNormalization()(x)\n",
        "    if extra_lstm_layer:\n",
        "        x = keras.layers.LSTM(16, activation=\"tanh\", return_sequences=False)(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    if n_output >= 2:\n",
        "        outputs = keras.layers.Dense(n_output, activation=\"softmax\")(x)\n",
        "    else:\n",
        "        outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    classifier2 = keras.Model(inputs, outputs)\n",
        "\n",
        "    return classifier2"
      ],
      "metadata": {
        "id": "2Zv-rMFKByaR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ## LatentCF++ models\n",
        "# reset seeds for numpy, tensorflow, python random package and python environment seed\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ### LSTM classifier\n",
        "\n",
        "classifier = ClassifierLSTM(\n",
        "    n_timesteps_padded, n_features, n_output=2\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "classifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=15, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM-FCN classifier:\")\n",
        "classifier_history = classifier.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=150,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=True,\n",
        "    validation_data=(X_validation, y_validation),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM-FCN classifier trained, with validation accuracy {acc}.\")\n",
        "\n",
        "confusion_matrix_df = pd.DataFrame(\n",
        "    confusion_matrix(y_true=y_test_classes, y_pred=y_pred_classes, labels=[1, 0]),\n",
        "    index=[\"True:1\", \"True:0\"],\n",
        "    columns=[\"Pred:1\", \"Pred:0\"],\n",
        ")\n",
        "print(confusion_matrix_df)\n"
      ],
      "metadata": {
        "id": "yNkKTXe6IIyF",
        "outputId": "55e7e22b-a218-4c5b-b33f-826c53870e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM-FCN classifier:\n",
            "Epoch 1/150\n",
            "104/104 [==============================] - 19s 52ms/step - loss: 0.1984 - accuracy: 0.9590 - val_loss: 0.2909 - val_accuracy: 1.0000\n",
            "Epoch 2/150\n",
            "104/104 [==============================] - 2s 18ms/step - loss: 0.0378 - accuracy: 0.9960 - val_loss: 0.6770 - val_accuracy: 0.5000\n",
            "Epoch 3/150\n",
            "104/104 [==============================] - 2s 20ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
            "Epoch 4/150\n",
            "104/104 [==============================] - 2s 18ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
            "Epoch 5/150\n",
            "104/104 [==============================] - 2s 20ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 6/150\n",
            "104/104 [==============================] - 2s 22ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 5.2614e-04 - val_accuracy: 1.0000\n",
            "Epoch 7/150\n",
            "104/104 [==============================] - 2s 20ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 3.6777e-04 - val_accuracy: 1.0000\n",
            "Epoch 8/150\n",
            "104/104 [==============================] - 2s 20ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.5172e-04 - val_accuracy: 1.0000\n",
            "Epoch 9/150\n",
            "104/104 [==============================] - 2s 18ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 1.5687e-04 - val_accuracy: 1.0000\n",
            "Epoch 10/150\n",
            "104/104 [==============================] - 2s 19ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.0545e-04 - val_accuracy: 1.0000\n",
            "Epoch 11/150\n",
            "104/104 [==============================] - 2s 20ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 8.1861e-05 - val_accuracy: 1.0000\n",
            "Epoch 12/150\n",
            "104/104 [==============================] - 3s 24ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 6.1687e-05 - val_accuracy: 1.0000\n",
            "Epoch 13/150\n",
            "104/104 [==============================] - 2s 20ms/step - loss: 8.9524e-04 - accuracy: 1.0000 - val_loss: 5.8734e-05 - val_accuracy: 1.0000\n",
            "Epoch 14/150\n",
            "104/104 [==============================] - 2s 18ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 4.5553e-05 - val_accuracy: 1.0000\n",
            "Epoch 15/150\n",
            "104/104 [==============================] - 2s 19ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 3.2876e-05 - val_accuracy: 1.0000\n",
            "Epoch 16/150\n",
            "104/104 [==============================] - 2s 19ms/step - loss: 7.5443e-04 - accuracy: 1.0000 - val_loss: 2.9459e-05 - val_accuracy: 1.0000\n",
            "12/12 [==============================] - 1s 7ms/step\n",
            "LSTM-FCN classifier trained, with validation accuracy 1.0.\n",
            "        Pred:1  Pred:0\n",
            "True:1     194       0\n",
            "True:0       0     172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def AutoencoderLSTM(n_timesteps, n_features):\n",
        "    # Define encoder and decoder structure\n",
        "    # structure from medium post: https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352\n",
        "    def EncoderLSTM(input):\n",
        "        # x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(input)\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(input)\n",
        "        # encoded = keras.layers.LSTM(32, activation='relu', return_sequences=False)(x)\n",
        "        encoded = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=False)(x)\n",
        "        return encoded\n",
        "\n",
        "    def DecoderLSTM(encoded):\n",
        "        x = keras.layers.RepeatVector(n_timesteps)(encoded)\n",
        "        # x = keras.layers.LSTM(32, activation='relu', return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(32, activation=\"tanh\", return_sequences=True)(x)\n",
        "        # x = keras.layers.LSTM(64, activation='relu', return_sequences=True)(x)\n",
        "        x = keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True)(x)\n",
        "        decoded = keras.layers.TimeDistributed(\n",
        "            keras.layers.Dense(n_features, activation=\"sigmoid\")\n",
        "        )(x)\n",
        "        return decoded\n",
        "\n",
        "    # Define the AE model\n",
        "    orig_input2 = keras.Input(shape=(n_timesteps, n_features))\n",
        "\n",
        "    autoencoder2 = keras.Model(\n",
        "        inputs=orig_input2, outputs=DecoderLSTM(EncoderLSTM(orig_input2))\n",
        "    )\n",
        "\n",
        "    return autoencoder2"
      ],
      "metadata": {
        "id": "OibhEdy9FBEv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "\n",
        "# ### LSTM autoencoder\n",
        "autoencoder = AutoencoderLSTM( n_timesteps_padded,n_features)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoder.fit(\n",
        "    X_train,\n",
        "    X_train,\n",
        "    epochs=50,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_validation, X_validation),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")\n"
      ],
      "metadata": {
        "id": "E6IxH8BLEKFG",
        "outputId": "92ab9dac-03dc-4c14-9fdd-d14b9cadb2d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for 1dCNN autoencoder:\n",
            "Epoch 1/50\n",
            "104/104 - 11s - loss: 0.0281 - val_loss: 0.0264 - 11s/epoch - 106ms/step\n",
            "Epoch 2/50\n",
            "104/104 - 3s - loss: 0.0262 - val_loss: 0.0273 - 3s/epoch - 31ms/step\n",
            "Epoch 3/50\n",
            "104/104 - 3s - loss: 0.0264 - val_loss: 0.0261 - 3s/epoch - 26ms/step\n",
            "Epoch 4/50\n",
            "104/104 - 3s - loss: 0.0262 - val_loss: 0.0263 - 3s/epoch - 26ms/step\n",
            "Epoch 5/50\n",
            "104/104 - 3s - loss: 0.0262 - val_loss: 0.0263 - 3s/epoch - 27ms/step\n",
            "Epoch 6/50\n",
            "104/104 - 3s - loss: 0.0262 - val_loss: 0.0262 - 3s/epoch - 31ms/step\n",
            "Epoch 7/50\n",
            "104/104 - 3s - loss: 0.0259 - val_loss: 0.0251 - 3s/epoch - 27ms/step\n",
            "Epoch 8/50\n",
            "104/104 - 3s - loss: 0.0264 - val_loss: 0.0261 - 3s/epoch - 27ms/step\n",
            "Epoch 9/50\n",
            "104/104 - 3s - loss: 0.0260 - val_loss: 0.0261 - 3s/epoch - 27ms/step\n",
            "Epoch 10/50\n",
            "104/104 - 3s - loss: 0.0259 - val_loss: 0.0272 - 3s/epoch - 31ms/step\n",
            "Epoch 11/50\n",
            "104/104 - 3s - loss: 0.0260 - val_loss: 0.0259 - 3s/epoch - 29ms/step\n",
            "Epoch 12/50\n",
            "104/104 - 3s - loss: 0.0257 - val_loss: 0.0287 - 3s/epoch - 28ms/step\n",
            "1dCNN autoencoder trained, with validation loss: 0.02514694817364216.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gettting the Global weights, needed for counterfactuals\n",
        "\n",
        "from _guided import get_global_weights\n",
        "from help_functions import evaluate\n",
        "pos_label = 1\n",
        "neg_label = 0\n",
        "\n",
        "step_weights = get_global_weights(\n",
        "        X,\n",
        "        y_classes,\n",
        "        classifier,\n",
        "        n_timesteps= n_timesteps,\n",
        "        n_features=n_features,\n",
        "        random_state=RANDOM_STATE,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hysd9dxSsx9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_weights"
      ],
      "metadata": {
        "id": "3WcwnNgc3RdP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c73781c-8d33-4980-c8a7-e324225abdb0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " reset_seeds()\n",
        "\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.9,\n",
        "    tolerance=1e-6,\n",
        "    max_iter=500,\n",
        "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),\n",
        "    autoencoder = autoencoder,\n",
        "    pred_margin_weight=0.7,\n",
        "    random_state= RANDOM_STATE,\n",
        "    step_weights = step_weights\n",
        "    )\n",
        "cf_model.fit(classifier)\n",
        "\n",
        "\n",
        "y_neg = y_classes[y_classes == 0]\n",
        "X_neg = X[y_classes == 0]\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    cf_embeddings, losses, weights = cf_model.transform(x = X_neg,pred_labels = y_neg)\n"
      ],
      "metadata": {
        "id": "f-Xy39aj8q9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa31ca3-f230-4b39-bf13-90844ed9ac2e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 samples been transformed.\n",
            "26 samples been transformed.\n",
            "51 samples been transformed.\n",
            "76 samples been transformed.\n",
            "101 samples been transformed.\n",
            "126 samples been transformed.\n",
            "151 samples been transformed.\n",
            "176 samples been transformed.\n",
            "201 samples been transformed.\n",
            "226 samples been transformed.\n",
            "251 samples been transformed.\n",
            "276 samples been transformed.\n",
            "301 samples been transformed.\n",
            "326 samples been transformed.\n",
            "351 samples been transformed.\n",
            "376 samples been transformed.\n",
            "401 samples been transformed.\n",
            "426 samples been transformed.\n",
            "451 samples been transformed.\n",
            "476 samples been transformed.\n",
            "501 samples been transformed.\n",
            "526 samples been transformed.\n",
            "551 samples been transformed.\n",
            "576 samples been transformed.\n",
            "601 samples been transformed.\n",
            "626 samples been transformed.\n",
            "651 samples been transformed.\n",
            "676 samples been transformed.\n",
            "701 samples been transformed.\n",
            "726 samples been transformed.\n",
            "751 samples been transformed.\n",
            "776 samples been transformed.\n",
            "801 samples been transformed.\n",
            "826 samples been transformed.\n",
            "851 samples been transformed.\n",
            "876 samples been transformed.\n",
            "901 samples been transformed.\n",
            "926 samples been transformed.\n",
            "951 samples been transformed.\n",
            "972 samples been transformed, in total.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Validity\n",
        "\n",
        "cf_pred = classifier.predict(cf_embeddings)[:, 1] # predicted probabilities of CFs\n",
        "cf_pred_labels = cf_pred\n",
        "for idx in range(cf_pred_labels.shape[0]):\n",
        "  if cf_pred_labels[idx] > 0.5:\n",
        "    cf_pred_labels[idx] = 1\n",
        "  else:\n",
        "    cf_pred_labels[idx] = 0\n",
        "\n",
        "\n",
        "print(f'Transformation_finished with validity_score = {validity_score(y_neg,cf_pred_labels)}')"
      ],
      "metadata": {
        "id": "lJvrV_LSjWME",
        "outputId": "ea060304-7f29-4caf-df1c-e25e82b2b708",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31/31 [==============================] - 0s 7ms/step\n",
            "Transformation_finished with validity_score = 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total = 0\n",
        "probability = 0.5\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = classifier.predict(counterfactual)[:, 1]\n",
        "    dist = (prediction - probability)\n",
        "    total +=dist\n",
        "mean_mse = total /cf_embeddings.shape[0]\n"
      ],
      "metadata": {
        "id": "ZkPSSHScmi2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "id": "6FXZX1A-5Er1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710c4e91-036a-4f81-d53a-ede6985b9bed"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mean MSE of the data is: [0.40097612] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total = 0\n",
        "probability = 0.5\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = classifier.predict(counterfactual)[:, 1]\n",
        "    dist = abs(prediction - probability)\n",
        "    total +=dist\n",
        "mean_mse = total /cf_embeddings.shape[0]\n"
      ],
      "metadata": {
        "id": "AcUwDKnZmj6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The Absolute Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "id": "1x5s2_8H5F3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b4c5c7-a214-40b4-98e2-07752e70e4f8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Absolute Mean MSE of the data is: [0.40097612] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Proximity\n",
        "def euclidean_distance(X, cf_samples):\n",
        "    paired_distances = np.linalg.norm(X - cf_samples, axis=1)\n",
        "    return np.mean(paired_distances)\n",
        "euclidean_distance(X_neg, cf_embeddings)"
      ],
      "metadata": {
        "id": "50Hed3JXrm1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ffe442-9f58-47cd-d057-47796a42d237"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.150506491749179"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_paddings(cf_samples, padding_size):\n",
        "    if padding_size != 0:\n",
        "        # use np.squeeze() to cut the last time-series dimension, for evaluation\n",
        "        cf_samples = np.squeeze(cf_samples[:, :-padding_size, :])\n",
        "    else:\n",
        "        cf_samples = np.squeeze(cf_samples)\n",
        "    return cf_samples"
      ],
      "metadata": {
        "id": "9MPQoXMjrFmn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove paddings because KDE does not work with paddings.\n",
        "\n",
        "X_unpadded = remove_paddings(X, padding_size)\n",
        "cf_embeddings_unpadded = remove_paddings(cf_embeddings, padding_size)"
      ],
      "metadata": {
        "id": "dh_BVpXMrtXu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def train_gaussian_kde(data, use_scotts_rule=False, use_silvermans_rule=None, manual_bandwidth=0.5):\n",
        "    \"\"\"\n",
        "    Train a Gaussian KDE on the provided data.\n",
        "\n",
        "    :param data: Multivariate data points used for KDE, shape (samples, timesteps).\n",
        "    :param use_scotts_rule: Use Scott's Rule for bandwidth estimation.\n",
        "    :param use_silvermans_rule: Use Silverman's Rule for bandwidth estimation.\n",
        "    :param manual_bandwidth: Manually specified bandwidth (float).\n",
        "    :return: A function that represents the trained KDE.\n",
        "    \"\"\"\n",
        "    n, d = map(tf.cast, [tf.shape(data)[0], tf.shape(data)[1]], [tf.float32, tf.float32])\n",
        "\n",
        "    # Calculate bandwidth\n",
        "    if use_scotts_rule:\n",
        "        sigma = tf.math.reduce_std(data)\n",
        "        bandwidth = n ** (-1.0 / (d + 4)) * sigma\n",
        "    elif use_silvermans_rule:\n",
        "        sigma = tf.math.reduce_std(data)\n",
        "        bandwidth = (4 * sigma ** 5 / (3 * n)) ** (1/5)\n",
        "    else:\n",
        "        if manual_bandwidth is None:\n",
        "            raise ValueError(\"Manual bandwidth must be provided if not using Scott's or Silverman's Rule.\")\n",
        "        bandwidth = manual_bandwidth\n",
        "\n",
        "    bandwidth = tf.cast(bandwidth, tf.float32)\n",
        "\n",
        "    def kde_fn(x_points):\n",
        "        \"\"\"\n",
        "        Evaluate the KDE on new data points.\n",
        "\n",
        "        :param x_points: New data points to evaluate, shape (samples, timesteps).\n",
        "        :return: Density estimates for each point.\n",
        "        \"\"\"\n",
        "        # Reshape and expand dimensions for broadcasting\n",
        "        x_points_exp = tf.expand_dims(x_points, 1)\n",
        "        data_exp = tf.expand_dims(data, 0)\n",
        "\n",
        "        # Calculate the kernel\n",
        "        diff = x_points_exp - data_exp\n",
        "        norm = tf.reduce_sum(diff ** 2, axis=2)\n",
        "        kernel_val = tf.exp(-norm / (2.0 * bandwidth ** 2))\n",
        "\n",
        "        # Density calculation\n",
        "        density = tf.reduce_mean(kernel_val, axis=1) / (bandwidth * tf.sqrt(2.0 * np.pi * d))\n",
        "        return density\n",
        "\n",
        "    return kde_fn\n",
        "\n",
        "def gaussian_kde_logpdf( kde_fn, x_points):\n",
        "  density = kde_fn(x_points)\n",
        "  return tf.math.log(density)\n"
      ],
      "metadata": {
        "id": "JwfIHHEHt06a"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "diffrences_from_abnormal = []\n",
        "diffrences_from_normal = []\n",
        "for dimention in range(cf_embeddings.shape[2]):\n",
        "\n",
        "\n",
        "  abnormal_data = tf.cast(X_unpadded[y_classes == 1][:,:,dimention],tf.float32)\n",
        "  normal_data = tf.cast(X_unpadded[y_classes == 0][:,:,dimention],tf.float32)\n",
        "  counterf_data = tf.cast(cf_embeddings_unpadded[:,:,dimention],tf.float32)\n",
        "\n",
        "  #get the kernel for every dimention of the trained\n",
        "  kernel = train_gaussian_kde(abnormal_data)\n",
        "\n",
        "  #get all the log likelihoods\n",
        "  log_likelihood_abnormal = np.mean(gaussian_kde_logpdf(kernel,abnormal_data))\n",
        "  log_likelihood_normal = np.mean(gaussian_kde_logpdf(kernel,normal_data))\n",
        "  log_likelihood_counterfactual = np.mean(gaussian_kde_logpdf(kernel,counterf_data))\n",
        "\n",
        "  #get the diffrences from the counterfactuals\n",
        "  diff_from_abnormal = abs(log_likelihood_counterfactual-log_likelihood_abnormal)\n",
        "  diffrences_from_abnormal.append(diff_from_abnormal)\n",
        "\n",
        "  diff_from_normal = abs(log_likelihood_counterfactual-log_likelihood_normal)\n",
        "  diffrences_from_normal.append(diff_from_normal)"
      ],
      "metadata": {
        "id": "xWhPgNjAspuq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(diffrences_from_normal)"
      ],
      "metadata": {
        "id": "9d4LPz8jhBve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058047f9-8033-477f-bbd2-d74897eb631d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[21.366467, 1.317986, 0.9960098]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(diffrences_from_abnormal)"
      ],
      "metadata": {
        "id": "bWvnGPiMi1Yt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8f6f98-b222-42b0-989c-4aabe476597c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.6452014, 2.8353987, 1.1343465]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(diffrences_from_normal))"
      ],
      "metadata": {
        "id": "vxNyE5DOkgIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4f3466-f714-4ac9-837b-73468677c055"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.8934875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(diffrences_from_abnormal))"
      ],
      "metadata": {
        "id": "_YUPomtilE7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac3a1db-aa58-49cd-e1a8-427be63ea25a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8716489\n"
          ]
        }
      ]
    }
  ]
}