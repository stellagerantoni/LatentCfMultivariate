{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stellagerantoni/LatentCfMultivariate/blob/main/multivariate_pendigits(cl_and_auto_trained_on_all).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/stellagerantoni/LatentCfMultivariate"
      ],
      "metadata": {
        "id": "1fwOXGEl_ine",
        "outputId": "b49e6e5c-89ba-4df2-f3b0-6c4055c383d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LatentCfMultivariate'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 146 (delta 85), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 7.67 MiB | 8.54 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wildboar\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q stumpy\n",
        "!pip install -q fastdtw\n",
        "!pip install aeon[all_extras]"
      ],
      "metadata": {
        "id": "89L3kts7CCan",
        "outputId": "f09c637e-2f0f-47a9-f544-039213991a44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.1/169.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aeon[all_extras]\n",
            "  Downloading aeon-0.6.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (23.2.0)\n",
            "Collecting deprecated>=1.2.13 (from aeon[all_extras])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: numba>=0.55 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.58.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (23.2)\n",
            "Requirement already satisfied: pandas<2.1.0,>=1.5.3 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn<1.4.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.11.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.2.1)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2023.8.1)\n",
            "Collecting filterpy>=1.4.5 (from aeon[all_extras])\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (3.9.0)\n",
            "Collecting hmmlearn>=0.2.7 (from aeon[all_extras])\n",
            "  Downloading hmmlearn-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonts>=0.12.4 (from aeon[all_extras])\n",
            "  Downloading gluonts-0.14.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-self-attention (from aeon[all_extras])\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kotsu>=0.3.1 (from aeon[all_extras])\n",
            "  Downloading kotsu-0.3.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (3.7.1)\n",
            "Collecting mne (from aeon[all_extras])\n",
            "  Downloading mne-1.6.0-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pmdarima>=1.8.0 (from aeon[all_extras])\n",
            "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prophet>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.1.5)\n",
            "Collecting pyod>=0.8.0 (from aeon[all_extras])\n",
            "  Downloading pyod-1.1.2.tar.gz (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-posthocs>=0.6.5 (from aeon[all_extras])\n",
            "  Downloading scikit_posthocs-0.8.1-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.12.2)\n",
            "Collecting statsforecast>=0.5.2 (from aeon[all_extras])\n",
            "  Downloading statsforecast-1.7.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plotly-resampler>=0.9.0 (from aeon[all_extras])\n",
            "  Downloading plotly_resampler-0.9.2-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.14.1)\n",
            "Requirement already satisfied: stumpy>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (1.12.0)\n",
            "Collecting tbats>=1.1.0 (from aeon[all_extras])\n",
            "  Downloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.22.0)\n",
            "Collecting tsfresh>=0.20.0 (from aeon[all_extras])\n",
            "  Downloading tsfresh-0.20.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.3/95.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tslearn>=0.5.2 (from aeon[all_extras])\n",
            "  Downloading tslearn-0.6.3-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (2023.7.0)\n",
            "Collecting mlflow<2.4.0 (from aeon[all_extras])\n",
            "  Downloading mlflow-2.3.2-py3-none-any.whl (17.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (from aeon[all_extras]) (0.10.1)\n",
            "Collecting esig<0.9.8.3,>=0.9.7 (from aeon[all_extras])\n",
            "  Downloading esig-0.9.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->aeon[all_extras]) (1.14.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (1.10.13)\n",
            "Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (4.66.1)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts>=0.12.4->aeon[all_extras]) (4.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->aeon[all_extras]) (2.8.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (8.1.7)\n",
            "Collecting databricks-cli<1,>=0.8.7 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.4)\n",
            "Collecting gitpython<4,>=2.1.0 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (6.0.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.20.3)\n",
            "Requirement already satisfied: pytz<2024 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2023.3.post1)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.31.0)\n",
            "Collecting importlib-metadata!=4.7.0,<7,>=3.7.0 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (0.4.4)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker<7,>=4.0.0 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.2.5)\n",
            "Collecting querystring-parser<2 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (2.0.24)\n",
            "Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (10.0.1)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.5.1)\n",
            "Collecting gunicorn<21 (from mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow<2.4.0->aeon[all_extras]) (3.1.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55->aeon[all_extras]) (0.41.1)\n",
            "Collecting dash>=2.9.0 (from plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading dash-2.14.2-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.8.0 (from plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly<6.0.0,>=5.5.0 in /usr/local/lib/python3.10/dist-packages (from plotly-resampler>=0.9.0->aeon[all_extras]) (5.15.0)\n",
            "Collecting tsdownsample==0.1.2 (from plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading tsdownsample-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->aeon[all_extras]) (1.3.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->aeon[all_extras]) (3.0.7)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->aeon[all_extras]) (2.0.7)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima>=1.8.0->aeon[all_extras]) (67.7.2)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (1.2.0)\n",
            "Requirement already satisfied: holidays>=0.25 in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (0.40)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from prophet>=1.1.0->aeon[all_extras]) (6.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyod>=0.8.0->aeon[all_extras]) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.0,>=1.0.0->aeon[all_extras]) (3.2.0)\n",
            "Collecting fugue>=0.8.1 (from statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading fugue-0.8.7-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.8/279.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting utilsforecast>=0.0.24 (from statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading utilsforecast-0.0.24-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12.1->aeon[all_extras]) (0.5.6)\n",
            "Requirement already satisfied: distributed>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh>=0.20.0->aeon[all_extras]) (2023.8.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->aeon[all_extras]) (2023.6.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask->aeon[all_extras]) (1.4.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne->aeon[all_extras]) (1.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne->aeon[all_extras]) (4.4.2)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from mne->aeon[all_extras]) (0.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from mne->aeon[all_extras]) (0.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->aeon[all_extras]) (2.15.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability->aeon[all_extras]) (0.1.8)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->aeon[all_extras]) (0.42.0)\n",
            "Requirement already satisfied: stanio~=0.3.0 in /usr/local/lib/python3.10/dist-packages (from cmdstanpy>=1.0.4->prophet>=1.1.0->aeon[all_extras]) (0.3.0)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras]) (3.0.1)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Collecting retrying (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Collecting ansi2html (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras])\n",
            "  Downloading ansi2html-1.9.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.9.0->plotly-resampler>=0.9.0->aeon[all_extras]) (1.5.8)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (2.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from databricks-cli<1,>=0.8.7->mlflow<2.4.0->aeon[all_extras]) (0.9.0)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (1.0.7)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (6.3.2)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2.11.0->tsfresh>=0.20.0->aeon[all_extras]) (3.0.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker<7,>=4.0.0->mlflow<2.4.0->aeon[all_extras]) (1.7.0)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3->mlflow<2.4.0->aeon[all_extras]) (2.1.2)\n",
            "Collecting triad>=0.9.3 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading triad-0.9.3-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading adagio-0.2.4-py3-none-any.whl (26 kB)\n",
            "Collecting qpd>=0.4.4 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading qpd-0.4.4-py3-none-any.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.2/169.2 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fugue-sql-antlr>=0.1.6 (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading fugue-sql-antlr-0.2.0.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sqlglot in /usr/local/lib/python3.10/dist-packages (from fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (19.9.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=2.1.0->mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow<2.4.0->aeon[all_extras]) (3.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow<2.4.0->aeon[all_extras]) (2.1.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly<6.0.0,>=5.5.0->plotly-resampler>=0.9.0->aeon[all_extras]) (8.2.3)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne->aeon[all_extras]) (4.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow<2.4.0->aeon[all_extras]) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow<2.4.0->aeon[all_extras]) (3.0.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (0.7.2)\n",
            "Collecting antlr4-python3-runtime<4.12 (from fugue-sql-antlr>=0.1.6->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading antlr4_python3_runtime-4.11.1-py3-none-any.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow<2.4.0->aeon[all_extras])\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (1.3.1)\n",
            "Collecting fs (from triad>=0.9.3->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras])\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow->aeon[all_extras]) (0.5.1)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.10/dist-packages (from fs->triad>=0.9.3->fugue>=0.8.1->statsforecast>=0.5.2->aeon[all_extras]) (1.4.4)\n",
            "Building wheels for collected packages: filterpy, pyod, keras-self-attention, fugue-sql-antlr\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110458 sha256=6c6320d2702a645742fc6351b189952816c2ac6bd5092af1d7bdc090a35ef2a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-1.1.2-py3-none-any.whl size=190292 sha256=e617d247550be7a905699ae33b821dbd9cdc67faefcf40ac485e9836cd536a54\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/1b/61/aa85b78c3c0c8871f4231e3f4a03bb23cecb7db829498380ee\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18895 sha256=42b434a1cbeb36d6cdf40fa323b2248593aebeccddfa44a8decb8186ea9f2f92\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "  Building wheel for fugue-sql-antlr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fugue-sql-antlr: filename=fugue_sql_antlr-0.2.0-py3-none-any.whl size=158196 sha256=103ef01d60e0e0e3b88b0a739beca8e14f67b1012e0289f3198648bc89636ed6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/b5/4e/216953a1c711da55de29ed7ecf158b4a5bf32ef93d69ad66dd\n",
            "Successfully built filterpy pyod keras-self-attention fugue-sql-antlr\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, antlr4-python3-runtime, tsdownsample, smmap, retrying, querystring-parser, orjson, Mako, keras-self-attention, importlib-metadata, gunicorn, fs, esig, deprecated, ansi2html, gitdb, docker, databricks-cli, alembic, utilsforecast, tslearn, triad, pyod, mne, kotsu, hmmlearn, gluonts, gitpython, filterpy, dash, aeon, scikit-posthocs, pmdarima, plotly-resampler, mlflow, fugue-sql-antlr, adagio, tsfresh, tbats, qpd, fugue, statsforecast\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "Successfully installed Mako-1.3.0 adagio-0.2.4 aeon-0.6.0 alembic-1.13.1 ansi2html-1.9.1 antlr4-python3-runtime-4.11.1 dash-2.14.2 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 databricks-cli-0.18.0 deprecated-1.2.14 docker-6.1.3 esig-0.9.8.2 filterpy-1.4.5 fs-2.4.16 fugue-0.8.7 fugue-sql-antlr-0.2.0 gitdb-4.0.11 gitpython-3.1.41 gluonts-0.14.3 gunicorn-20.1.0 hmmlearn-0.3.0 importlib-metadata-6.11.0 keras-self-attention-0.51.0 kotsu-0.3.3 mlflow-2.3.2 mne-1.6.0 orjson-3.9.10 plotly-resampler-0.9.2 pmdarima-2.0.4 pyod-1.1.2 qpd-0.4.4 querystring-parser-1.2.4 retrying-1.3.4 scikit-posthocs-0.8.1 smmap-5.0.1 statsforecast-1.7.1 tbats-1.1.3 triad-0.9.3 tsdownsample-0.1.2 tsfresh-0.20.1 tslearn-0.6.3 utilsforecast-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from argparse import ArgumentParser\n",
        "from aeon.datasets import load_classification\n",
        "\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy.spatial import distance_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KDTree, KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from wildboar.datasets import load_dataset\n",
        "from wildboar.ensemble import ShapeletForestClassifier\n",
        "from wildboar.explain.counterfactual import counterfactuals\n",
        "%cd '/content/LatentCfMultivariate'\n",
        "from _guided import ModifiedLatentCF\n",
        "from help_functions import *\n",
        "from keras_models import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdkpan5lCGRH",
        "outputId": "9eeeaea8-230c-4c19-9a98-db3e8d7d2504"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LatentCfMultivariate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "RANDOM_STATE = 39"
      ],
      "metadata": {
        "id": "GJE1AxFnE51S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACTUALL CODE**"
      ],
      "metadata": {
        "id": "6vVfmpyuZyC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the dataset\n",
        "\n",
        "RANDOM_STATE = 39\n",
        "X,y = load_classification('PenDigits')\n",
        "X = X.transpose(0,2,1)\n",
        "print(f'shape of X = {X.shape}')\n",
        "print(f'shape of y = {y.shape}')\n",
        "#print(f'data imformation = {data_information}')\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
        "print(f'shape of X train = {X_train.shape}')\n",
        "print(f'shape of y train = {y_train.shape}')"
      ],
      "metadata": {
        "id": "W4m9pwqyVY1b",
        "outputId": "3ec03ef4-c9bc-4aa3-e0b6-3404372662e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of X = (10992, 8, 2)\n",
            "shape of y = (10992,)\n",
            "shape of X train = (8793, 8, 2)\n",
            "shape of y train = (8793,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upsample the minority class\n",
        "\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'before: {class_counts}')\n",
        "X_train,y_train = upsample_minority_multivariate(X_train,y_train)\n",
        "X,y = upsample_minority_multivariate(X, y)\n",
        "unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "print(f'after: {class_counts}')"
      ],
      "metadata": {
        "id": "Q2v7QdrHieA8",
        "outputId": "52b5a5a3-b20a-4699-fd55-994a5dc1fcb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before: [914 914 915 844 915 844 845 914 844 844]\n",
            "after: [915 915 915 915 915 915 915 915 915 915]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Processing and Padding all our data\n",
        "#Padding needed for autoencoder\n",
        "\n",
        "n_training,n_timesteps, n_features= X_train.shape\n",
        "\n",
        "X, trained_scaler =  normalize_multivariate(data=X, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_train_processed, trained_scaler =  normalize_multivariate(data=X_train, n_timesteps=n_timesteps, n_features = n_features)\n",
        "X_test_processed, _ =  normalize_multivariate(data=X_test, n_timesteps=n_timesteps, scaler=trained_scaler, n_features = n_features)\n",
        "\n",
        "X, padding_size = conditional_pad_multivariate(X)\n",
        "X_train_processed_padded, padding_size = conditional_pad_multivariate(X_train_processed) # add extra padding zeros if n_timesteps cannot be divided by 4, required for 1dCNN autoencoder structure\n",
        "X_test_processed_padded, _ = conditional_pad_multivariate(X_test_processed)\n",
        "\n",
        "n_timesteps_padded = X_train_processed_padded.shape[1]\n",
        "print(f\"Data pre-processed, original #timesteps={n_timesteps}, padded #timesteps={n_timesteps_padded}.\")\n",
        "\n",
        "#check the processing (0,1) min should be min 0 and max should be max 1\n",
        "print(f\"\\nmin value = {np.min(X_train)}, max value = {np.max(X_train)}\")\n",
        "print(f\"min value normalized = {np.min(X_train_processed)}, max value normalized= {np.max(X_train_processed)}\")\n",
        "\n",
        "#check that padding paddes the right dimention\n",
        "print(f\"\\nX_train.shape = {X_train.shape}\" )\n",
        "print(f\"X_train_processed_padded.shape = {X_train_processed_padded.shape}\")\n"
      ],
      "metadata": {
        "id": "00Q9QjKy7wEZ",
        "outputId": "390dd9e7-dc68-4e1a-f02a-a21cb84194fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data pre-processed, original #timesteps=8, padded #timesteps=8.\n",
            "\n",
            "min value = 0.0, max value = 100.0\n",
            "min value normalized = 0.0, max value normalized= 1.0\n",
            "\n",
            "X_train.shape = (9150, 8, 2)\n",
            "X_train_processed_padded.shape = (9150, 8, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=RANDOM_STATE, stratify=y_train)\n"
      ],
      "metadata": {
        "id": "1mYFZmsvtB9q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the two forms of labels needed\n",
        "#-the y_classes (1,0,1,0,...)\n",
        "#-the y (one hot encoded)\n",
        "\n",
        "print(f'X_train = {X_train.shape}')\n",
        "print(f'X_validation = {X_validation.shape}')\n",
        "print(f'X_test = {X_test.shape}')\n",
        "\n",
        "y_classes = y\n",
        "y_train_classes = y_train\n",
        "y_validation_classes = y_validation\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, len(np.unique(y)))\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_validation = to_categorical(y_validation, len(np.unique(y_validation)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
        "\n",
        "print(f'\\ny_train_classes = {y_train_classes.shape}, y_validation_classes = {y_validation_classes.shape}, y_test_classes = {y_test_classes.shape}')\n",
        "print(f'y_train = {y_train.shape}, y_validation = {y_validation.shape}, y_test= {y_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9b2F-vytxzU",
        "outputId": "a40d923b-4076-4798-9dc6-2e03a4bb66db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train = (7320, 8, 2)\n",
            "X_validation = (1830, 8, 2)\n",
            "X_test = (2199, 8, 2)\n",
            "\n",
            "y_train_classes = (7320,), y_validation_classes = (1830,), y_test_classes = (2199,)\n",
            "y_train = (7320, 10), y_validation = (1830, 10), y_test= (2199, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(y.shape[0]):\n",
        "  if y[i] == n_2:\n",
        "    y[i]=1\n",
        "  else:\n",
        "    y [i]=0"
      ],
      "metadata": {
        "id": "fiCPol0fjSJe",
        "outputId": "893d4fe8-81dc-4e62-fe23-f5cbe28cbd90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'n_2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5f57a963c78e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ## 2. LatentCF models\n",
        "# reset seeds for numpy, tensorflow, python random package and python environment seed\n",
        "reset_seeds()\n",
        "###############################################\n",
        "# ### 1dCNN classifier\n",
        "\n",
        "cnnClassifier = Classifier(\n",
        "    n_timesteps_padded, n_features, n_output=10, add_dense_layer = False\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "cnnClassifier.compile(\n",
        "    optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping_accuracy = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=15, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for LSTM-FCN classifier:\")\n",
        "classifier_history = cnnClassifier.fit(\n",
        "    X_train,\n",
        "    y_train_classes,\n",
        "    epochs=150,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=True,\n",
        "    validation_data=(X_validation, y_validation_classes),\n",
        "    callbacks=[early_stopping_accuracy],\n",
        ")\n",
        "\n",
        "y_pred = cnnClassifier.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "acc = balanced_accuracy_score(y_true=y_test_classes, y_pred=y_pred_classes)\n",
        "print(f\"LSTM-FCN classifier trained, with validation accuracy {acc}.\")"
      ],
      "metadata": {
        "id": "yNkKTXe6IIyF",
        "outputId": "c1af396c-a6c2-4997-b3f4-1b6bdb286c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training log for LSTM-FCN classifier:\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((12, 10) vs (12, 1)).\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a9a8346b847f>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mreset_seeds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training log for LSTM-FCN classifier:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m classifier_history = cnnClassifier.fit(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0my_train_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((12, 10) vs (12, 1)).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "\n",
        "\n",
        "# ### 1dCNN autoencoder\n",
        "autoencoder = Autoencoder( n_timesteps_padded,n_features,64)\n",
        "optimizer = keras.optimizers.Adam(lr=0.0005)\n",
        "autoencoder.compile(optimizer=optimizer, loss=\"mse\")\n",
        "\n",
        "# Define the early stopping criteria\n",
        "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, restore_best_weights=True)\n",
        "# Train the model\n",
        "reset_seeds()\n",
        "print(\"Training log for 1dCNN autoencoder:\")\n",
        "autoencoder_history = autoencoder.fit(\n",
        "    X_train,\n",
        "    X_train,\n",
        "    epochs=50,\n",
        "    batch_size=12,\n",
        "    shuffle=True,\n",
        "    verbose=2,\n",
        "    validation_data=(X_validation, X_validation),\n",
        "    callbacks=[early_stopping])\n",
        "\n",
        "ae_val_loss = np.min(autoencoder_history.history['val_loss'])\n",
        "print(f\"1dCNN autoencoder trained, with validation loss: {ae_val_loss}.\")\n"
      ],
      "metadata": {
        "id": "E6IxH8BLEKFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 5\n",
        "sample = X_test[idx:idx+1]\n",
        "sample_label = y_test[idx:idx+1]\n",
        "autoencoded_sample = autoencoder.predict(sample)\n",
        "visualise_decoded_digit(autoencoded_sample,sample_label)\n",
        "visualise_digit(X_test,y_test,idx)"
      ],
      "metadata": {
        "id": "KGldOuWp06wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_two_digits(X,y,n_1,n_2, RANDOM_STATE):\n",
        "  #get the normal and abnormal label\n",
        "  #abnormal is the target label for the latentCF++ model\n",
        "  normal_label = n_1\n",
        "  abnormal_label = n_2\n",
        "\n",
        "  #Get the indices\n",
        "  normal_indices = np.where(y == normal_label)[0]\n",
        "  abnormal_indices = np.where(y == abnormal_label)[0]\n",
        "\n",
        "  #Use the indices to get the wanted data points\n",
        "  X_abnormal = X[abnormal_indices]\n",
        "  y_abnormal = y[abnormal_indices]\n",
        "\n",
        "  X_normal = X[normal_indices]\n",
        "  y_normal = y[normal_indices]\n",
        "\n",
        "  #stack all the data together again\n",
        "  X = np.concatenate([X_abnormal,X_normal])\n",
        "  y = np.concatenate([y_abnormal,y_normal])\n",
        "\n",
        "  #shuffle them\n",
        "  X, y = shuffle(X, y, random_state=RANDOM_STATE)\n",
        "\n",
        "  #iterate over the dataset to make the labels 0 for abnormal and 1 for normal\n",
        "\n",
        "  for i in range(y.shape[0]):\n",
        "    if y[i] == n_2:\n",
        "      y[i]=1\n",
        "    else:\n",
        "      y [i]=0\n",
        "\n",
        "  y = y.astype(int)\n",
        "  print(f'Class 0 represents number {n_1}. [1.,0.]')\n",
        "  print(f'Class 1 represents number {n_2}. [0.,1.]\\n')\n",
        "\n",
        "  return X,y"
      ],
      "metadata": {
        "id": "f8fX4Ipx2P2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get only two classes from the dataset (only too needed for counterfactuals)\n",
        "\n",
        "X, y = extract_two_digits(X,y,'0','8', RANDOM_STATE)\n",
        "X_train, y_train = extract_two_digits(X_train_processed_padded,y_train,'0','8', RANDOM_STATE)\n",
        "X_test, y_test = extract_two_digits(X_test_processed_padded,y_test,'0','8', RANDOM_STATE)"
      ],
      "metadata": {
        "id": "2JYhT7Ne1_s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the two forms of labels needed\n",
        "#-the y_classes (1,0,1,0,...)\n",
        "#-the y (one hot encoded)\n",
        "\n",
        "print(f'X_train = {X_train.shape}')\n",
        "print(f'X_validation = {X_validation.shape}')\n",
        "print(f'X_test = {X_test.shape}')\n",
        "\n",
        "y_classes = y\n",
        "y_train_classes = y_train\n",
        "y_validation_classes = y_validation\n",
        "y_test_classes = y_test\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y = to_categorical(y, len(np.unique(y)))\n",
        "y_train = to_categorical(y_train, len(np.unique(y_train)))\n",
        "y_validation = to_categorical(y_validation, len(np.unique(y_validation)))\n",
        "y_test = to_categorical(y_test, len(np.unique(y_test)))\n",
        "\n",
        "print(f'\\ny_train_classes = {y_train_classes.shape}, y_validation_classes = {y_validation_classes.shape}, y_test_classes = {y_test_classes.shape}')\n",
        "print(f'y_train = {y_train.shape}, y_validation = {y_validation.shape}, y_test= {y_test.shape}')"
      ],
      "metadata": {
        "id": "vgtijdZd4fJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gettting the Global weights, needed for counterfactuals\n",
        "\n",
        "from _guided import get_global_weights\n",
        "from help_functions import evaluate\n",
        "pos_label = 1\n",
        "neg_label = 0\n",
        "\n",
        "step_weights = get_global_weights(\n",
        "        X,\n",
        "        y_classes,\n",
        "        cnnClassifier,\n",
        "        n_timesteps= n_timesteps,\n",
        "        n_features=n_features,\n",
        "        random_state=RANDOM_STATE,\n",
        ")\n"
      ],
      "metadata": {
        "id": "hysd9dxSsx9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_weights"
      ],
      "metadata": {
        "id": "tni3cK7BHkKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds()\n",
        "cf_model = ModifiedLatentCF(\n",
        "    probability=0.95, max_iter=300, optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),autoencoder = autoencoder,\n",
        "    pred_margin_weight=1, step_weights = step_weightsnew, random_state= RANDOM_STATE)\n",
        "cf_model.fit(cnnClassifier)\n",
        "\n",
        "y_neg_classes = y_classes[y_classes == 0][0:2]\n",
        "X_neg = X[y_classes == 0][0:2]\n"
      ],
      "metadata": {
        "id": "JbgIuyWoJAT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "    cf_embeddings, losses, weights = cf_model.transform(X_neg, y_neg_classes) #self, x, pred_label\n",
        "z_pred = cnnClassifier.predict(cf_embeddings) # predicted probabilities of CFs\n",
        "cf_pred_labels = np.argmax(z_pred, axis=1)\n",
        "\n",
        "#evaluate_res = evaluate(X_pred_neg, best_cf_samples, z_pred, n_timesteps, tree, max_distance)\n",
        "#evaluate_res #  proximity, validity, cost_mean, cost_std\n",
        "\n",
        "print(f'Transformation_finished with validity_score = {validity_score(y_neg_classes,cf_pred_labels)}')"
      ],
      "metadata": {
        "id": "f-Xy39aj8q9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(6):\n",
        "  print(\"actual\")\n",
        "  visualise_digit(X_neg,y_neg_classes,i-1,figsize = (5,5))\n",
        "  print(\"counterfactual\")\n",
        "  visualise_digit(cf_embeddings,1-y_neg_classes,i-1,figsize = (5,5))\n"
      ],
      "metadata": {
        "id": "UbvQjP1RpiYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVv7CJtshJRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validity\n",
        "cf_pred = cnnClassifier.predict(cf_embeddings)[:, 1] # predicted probabilities of CFs\n",
        "cf_pred_labels = cf_pred\n",
        "for idx in range(cf_pred_labels.shape[0]):\n",
        "  if cf_pred_labels[idx] > 0.5:\n",
        "    cf_pred_labels[idx] = 1\n",
        "  else:\n",
        "    cf_pred_labels[idx] = 0\n",
        "\n",
        "\n",
        "print(f'Transformation_finished with validity_score = {validity_score(y_neg,cf_pred_labels)}')"
      ],
      "metadata": {
        "id": "Ov4igLqcorG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity withour absolute\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total_mse = 0\n",
        "probability = 0.9\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = cnnClassifier.predict(counterfactual)[:, 1]\n",
        "    mse = MeanSquaredError()\n",
        "    mse_dist = mse(probability, prediction).numpy()\n",
        "    total_mse +=mse_dist\n",
        "mean_mse = total_mse /cf_embeddings.shape[0]\n",
        "print(f\"The Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "id": "Nvlack0n40dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating proximity with absolute\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "total = 0\n",
        "probability = 0.9\n",
        "for idx in range(cf_embeddings.shape[0]):\n",
        "    counterfactual = cf_embeddings[idx,np.newaxis]\n",
        "    prediction = cnnClassifier.predict(counterfactual)[:, 1]\n",
        "    dist = abs(prediction - probability)\n",
        "    total +=dist\n",
        "mean_mse = total /cf_embeddings.shape[0]\n",
        "print(f\"The Mean MSE of the data is: {mean_mse} \")"
      ],
      "metadata": {
        "id": "oK3o9SQB41a9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Proximity\n",
        "def euclidean_distance(X, cf_samples):\n",
        "    paired_distances = np.linalg.norm(X - cf_samples, axis=1)\n",
        "    return np.mean(paired_distances)\n",
        "euclidean_distance(X_neg, cf_embeddings)"
      ],
      "metadata": {
        "id": "ZTsPvh1lotsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import gaussian_kde\n",
        "\n",
        "diffrences_from_abnormal = []\n",
        "diffrences_from_normal = []\n",
        "for dimention in range(cf_embeddings.shape[2]):\n",
        "\n",
        "\n",
        "  abnormal_data = X[y_classes == 1][:,:,dimention]\n",
        "  normal_data = X[y_classes == 0][:,:,dimention]\n",
        "  counterf_data = cf_embeddings[:,:,dimention]\n",
        "\n",
        "  #get the kernel for every dimention of the trained\n",
        "  kernel = gaussian_kde(abnormal_data.T,bw_method=None)\n",
        "\n",
        "  #get all the log likelihoods\n",
        "  log_likelihood_abnormal = np.mean(kernel.logpdf(abnormal_data.T))\n",
        "  log_likelihood_normal = np.mean(kernel.logpdf(normal_data.T))\n",
        "  log_likelihood_counterfactual = np.mean(kernel.logpdf(counterf_data.T))\n",
        "\n",
        "  #get the diffrences from the counterfactuals\n",
        "  diff_from_abnormal = abs(log_likelihood_counterfactual-log_likelihood_abnormal)\n",
        "  diffrences_from_abnormal.append(diff_from_abnormal)\n",
        "\n",
        "  diff_from_normal = abs(log_likelihood_counterfactual-log_likelihood_normal)\n",
        "  diffrences_from_normal.append(diff_from_normal)\n"
      ],
      "metadata": {
        "id": "7GSjwx8w4liQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffrences_from_normal"
      ],
      "metadata": {
        "id": "G6rd-6JvFLP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffrences_from_abnormal"
      ],
      "metadata": {
        "id": "7j6tO69OFNdv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}